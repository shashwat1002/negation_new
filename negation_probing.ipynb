{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaWrapper(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper on roberta that gives mean-pooled representations of each layer in a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.model_obj = RobertaModel.from_pretrained(\n",
    "            \"roberta-base\").eval()\n",
    "        self.model_obj.eval()\n",
    "        self.tokenizer_obj = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        self.config_obj = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "\n",
    "        encoder_ret = self.tokenizer_obj(\n",
    "            input_text, truncation=True, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "\n",
    "        encoder_text_ids = encoder_ret.input_ids.to(self.device)\n",
    "        attention_mask = encoder_ret.attention_mask.to(self.device) # 1 for not pad\n",
    "\n",
    "        ic(encoder_text_ids.device)\n",
    "        ic(self.model_obj.device)\n",
    "        encoder_states = self.model_obj(\n",
    "            encoder_text_ids, output_hidden_states=True, attention_mask=attention_mask)\n",
    "\n",
    "        hs_tuple = encoder_states[\"hidden_states\"]\n",
    "\n",
    "        mean_pooled_all_layers = []\n",
    "\n",
    "        for layer, hs in enumerate(hs_tuple):\n",
    "            ic(hs_tuple[layer].size())\n",
    "            # hs = hs_tuple[layer] # (batch_size x sequence_length x dimension)\n",
    "            hs_masked = hs * attention_mask[:, :, None] # ideally zeros out the pad associated representations\n",
    "            ic(hs_masked.size())\n",
    "            seq_lengths = attention_mask.sum(dim=1) # each line here represents sequence length\n",
    "\n",
    "            hs_masked_sum = hs_masked.sum(dim=1)\n",
    "            hs_avg = hs_masked_sum / seq_lengths[:, None]\n",
    "            mean_pooled_all_layers.append(hs_avg)\n",
    "\n",
    "        return mean_pooled_all_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset data_xor/default to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_xor/default/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a120d84ec54978bae8029295e5b2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b829d4a667f4ad493a422b6c68a65cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b6c95243204e878ebb6ed3cb19be03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset data_xor downloaded and prepared to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_xor/default/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120bc83770054cacbb7b3c0af177dde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "model_wrapped = RobertaWrapper(device=\"cuda\")\n",
    "test_dataset_xor = datasets.load_dataset(\"data_scripts/data_xor.py\", add_sep=False)[\"train\"]\n",
    "# test_dataloader = DataLoader(test_dataset_xor, batch_size=3)\n",
    "\n",
    "# print(next(iter(test_dataloader)))\n",
    "# output = model_wrapped(next(iter(test_dataloader))[\"content\"])\n",
    "# print(len(output))\n",
    "# print(output[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states_many_examples(model, data, n=100, layer=-1, batch_size=1, query_column=\"content\"):\n",
    "    \"\"\"\n",
    "    Takes a bunch of sequences and runs them through RoBERTa to generate the mean-pooled hidden states.\n",
    "\n",
    "    This is unbatched and kept inefficient for simplicity\n",
    "\n",
    "    can be done in batches on a GPU to make it faster\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_hidden_states, all_labels = [], []\n",
    "    # all_hidden_states: will have elements for each RoBERTa layer, each element represents the mean-pooled representations for the whole data at that layer\n",
    "\n",
    "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # loop\n",
    "    # for idx in tqdm(range(n)):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        if ((i+1) * batch_size) > n:\n",
    "            break\n",
    "        text, true_label = batch[query_column], batch[\"label\"].to(model.device)\n",
    "        ic(text)\n",
    "        ic(true_label)\n",
    "\n",
    "\n",
    "        # get hidden states\n",
    "        with torch.no_grad():\n",
    "            outs = model(text)\n",
    "        # outs: [hidden states]\n",
    "        ic(outs[0].size())\n",
    "\n",
    "        # initialize if empty\n",
    "        if len(all_hidden_states) == 0:\n",
    "            for i in range(len(outs)):\n",
    "                all_hidden_states.append([])\n",
    "\n",
    "\n",
    "        # collect\n",
    "        for i, hidden_state in enumerate(outs):\n",
    "            all_hidden_states[i].append(hidden_state)\n",
    "\n",
    "        all_labels.append(true_label)\n",
    "\n",
    "    ic(len(all_hidden_states))\n",
    "    ic(len(all_hidden_states[0]))\n",
    "    ic(all_hidden_states[0][0].size())\n",
    "    ic(torch.cat(all_hidden_states[0], dim=0).size())\n",
    "\n",
    "    all_hidden_states = [torch.cat(all_hidden_states[i], dim=0) for i in range(len(all_hidden_states))]\n",
    "\n",
    "\n",
    "    return all_hidden_states, torch.cat(all_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "ic.disable()\n",
    "outs = get_hidden_states_many_examples(model_wrapped, test_dataset_xor, n=4, batch_size=2)\n",
    "print(len(outs[0]))\n",
    "print(outs[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_across_layers(experiment, train_input, train_labels, test_input, test_labels):\n",
    "    \"\"\"\n",
    "    Runs a probing experiment over representations from all layers of the model.\n",
    "    The whole thing works on cached embeddings\n",
    "\n",
    "    experiment: method (train: Tensor, test: Tensor, label_train: Tensor, label_test: Tensor) -> (fit_model, metrics). Each experiment will fit _some_ model on the data and return the model and the results\n",
    "    train_input: list of 13 elements, each of which is a tensor of size (num_datapoints, embedding_dim)\n",
    "    train_labels: tensor (num_datapoints, )\n",
    "    test_input: same format as train_input\n",
    "    test_labels: same format as train_labels\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_results = []\n",
    "    list_of_probing_models = []\n",
    "\n",
    "    for i in range(len(train_input)):\n",
    "        train_current_layer = train_input[i]\n",
    "        test_current_layer = test_input[i]\n",
    "\n",
    "        model, metrics = experiment(train_current_layer, test_current_layer, train_labels, test_labels)\n",
    "\n",
    "        list_of_results.append(metrics)\n",
    "        list_of_probing_models.append(model)\n",
    "\n",
    "    return list_of_probing_models, list_of_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_experiment(train_input, test_input, train_labels, test_labels, probe_model):\n",
    "    \"\"\"\n",
    "    Gets an initialized probe model and fits it on data and runs some experiments\n",
    "    expected to be curried and sent as a callback to run_experiment_across_layers\n",
    "    \"\"\"\n",
    "\n",
    "    train_input_numpy = train_input.detach().cpu().numpy()\n",
    "    test_input_numpy = test_input.detach().cpu().numpy()\n",
    "    ic(train_labels.size())\n",
    "    train_labels_numpy = train_labels.detach().cpu().numpy()\n",
    "    test_labels_numpy = test_labels.detach().cpu().numpy()\n",
    "\n",
    "    probe_model.fit(train_input_numpy, train_labels_numpy)\n",
    "\n",
    "    accuracy = probe_model.score(test_input_numpy, test_labels_numpy)\n",
    "\n",
    "    return probe_model, {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def linear_probe_experiment(train_input, test_input, train_labels, test_labels):\n",
    "    # initialize linear probe and run probe experiment\n",
    "    lr = LogisticRegression(class_weight=\"balanced\", verbose=1, max_iter=1000)\n",
    "    return probe_experiment(train_input, test_input, train_labels, test_labels, lr)\n",
    "\n",
    "\n",
    "def mlp_probe_experiment(train_input, test_input, train_labels, test_labels):\n",
    "    # initialize an mlp probe and run probe experiment\n",
    "    mlp = MLPClassifier(random_state=1, max_iter=1000, verbose=True, hidden_layer_sizes=(300,))\n",
    "    return probe_experiment(train_input, test_input, train_labels, test_labels, mlp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report_all_layers(input_all_layers, labels, models):\n",
    "    \"\"\"\n",
    "    input_all_layers is a list of 13 layers\n",
    "    labels is a tensor\n",
    "    \"\"\"\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    preds_for_all_layers = [models[i].predict(input_for_layer.detach().cpu().numpy()) for i, input_for_layer in enumerate(input_all_layers)]\n",
    "    classification_reports = [classification_report(y_true=labels, y_pred=pred) for pred in preds_for_all_layers]\n",
    "    return classification_reports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The XOR experiment\n",
    "\n",
    "We have negated and non-negated versions of the same propositions in LAMA\n",
    "\n",
    "- A: Einstein was born in Austria\n",
    "- A': Eisntein was not born in Austria\n",
    "\n",
    "We make the following combinations\n",
    "- AA\n",
    "- AA'\n",
    "- A'A\n",
    "- A'A'\n",
    "\n",
    "We classify contradictory statements together and non-contradictory statements together\n",
    "\n",
    "The point is to ask if a linear regression can seperate out this classification, the point is to look if the model is doing compositionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Old caching folder {'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts'}/data_xor/default/0.0.0 for dataset data_xor exists but not data were found. Removing it. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset data_xor/default to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts'}/data_xor/default/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25e1072072a41da97b91105e20477f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ac4875be84ed1a217d2d0684eb8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55abfd9afafd4be488b376982c6b30d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset data_xor downloaded and prepared to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts'}/data_xor/default/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056ff702dc714d1d841c887099a09f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_xor (/home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts'}/data_xor/default/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca5c671435547b28948a0a99334aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_xor (/home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts'}/data_xor/default/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1957e3f20f4af78811b47823ede834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ic.disable()\n",
    "model_wrapped = RobertaWrapper(device=\"cuda\")\n",
    "train_dataset_xor = datasets.load_dataset(\"data_scripts/data_xor.py\")[\"train\"]\n",
    "test_dataset_xor = datasets.load_dataset(\"data_scripts/data_xor.py\")[\"test\"]\n",
    "dev_dataset_xor = datasets.load_dataset(\"data_scripts/data_xor.py\")[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = get_hidden_states_many_examples(model_wrapped, train_dataset_xor, n=248, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test_labels = get_hidden_states_many_examples(model_wrapped, test_dataset_xor, n=1000, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(len(test_labels))\n",
    "ic(test_labels.size())\n",
    "ic(train_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  2.12590D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769     38     47      1     0     0   3.904D-03   1.385D+02\n",
      "  F =   138.46310439172152     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  7.68951D-01\n",
      "\n",
      "At iterate   50    f=  1.36585D+02    |proj g|=  4.58511D-02\n",
      "\n",
      "At iterate  100    f=  1.36511D+02    |proj g|=  2.77650D-02\n",
      "\n",
      "At iterate  150    f=  1.36505D+02    |proj g|=  2.54826D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    169    201      1     0     0   5.266D-02   1.365D+02\n",
      "  F =   136.50446600493490     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  3.91709D-01\n",
      "\n",
      "At iterate   50    f=  1.36682D+02    |proj g|=  3.51229D-01\n",
      "\n",
      "At iterate  100    f=  1.36677D+02    |proj g|=  1.72780D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    129    151      1     0     0   5.417D-02   1.367D+02\n",
      "  F =   136.67684528559542     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  5.53660D-01\n",
      "\n",
      "At iterate   50    f=  1.30284D+02    |proj g|=  1.67079D+00\n",
      "\n",
      "At iterate  100    f=  1.30276D+02    |proj g|=  2.17239D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    136    160      1     0     0   1.518D-02   1.303D+02\n",
      "  F =   130.27584075334329     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  1.30746D+00\n",
      "\n",
      "At iterate   50    f=  1.12543D+02    |proj g|=  1.29317D-01\n",
      "\n",
      "At iterate  100    f=  1.12529D+02    |proj g|=  9.06431D-02\n",
      "\n",
      "At iterate  150    f=  1.12529D+02    |proj g|=  8.27823D-02\n",
      "\n",
      "At iterate  200    f=  1.12526D+02    |proj g|=  2.41170D-02\n",
      "\n",
      "At iterate  250    f=  1.12520D+02    |proj g|=  2.52687D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    263    314      1     0     0   3.774D-02   1.125D+02\n",
      "  F =   112.51994812648996     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  2.64556D+00\n",
      "\n",
      "At iterate   50    f=  8.23983D+01    |proj g|=  3.32398D-01\n",
      "\n",
      "At iterate  100    f=  8.23499D+01    |proj g|=  4.86610D-02\n",
      "\n",
      "At iterate  150    f=  8.23397D+01    |proj g|=  1.74758D+00\n",
      "\n",
      "At iterate  200    f=  8.21291D+01    |proj g|=  1.08113D+00\n",
      "\n",
      "At iterate  250    f=  8.20855D+01    |proj g|=  5.22367D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    285    335      1     0     0   3.437D-03   8.209D+01\n",
      "  F =   82.085027624015950     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  6.93641D+00\n",
      "\n",
      "At iterate   50    f=  6.18789D+01    |proj g|=  6.93310D-01\n",
      "\n",
      "At iterate  100    f=  6.18741D+01    |proj g|=  2.37535D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  150    f=  6.18601D+01    |proj g|=  1.09283D+00\n",
      "\n",
      "At iterate  200    f=  6.17714D+01    |proj g|=  3.58208D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    216    266      1     0     0   3.890D-03   6.177D+01\n",
      "  F =   61.771385630659708     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  7.19091D+00\n",
      "\n",
      "At iterate   50    f=  5.28761D+01    |proj g|=  3.30486D+00\n",
      "\n",
      "At iterate  100    f=  5.28499D+01    |proj g|=  4.81271D-02\n",
      "\n",
      "At iterate  150    f=  5.28454D+01    |proj g|=  3.00523D-01\n",
      "\n",
      "At iterate  200    f=  5.27528D+01    |proj g|=  8.12982D-02\n",
      "\n",
      "At iterate  250    f=  5.27486D+01    |proj g|=  1.59318D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    263    318      1     0     0   1.859D-02   5.275D+01\n",
      "  F =   52.748559234917053     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  8.88887D+00\n",
      "\n",
      "At iterate   50    f=  4.28386D+01    |proj g|=  5.82716D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  100    f=  4.27427D+01    |proj g|=  2.05955D+00\n",
      "\n",
      "At iterate  150    f=  4.26421D+01    |proj g|=  2.58904D-01\n",
      "\n",
      "At iterate  200    f=  4.26393D+01    |proj g|=  1.80179D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    202    240      1     0     0   8.666D-03   4.264D+01\n",
      "  F =   42.639252053255255     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  7.36890D+00\n",
      "\n",
      "At iterate   50    f=  4.19332D+01    |proj g|=  1.02934D-01\n",
      "\n",
      "At iterate  100    f=  4.18289D+01    |proj g|=  5.73846D-03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    122    144      1     0     0   5.178D-03   4.183D+01\n",
      "  F =   41.828825780096629     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  9.45586D+00\n",
      "\n",
      "At iterate   50    f=  4.80223D+01    |proj g|=  4.51473D-01\n",
      "\n",
      "At iterate  100    f=  4.76895D+01    |proj g|=  4.53148D-01\n",
      "\n",
      "At iterate  150    f=  4.76810D+01    |proj g|=  1.38989D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    165    188      1     0     0   1.007D-02   4.768D+01\n",
      "  F =   47.680962687958100     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  5.34123D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  5.12693D+01    |proj g|=  9.64947D-01\n",
      "\n",
      "At iterate  100    f=  5.12650D+01    |proj g|=  4.08407D-02\n",
      "\n",
      "At iterate  150    f=  5.12330D+01    |proj g|=  3.53120D-01\n",
      "\n",
      "At iterate  200    f=  5.11456D+01    |proj g|=  3.04957D+00\n",
      "\n",
      "At iterate  250    f=  5.11392D+01    |proj g|=  9.01741D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    258    303      1     0     0   2.090D-02   5.114D+01\n",
      "  F =   51.139197386719644     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+02    |proj g|=  2.55190D+00\n",
      "\n",
      "At iterate   50    f=  8.76512D+01    |proj g|=  5.32139D-01\n",
      "\n",
      "At iterate  100    f=  8.74313D+01    |proj g|=  1.32486D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    141    174      1     0     0   1.127D-03   8.743D+01\n",
      "  F =   87.430339696087401     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "models, accuracies = run_experiment_across_layers(linear_probe_experiment, train, train_labels, test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_xor_no_control = generate_classification_report_all_layers(test, test_labels, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.55       500\n",
      "           1       0.56      0.57      0.57       500\n",
      "\n",
      "    accuracy                           0.56      1000\n",
      "   macro avg       0.56      0.56      0.56      1000\n",
      "weighted avg       0.56      0.56      0.56      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.56      0.56       500\n",
      "           1       0.56      0.57      0.57       500\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.57      0.57      0.57      1000\n",
      "weighted avg       0.57      0.57      0.57      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58       500\n",
      "           1       0.57      0.55      0.56       500\n",
      "\n",
      "    accuracy                           0.57      1000\n",
      "   macro avg       0.57      0.57      0.57      1000\n",
      "weighted avg       0.57      0.57      0.57      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_no_control[:3]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71       500\n",
      "           1       0.72      0.65      0.68       500\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.70      0.70      0.70      1000\n",
      "weighted avg       0.70      0.70      0.70      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.83      0.81       500\n",
      "           1       0.82      0.79      0.81       500\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.81      0.81      0.81      1000\n",
      "weighted avg       0.81      0.81      0.81      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       500\n",
      "           1       0.88      0.87      0.88       500\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_no_control[3:6]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       500\n",
      "           1       0.91      0.90      0.91       500\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       500\n",
      "           1       0.95      0.91      0.93       500\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.93      0.93      0.93      1000\n",
      "weighted avg       0.93      0.93      0.93      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       500\n",
      "           1       0.96      0.95      0.95       500\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.96      0.95      0.95      1000\n",
      "weighted avg       0.96      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_no_control[6:9]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       500\n",
      "           1       0.93      0.96      0.94       500\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.94      0.94      0.94      1000\n",
      "weighted avg       0.94      0.94      0.94      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94       500\n",
      "           1       0.93      0.96      0.95       500\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.95      0.95      0.94      1000\n",
      "weighted avg       0.95      0.94      0.94      1000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       500\n",
      "           1       0.89      0.95      0.92       500\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.92      0.92      0.92      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_no_control[-3:]:\n",
    "    print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Experiment\n",
    "replace negation with gibberish and run the linear probes for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_xor (/home2/shashwat.s/main_thread/negation_new/{'cache_dir': 'control_task', 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'd2a449d4e373869e0295a5e1aff1c8052abb878b8e40f9c2241d9639b4a4bf72', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'task_num': 1}/data_xor/default/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088dc97751f7443386e07aeb5436b8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_dataset_xor_1 = datasets.load_dataset(\"data_scripts/data_xor.py\", task_num=1, cache_dir=\"control_task\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_control1, test_labels_control1 = get_hidden_states_many_examples(model_wrapped, test_dataset_xor_1, n=10000, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_control1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_xor_control_1 = generate_classification_report_all_layers(test_control1, test_labels_control1, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66      3350\n",
      "           1       0.50      0.04      0.08      3350\n",
      "\n",
      "    accuracy                           0.50      6700\n",
      "   macro avg       0.50      0.50      0.37      6700\n",
      "weighted avg       0.50      0.50      0.37      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.70      0.59      3350\n",
      "           1       0.53      0.33      0.41      3350\n",
      "\n",
      "    accuracy                           0.52      6700\n",
      "   macro avg       0.52      0.52      0.50      6700\n",
      "weighted avg       0.52      0.52      0.50      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56      3350\n",
      "           1       0.55      0.53      0.54      3350\n",
      "\n",
      "    accuracy                           0.55      6700\n",
      "   macro avg       0.55      0.55      0.55      6700\n",
      "weighted avg       0.55      0.55      0.55      6700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_control_1[:3]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.75      3350\n",
      "           1       0.76      0.69      0.72      3350\n",
      "\n",
      "    accuracy                           0.74      6700\n",
      "   macro avg       0.74      0.74      0.74      6700\n",
      "weighted avg       0.74      0.74      0.74      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79      3350\n",
      "           1       0.80      0.76      0.78      3350\n",
      "\n",
      "    accuracy                           0.78      6700\n",
      "   macro avg       0.79      0.78      0.78      6700\n",
      "weighted avg       0.79      0.78      0.78      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.89      0.81      3350\n",
      "           1       0.86      0.69      0.76      3350\n",
      "\n",
      "    accuracy                           0.79      6700\n",
      "   macro avg       0.80      0.79      0.79      6700\n",
      "weighted avg       0.80      0.79      0.79      6700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_control_1[3:6]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.96      0.79      3350\n",
      "           1       0.93      0.53      0.67      3350\n",
      "\n",
      "    accuracy                           0.74      6700\n",
      "   macro avg       0.80      0.74      0.73      6700\n",
      "weighted avg       0.80      0.74      0.73      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83      3350\n",
      "           1       0.94      0.65      0.77      3350\n",
      "\n",
      "    accuracy                           0.80      6700\n",
      "   macro avg       0.84      0.80      0.80      6700\n",
      "weighted avg       0.84      0.80      0.80      6700\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.89      0.75      3350\n",
      "           1       0.82      0.50      0.62      3350\n",
      "\n",
      "    accuracy                           0.70      6700\n",
      "   macro avg       0.73      0.70      0.68      6700\n",
      "weighted avg       0.73      0.70      0.68      6700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports_xor_control_1[-3:]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "os.statvfs_result(f_bsize=1048576, f_frsize=1048576, f_blocks=34191575, f_bfree=9798649, f_bavail=8082217, f_files=549253120, f_ffree=360303603, f_favail=360303603, f_flag=4096, f_namemax=255)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "os.statvfs(datasets.config.HF_DATASETS_CACHE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negation Detection\n",
    "\n",
    "Very dumb experiment, checking for negation in the statement (from mean-pooled reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_detect_neg (/home/mrcreator/research/main_thread/fresh_repo/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': '1b53f52a30ee637c38ac67e920770d58d6c1da4f3bed41fbe8442d79b6881e56', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_detect_neg/default/0.0.0)\n",
      "100%|| 3/3 [00:00<00:00, 671.12it/s]\n",
      "Found cached dataset data_detect_neg (/home/mrcreator/research/main_thread/fresh_repo/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': '1b53f52a30ee637c38ac67e920770d58d6c1da4f3bed41fbe8442d79b6881e56', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_detect_neg/default/0.0.0)\n",
      "100%|| 3/3 [00:00<00:00, 509.80it/s]\n",
      "Found cached dataset data_detect_neg (/home/mrcreator/research/main_thread/fresh_repo/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': '1b53f52a30ee637c38ac67e920770d58d6c1da4f3bed41fbe8442d79b6881e56', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_detect_neg/default/0.0.0)\n",
      "100%|| 3/3 [00:00<00:00, 425.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset_neg_detect = datasets.load_dataset(\"data_scripts/data_detect_neg.py\", add_sep=False)[\"train\"]\n",
    "test_dataset_neg_detect = datasets.load_dataset(\"data_scripts/data_detect_neg.py\", add_sep=False)[\"test\"]\n",
    "dev_dataset_neg_detect = datasets.load_dataset(\"data_scripts/data_detect_neg.py\", add_sep=False)[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detect_neg, train_labels_det_neg = get_hidden_states_many_examples(model_wrapped, train_dataset_neg_detect, n=2000, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detect_neg, test_labels_det_neg = get_hidden_states_many_examples(model_wrapped, test_dataset_neg_detect, n=10000, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  3.32225D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  2.04545D+02    |proj g|=  7.64498D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769     77     90      1     0     0   9.888D-04   2.045D+02\n",
      "  F =   204.53285633775937     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  6.85403D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  1.24328D+02    |proj g|=  2.31646D+00\n",
      "\n",
      "At iterate  100    f=  1.24240D+02    |proj g|=  4.26313D-02\n",
      "\n",
      "At iterate  150    f=  1.24191D+02    |proj g|=  1.07950D+00\n",
      "\n",
      "At iterate  200    f=  1.24187D+02    |proj g|=  1.55634D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    221    257      1     0     0   8.335D-04   1.242D+02\n",
      "  F =   124.18733996052116     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  8.76241D+01\n",
      "\n",
      "At iterate   50    f=  1.18364D+02    |proj g|=  1.78367D+00\n",
      "\n",
      "At iterate  100    f=  1.18296D+02    |proj g|=  2.40666D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    135    152      1     0     0   1.686D-02   1.183D+02\n",
      "  F =   118.29520162589269     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  9.52775D+01\n",
      "\n",
      "At iterate   50    f=  1.16833D+02    |proj g|=  1.88923D+00\n",
      "\n",
      "At iterate  100    f=  1.16655D+02    |proj g|=  1.36041D+00\n",
      "\n",
      "At iterate  150    f=  1.16652D+02    |proj g|=  2.10000D-01\n",
      "\n",
      "At iterate  200    f=  1.16599D+02    |proj g|=  2.52958D+00\n",
      "\n",
      "At iterate  250    f=  1.16533D+02    |proj g|=  2.55485D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    288    338      1     0     0   1.466D-03   1.165D+02\n",
      "  F =   116.53231400271187     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  9.66319D+01\n",
      "\n",
      "At iterate   50    f=  1.12514D+02    |proj g|=  4.09364D+00\n",
      "\n",
      "At iterate  100    f=  1.12424D+02    |proj g|=  3.34658D-02\n",
      "\n",
      "At iterate  150    f=  1.12392D+02    |proj g|=  2.10947D+00\n",
      "\n",
      "At iterate  200    f=  1.12299D+02    |proj g|=  6.76007D-02\n",
      "\n",
      "At iterate  250    f=  1.12291D+02    |proj g|=  3.16846D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    261    302      1     0     0   2.026D-02   1.123D+02\n",
      "  F =   112.29124508297348     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.05673D+02\n",
      "\n",
      "At iterate   50    f=  9.97154D+01    |proj g|=  1.16181D+00\n",
      "\n",
      "At iterate  100    f=  9.94567D+01    |proj g|=  3.38586D-01\n",
      "\n",
      "At iterate  150    f=  9.94530D+01    |proj g|=  7.67354D-02\n",
      "\n",
      "At iterate  200    f=  9.93549D+01    |proj g|=  1.76517D+00\n",
      "\n",
      "At iterate  250    f=  9.92998D+01    |proj g|=  6.80629D-01\n",
      "\n",
      "At iterate  300    f=  9.92980D+01    |proj g|=  2.56711D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    300    355      1     0     0   2.567D-02   9.930D+01\n",
      "  F =   99.298039687483652     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.15866D+02\n",
      "\n",
      "At iterate   50    f=  9.87568D+01    |proj g|=  1.97864D-01\n",
      "\n",
      "At iterate  100    f=  9.87074D+01    |proj g|=  1.22188D-01\n",
      "\n",
      "At iterate  150    f=  9.86906D+01    |proj g|=  1.64428D+00\n",
      "\n",
      "At iterate  200    f=  9.85642D+01    |proj g|=  8.29998D-02\n",
      "\n",
      "At iterate  250    f=  9.85567D+01    |proj g|=  1.07981D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    266    304      1     0     0   2.525D-03   9.856D+01\n",
      "  F =   98.556615830188392     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.10859D+02\n",
      "\n",
      "At iterate   50    f=  9.56001D+01    |proj g|=  4.32750D+00\n",
      "\n",
      "At iterate  100    f=  9.53668D+01    |proj g|=  3.06772D-01\n",
      "\n",
      "At iterate  150    f=  9.53629D+01    |proj g|=  6.56175D-01\n",
      "\n",
      "At iterate  200    f=  9.53334D+01    |proj g|=  4.24689D+00\n",
      "\n",
      "At iterate  250    f=  9.51900D+01    |proj g|=  1.80423D+00\n",
      "\n",
      "At iterate  300    f=  9.51655D+01    |proj g|=  2.39485D-01\n",
      "\n",
      "At iterate  350    f=  9.51651D+01    |proj g|=  1.57215D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    352    420      1     0     0   3.563D-02   9.517D+01\n",
      "  F =   95.165074383945154     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.18782D+02\n",
      "\n",
      "At iterate   50    f=  9.46269D+01    |proj g|=  4.35112D+00\n",
      "\n",
      "At iterate  100    f=  9.43411D+01    |proj g|=  8.65839D-01\n",
      "\n",
      "At iterate  150    f=  9.43328D+01    |proj g|=  3.43572D-01\n",
      "\n",
      "At iterate  200    f=  9.43028D+01    |proj g|=  2.45027D+00\n",
      "\n",
      "At iterate  250    f=  9.41723D+01    |proj g|=  4.84260D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    291    343      1     0     0   2.990D-02   9.416D+01\n",
      "  F =   94.162954357943121     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.24954D+02\n",
      "\n",
      "At iterate   50    f=  9.40611D+01    |proj g|=  2.12306D+00\n",
      "\n",
      "At iterate  100    f=  9.38564D+01    |proj g|=  1.34114D-01\n",
      "\n",
      "At iterate  150    f=  9.38533D+01    |proj g|=  1.49877D-01\n",
      "\n",
      "At iterate  200    f=  9.38198D+01    |proj g|=  4.14386D-01\n",
      "\n",
      "At iterate  250    f=  9.37334D+01    |proj g|=  1.00106D+00\n",
      "\n",
      "At iterate  300    f=  9.37281D+01    |proj g|=  2.18188D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    308    358      1     0     0   1.462D-02   9.373D+01\n",
      "  F =   93.728067621968592     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.23173D+02\n",
      "\n",
      "At iterate   50    f=  9.79266D+01    |proj g|=  8.36408D+00\n",
      "\n",
      "At iterate  100    f=  9.75858D+01    |proj g|=  1.26014D-01\n",
      "\n",
      "At iterate  150    f=  9.75823D+01    |proj g|=  1.26635D-01\n",
      "\n",
      "At iterate  200    f=  9.75009D+01    |proj g|=  7.56717D-01\n",
      "\n",
      "At iterate  250    f=  9.74839D+01    |proj g|=  1.18486D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    265    315      1     0     0   3.769D-02   9.748D+01\n",
      "  F =   97.483860475440594     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.33309D+02\n",
      "\n",
      "At iterate   50    f=  1.11224D+02    |proj g|=  9.15769D-01\n",
      "\n",
      "At iterate  100    f=  1.10809D+02    |proj g|=  1.17546D+00\n",
      "\n",
      "At iterate  150    f=  1.10801D+02    |proj g|=  4.81832D-02\n",
      "\n",
      "At iterate  200    f=  1.10794D+02    |proj g|=  3.47335D-01\n",
      "\n",
      "At iterate  250    f=  1.10709D+02    |proj g|=  5.41799D-01\n",
      "\n",
      "At iterate  300    f=  1.10635D+02    |proj g|=  1.88228D-01\n",
      "\n",
      "At iterate  350    f=  1.10633D+02    |proj g|=  7.05715D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    366    428      1     0     0   1.870D-02   1.106D+02\n",
      "  F =   110.63293522128515     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  6.48358D+01\n",
      "\n",
      "At iterate   50    f=  2.27067D+02    |proj g|=  2.24334D+00\n",
      "\n",
      "At iterate  100    f=  2.26869D+02    |proj g|=  1.10526D+00\n",
      "\n",
      "At iterate  150    f=  2.26733D+02    |proj g|=  4.45661D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    170    203      1     0     0   1.872D-02   2.267D+02\n",
      "  F =   226.73187817552341     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1),\n",
       "  LogisticRegression(class_weight='balanced', max_iter=1000, verbose=1)],\n",
       " [{'accuracy': 0.9832},\n",
       "  {'accuracy': 0.9828},\n",
       "  {'accuracy': 0.9832},\n",
       "  {'accuracy': 0.9831},\n",
       "  {'accuracy': 0.9831},\n",
       "  {'accuracy': 0.983},\n",
       "  {'accuracy': 0.9832},\n",
       "  {'accuracy': 0.9833},\n",
       "  {'accuracy': 0.9833},\n",
       "  {'accuracy': 0.9833},\n",
       "  {'accuracy': 0.9834},\n",
       "  {'accuracy': 0.9833},\n",
       "  {'accuracy': 0.983}])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment_across_layers(linear_probe_experiment, train_detect_neg, train_labels_det_neg, test_detect_neg, test_labels_det_neg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negation Consistency Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old caching folder {'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'b61b143f8583c778d436a1730d12372367c64ff9c71127a6af2f230568607fe8', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_negation_consistency/default/0.0.0 for dataset data_negation_consistency exists but not data were found. Removing it. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset data_negation_consistency/default to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'b61b143f8583c778d436a1730d12372367c64ff9c71127a6af2f230568607fe8', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_negation_consistency/default/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0723c87cb9f44db69f05bf58c511d1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351771200bc549bd800a417ab044e9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d1a5e513194a2695d3b4d4797d318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset data_negation_consistency downloaded and prepared to /home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'b61b143f8583c778d436a1730d12372367c64ff9c71127a6af2f230568607fe8', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_negation_consistency/default/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178ef47a744a41b8aa53016e5c79bc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_negation_consistency (/home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'b61b143f8583c778d436a1730d12372367c64ff9c71127a6af2f230568607fe8', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_negation_consistency/default/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f697482b8c466dac85afe14e6089c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset data_negation_consistency (/home2/shashwat.s/main_thread/negation_new/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': 'b61b143f8583c778d436a1730d12372367c64ff9c71127a6af2f230568607fe8', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_negation_consistency/default/0.0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253463bfb29e4d7ebe1f628f6663d18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_neg_consistency = datasets.load_dataset(\"data_scripts/data_negation_consistency.py\", add_sep=False)[\"train\"]\n",
    "test_dataset_neg_consistency = datasets.load_dataset(\"data_scripts/data_negation_consistency.py\", add_sep=False)[\"test\"]\n",
    "dev_dataset_neg_consistency = datasets.load_dataset(\"data_scripts/data_negation_consistency.py\", add_sep=False)[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23450\n",
      "10066\n",
      "0.4292537313432836\n"
     ]
    }
   ],
   "source": [
    "# checking for statistics\n",
    "len(train_dataset_neg_consistency['label'])\n",
    "total = 0\n",
    "ones = 0\n",
    "\n",
    "for label in train_dataset_neg_consistency['label']:\n",
    "    total += 1\n",
    "    ones += label\n",
    "\n",
    "print(total)\n",
    "print(ones)\n",
    "print(ones/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5026\n",
      "2160\n",
      "0.4297652208515718\n"
     ]
    }
   ],
   "source": [
    "# checking for statistics\n",
    "len(test_dataset_neg_consistency['label'])\n",
    "total = 0\n",
    "ones = 0\n",
    "\n",
    "for label in test_dataset_neg_consistency['label']:\n",
    "    total += 1\n",
    "    ones += label\n",
    "\n",
    "print(total)\n",
    "print(ones)\n",
    "print(ones/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_dataset_neg_consistency['subject']).intersection(set(test_dataset_neg_consistency['subject'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detect_neg_consistency, train_labels_det_neg_consistency = get_hidden_states_many_examples(model_wrapped, train_dataset_neg_consistency, n=23000, batch_size=100, query_column=\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detect_neg_consistency, test_labels_det_neg_consistency = get_hidden_states_many_examples(model_wrapped, test_dataset_neg_consistency, n=5000, batch_size=100, query_column=\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  2.73022D+01\n",
      "\n",
      "At iterate   50    f=  9.52618D+02    |proj g|=  7.16117D-01\n",
      "\n",
      "At iterate  100    f=  9.51333D+02    |proj g|=  2.73071D-01\n",
      "\n",
      "At iterate  150    f=  9.51328D+02    |proj g|=  1.97448D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    166    193      1     0     0   3.737D-03   9.513D+02\n",
      "  F =   951.32789911322618     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  6.49952D+01\n",
      "\n",
      "At iterate   50    f=  8.71338D+02    |proj g|=  2.90839D+01\n",
      "\n",
      "At iterate  100    f=  8.62683D+02    |proj g|=  9.23039D+00\n",
      "\n",
      "At iterate  150    f=  8.62314D+02    |proj g|=  6.76100D-01\n",
      "\n",
      "At iterate  200    f=  8.62284D+02    |proj g|=  1.71411D+00\n",
      "\n",
      "At iterate  250    f=  8.62259D+02    |proj g|=  1.01424D+00\n",
      "\n",
      "At iterate  300    f=  8.62191D+02    |proj g|=  1.09729D+00\n",
      "\n",
      "At iterate  350    f=  8.62130D+02    |proj g|=  1.14203D+00\n",
      "\n",
      "At iterate  400    f=  8.62085D+02    |proj g|=  6.22453D-01\n",
      "\n",
      "At iterate  450    f=  8.62079D+02    |proj g|=  2.11507D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    475    564      1     0     0   1.006D-01   8.621D+02\n",
      "  F =   862.07895026491610     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  8.84720D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  9.01576D+02    |proj g|=  9.19384D+01\n",
      "\n",
      "At iterate  100    f=  8.87601D+02    |proj g|=  1.69773D+01\n",
      "\n",
      "At iterate  150    f=  8.86061D+02    |proj g|=  7.19906D-01\n",
      "\n",
      "At iterate  200    f=  8.85713D+02    |proj g|=  1.43419D+01\n",
      "\n",
      "At iterate  250    f=  8.85675D+02    |proj g|=  2.25672D-01\n",
      "\n",
      "At iterate  300    f=  8.85664D+02    |proj g|=  4.26744D-01\n",
      "\n",
      "At iterate  350    f=  8.85649D+02    |proj g|=  1.12485D+00\n",
      "\n",
      "At iterate  400    f=  8.85606D+02    |proj g|=  1.80670D-01\n",
      "\n",
      "At iterate  450    f=  8.85521D+02    |proj g|=  3.69300D+00\n",
      "\n",
      "At iterate  500    f=  8.85462D+02    |proj g|=  1.92564D+00\n",
      "\n",
      "At iterate  550    f=  8.85436D+02    |proj g|=  3.13820D-01\n",
      "\n",
      "At iterate  600    f=  8.85433D+02    |proj g|=  1.38952D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    625    752      1     0     0   3.130D-01   8.854D+02\n",
      "  F =   885.43275087100380     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  8.06261D+01\n",
      "\n",
      "At iterate   50    f=  9.45977D+02    |proj g|=  1.65521D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  100    f=  8.87903D+02    |proj g|=  3.93771D+01\n",
      "\n",
      "At iterate  150    f=  8.85338D+02    |proj g|=  4.49524D+00\n",
      "\n",
      "At iterate  200    f=  8.84884D+02    |proj g|=  8.12346D+00\n",
      "\n",
      "At iterate  250    f=  8.84783D+02    |proj g|=  6.64434D+00\n",
      "\n",
      "At iterate  300    f=  8.84761D+02    |proj g|=  1.40662D-01\n",
      "\n",
      "At iterate  350    f=  8.84743D+02    |proj g|=  2.50080D+00\n",
      "\n",
      "At iterate  400    f=  8.84687D+02    |proj g|=  1.16808D+00\n",
      "\n",
      "At iterate  450    f=  8.84634D+02    |proj g|=  6.42694D+00\n",
      "\n",
      "At iterate  500    f=  8.84451D+02    |proj g|=  5.44426D+00\n",
      "\n",
      "At iterate  550    f=  8.84223D+02    |proj g|=  8.24968D+00\n",
      "\n",
      "At iterate  600    f=  8.84126D+02    |proj g|=  1.22415D+00\n",
      "\n",
      "At iterate  650    f=  8.84098D+02    |proj g|=  8.69084D-01\n",
      "\n",
      "At iterate  700    f=  8.84089D+02    |proj g|=  1.53391D+00\n",
      "\n",
      "At iterate  750    f=  8.84085D+02    |proj g|=  8.74724D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    777    913      1     0     0   1.004D-01   8.841D+02\n",
      "  F =   884.08492950658308     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  7.84512D+01\n",
      "\n",
      "At iterate   50    f=  9.58128D+02    |proj g|=  4.64250D+01\n",
      "\n",
      "At iterate  100    f=  9.27130D+02    |proj g|=  7.25583D+01\n",
      "\n",
      "At iterate  150    f=  9.14216D+02    |proj g|=  4.79869D+00\n",
      "\n",
      "At iterate  200    f=  9.12305D+02    |proj g|=  1.10416D+01\n",
      "\n",
      "At iterate  250    f=  9.12125D+02    |proj g|=  6.78876D+00\n",
      "\n",
      "At iterate  300    f=  9.12089D+02    |proj g|=  7.69401D+00\n",
      "\n",
      "At iterate  350    f=  9.12075D+02    |proj g|=  6.62752D-01\n",
      "\n",
      "At iterate  400    f=  9.12039D+02    |proj g|=  4.75065D+00\n",
      "\n",
      "At iterate  450    f=  9.11974D+02    |proj g|=  6.32191D-01\n",
      "\n",
      "At iterate  500    f=  9.11811D+02    |proj g|=  6.63213D+00\n",
      "\n",
      "At iterate  550    f=  9.11540D+02    |proj g|=  2.79364D+01\n",
      "\n",
      "At iterate  600    f=  9.11392D+02    |proj g|=  2.10869D+00\n",
      "\n",
      "At iterate  650    f=  9.11322D+02    |proj g|=  3.86027D+00\n",
      "\n",
      "At iterate  700    f=  9.11304D+02    |proj g|=  2.35477D+00\n",
      "\n",
      "At iterate  750    f=  9.11299D+02    |proj g|=  3.28066D-01\n",
      "\n",
      "At iterate  800    f=  9.11298D+02    |proj g|=  2.51364D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    801    952      1     0     0   3.186D-02   9.113D+02\n",
      "  F =   911.29846357953181     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  1.03533D+02\n",
      "\n",
      "At iterate   50    f=  1.01027D+03    |proj g|=  1.68317D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  100    f=  9.38561D+02    |proj g|=  1.07092D+02\n",
      "\n",
      "At iterate  150    f=  9.24857D+02    |proj g|=  3.29961D+01\n",
      "\n",
      "At iterate  200    f=  9.19097D+02    |proj g|=  2.27966D+00\n",
      "\n",
      "At iterate  250    f=  9.18317D+02    |proj g|=  1.10955D+01\n",
      "\n",
      "At iterate  300    f=  9.18065D+02    |proj g|=  4.58017D+00\n",
      "\n",
      "At iterate  350    f=  9.17941D+02    |proj g|=  4.98469D-01\n",
      "\n",
      "At iterate  400    f=  9.17915D+02    |proj g|=  6.85732D-01\n",
      "\n",
      "At iterate  450    f=  9.17905D+02    |proj g|=  5.86384D-01\n",
      "\n",
      "At iterate  500    f=  9.17897D+02    |proj g|=  1.32474D+00\n",
      "\n",
      "At iterate  550    f=  9.17876D+02    |proj g|=  2.08482D-01\n",
      "\n",
      "At iterate  600    f=  9.17831D+02    |proj g|=  1.32920D+00\n",
      "\n",
      "At iterate  650    f=  9.17797D+02    |proj g|=  8.77780D+00\n",
      "\n",
      "At iterate  700    f=  9.17761D+02    |proj g|=  1.90957D+00\n",
      "\n",
      "At iterate  750    f=  9.17731D+02    |proj g|=  4.64851D+00\n",
      "\n",
      "At iterate  800    f=  9.17696D+02    |proj g|=  4.85197D+00\n",
      "\n",
      "At iterate  850    f=  9.17609D+02    |proj g|=  1.29428D+00\n",
      "\n",
      "At iterate  900    f=  9.17408D+02    |proj g|=  5.66884D+00\n",
      "\n",
      "At iterate  950    f=  9.17291D+02    |proj g|=  6.33924D-01\n",
      "\n",
      "At iterate 1000    f=  9.17259D+02    |proj g|=  1.58026D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769   1000   1172      1     0     0   1.580D+00   9.173D+02\n",
      "  F =   917.25947106156423     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/shashwat.s/anaconda3/envs/new_research_env/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  9.45374D+01\n",
      "\n",
      "At iterate   50    f=  9.71952D+02    |proj g|=  2.57800D+01\n",
      "\n",
      "At iterate  100    f=  9.28865D+02    |proj g|=  1.26839D+01\n",
      "\n",
      "At iterate  150    f=  9.24453D+02    |proj g|=  1.23432D+01\n",
      "\n",
      "At iterate  200    f=  9.23265D+02    |proj g|=  6.98147D+00\n",
      "\n",
      "At iterate  250    f=  9.22942D+02    |proj g|=  4.56957D+00\n",
      "\n",
      "At iterate  300    f=  9.22869D+02    |proj g|=  2.57783D-01\n",
      "\n",
      "At iterate  350    f=  9.22845D+02    |proj g|=  6.96605D-01\n",
      "\n",
      "At iterate  400    f=  9.22839D+02    |proj g|=  9.63007D-02\n",
      "\n",
      "At iterate  450    f=  9.22837D+02    |proj g|=  1.11803D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    470    557      1     0     0   2.283D-01   9.228D+02\n",
      "  F =   922.83716292994859     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  7.03438D+01\n",
      "\n",
      "At iterate   50    f=  1.00394D+03    |proj g|=  4.06428D+01\n",
      "\n",
      "At iterate  100    f=  9.42375D+02    |proj g|=  1.78309D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  150    f=  9.18712D+02    |proj g|=  4.49625D+01\n",
      "\n",
      "At iterate  200    f=  9.13498D+02    |proj g|=  1.07204D+01\n",
      "\n",
      "At iterate  250    f=  9.12722D+02    |proj g|=  7.03243D+00\n",
      "\n",
      "At iterate  300    f=  9.12579D+02    |proj g|=  1.08591D+00\n",
      "\n",
      "At iterate  350    f=  9.12528D+02    |proj g|=  1.57841D+01\n",
      "\n",
      "At iterate  400    f=  9.12513D+02    |proj g|=  1.50616D+00\n",
      "\n",
      "At iterate  450    f=  9.12504D+02    |proj g|=  2.35127D+00\n",
      "\n",
      "At iterate  500    f=  9.12502D+02    |proj g|=  8.67843D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    500    581      1     0     0   8.678D-01   9.125D+02\n",
      "  F =   912.50231360058206     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  6.89429D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  9.13284D+02    |proj g|=  3.47340D+00\n",
      "\n",
      "At iterate  100    f=  9.01773D+02    |proj g|=  7.74643D+00\n",
      "\n",
      "At iterate  150    f=  9.00946D+02    |proj g|=  3.61028D+00\n",
      "\n",
      "At iterate  200    f=  9.00764D+02    |proj g|=  7.55689D+00\n",
      "\n",
      "At iterate  250    f=  9.00675D+02    |proj g|=  6.50622D+00\n",
      "\n",
      "At iterate  300    f=  9.00639D+02    |proj g|=  2.94627D+00\n",
      "\n",
      "At iterate  350    f=  9.00628D+02    |proj g|=  3.65034D+00\n",
      "\n",
      "At iterate  400    f=  9.00616D+02    |proj g|=  6.32066D-01\n",
      "\n",
      "At iterate  450    f=  9.00608D+02    |proj g|=  5.20356D+00\n",
      "\n",
      "At iterate  500    f=  9.00547D+02    |proj g|=  3.07591D+00\n",
      "\n",
      "At iterate  550    f=  9.00408D+02    |proj g|=  2.60152D+00\n",
      "\n",
      "At iterate  600    f=  9.00305D+02    |proj g|=  5.88589D+00\n",
      "\n",
      "At iterate  650    f=  9.00219D+02    |proj g|=  6.07319D+00\n",
      "\n",
      "At iterate  700    f=  9.00185D+02    |proj g|=  4.88959D-01\n",
      "\n",
      "At iterate  750    f=  9.00174D+02    |proj g|=  6.33497D-01\n",
      "\n",
      "At iterate  800    f=  9.00168D+02    |proj g|=  1.18751D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    816    960      1     0     0   4.510D-01   9.002D+02\n",
      "  F =   900.16802931765960     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  8.29858D+01\n",
      "\n",
      "At iterate   50    f=  9.29043D+02    |proj g|=  1.24939D+01\n",
      "\n",
      "At iterate  100    f=  8.95504D+02    |proj g|=  3.94853D+00\n",
      "\n",
      "At iterate  150    f=  8.90509D+02    |proj g|=  3.61422D+00\n",
      "\n",
      "At iterate  200    f=  8.89900D+02    |proj g|=  4.75914D+00\n",
      "\n",
      "At iterate  250    f=  8.89849D+02    |proj g|=  5.60374D+00\n",
      "\n",
      "At iterate  300    f=  8.89831D+02    |proj g|=  5.14311D+00\n",
      "\n",
      "At iterate  350    f=  8.89820D+02    |proj g|=  1.36623D+00\n",
      "\n",
      "At iterate  400    f=  8.89815D+02    |proj g|=  1.27540D-01\n",
      "\n",
      "At iterate  450    f=  8.89809D+02    |proj g|=  4.48564D-01\n",
      "\n",
      "At iterate  500    f=  8.89796D+02    |proj g|=  3.14158D+00\n",
      "\n",
      "At iterate  550    f=  8.89774D+02    |proj g|=  1.98201D+00\n",
      "\n",
      "At iterate  600    f=  8.89701D+02    |proj g|=  1.70395D+00\n",
      "\n",
      "At iterate  650    f=  8.89629D+02    |proj g|=  7.01314D+00\n",
      "\n",
      "At iterate  700    f=  8.89497D+02    |proj g|=  8.69157D-01\n",
      "\n",
      "At iterate  750    f=  8.89407D+02    |proj g|=  3.77674D+00\n",
      "\n",
      "At iterate  800    f=  8.89364D+02    |proj g|=  2.37717D+00\n",
      "\n",
      "At iterate  850    f=  8.89353D+02    |proj g|=  1.95223D+00\n",
      "\n",
      "At iterate  900    f=  8.89350D+02    |proj g|=  7.83052D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    930   1079      1     0     0   7.262D-02   8.893D+02\n",
      "  F =   889.34951251090217     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  8.71307D+01\n",
      "\n",
      "At iterate   50    f=  9.81862D+02    |proj g|=  2.32734D+01\n",
      "\n",
      "At iterate  100    f=  9.00679D+02    |proj g|=  8.63066D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  150    f=  8.83617D+02    |proj g|=  5.11324D+01\n",
      "\n",
      "At iterate  200    f=  8.78725D+02    |proj g|=  2.22912D+01\n",
      "\n",
      "At iterate  250    f=  8.77696D+02    |proj g|=  6.66085D+00\n",
      "\n",
      "At iterate  300    f=  8.77319D+02    |proj g|=  3.97054D+00\n",
      "\n",
      "At iterate  350    f=  8.77232D+02    |proj g|=  4.53534D-01\n",
      "\n",
      "At iterate  400    f=  8.77212D+02    |proj g|=  3.76740D+00\n",
      "\n",
      "At iterate  450    f=  8.77207D+02    |proj g|=  1.19489D+00\n",
      "\n",
      "At iterate  500    f=  8.77191D+02    |proj g|=  2.06597D+00\n",
      "\n",
      "At iterate  550    f=  8.77148D+02    |proj g|=  1.61761D+00\n",
      "\n",
      "At iterate  600    f=  8.77067D+02    |proj g|=  9.64368D+00\n",
      "\n",
      "At iterate  650    f=  8.76872D+02    |proj g|=  1.68948D+00\n",
      "\n",
      "At iterate  700    f=  8.76811D+02    |proj g|=  2.23866D+00\n",
      "\n",
      "At iterate  750    f=  8.76779D+02    |proj g|=  4.30763D+00\n",
      "\n",
      "At iterate  800    f=  8.76772D+02    |proj g|=  4.63838D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    830    985      1     0     0   2.962D-01   8.768D+02\n",
      "  F =   876.77147908135544     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  9.76954D+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  9.17670D+02    |proj g|=  2.83455D+01\n",
      "\n",
      "At iterate  100    f=  8.68328D+02    |proj g|=  2.45152D+01\n",
      "\n",
      "At iterate  150    f=  8.57536D+02    |proj g|=  2.10658D+01\n",
      "\n",
      "At iterate  200    f=  8.55610D+02    |proj g|=  2.33572D+01\n",
      "\n",
      "At iterate  250    f=  8.54972D+02    |proj g|=  3.93009D+00\n",
      "\n",
      "At iterate  300    f=  8.54867D+02    |proj g|=  2.71886D+00\n",
      "\n",
      "At iterate  350    f=  8.54833D+02    |proj g|=  3.95704D+00\n",
      "\n",
      "At iterate  400    f=  8.54823D+02    |proj g|=  2.83478D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    445    526      1     0     0   1.063D-01   8.548D+02\n",
      "  F =   854.82066756258598     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.38629D+03    |proj g|=  7.51754D+01\n",
      "\n",
      "At iterate   50    f=  1.00393D+03    |proj g|=  1.25267D+00\n",
      "\n",
      "At iterate  100    f=  1.00387D+03    |proj g|=  1.04436D-01\n",
      "\n",
      "At iterate  150    f=  1.00386D+03    |proj g|=  2.60959D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  200    f=  1.00386D+03    |proj g|=  2.68867D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769    219    259      1     0     0   3.523D-01   1.004D+03\n",
      "  F =   1003.8550754738139     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "linear_classifiers, accuracies = run_experiment_across_layers(linear_probe_experiment, train_detect_neg_consistency, train_labels_det_neg_consistency, test_detect_neg_consistency, test_labels_det_neg_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63604699\n",
      "Iteration 2, loss = 0.59836796\n",
      "Iteration 3, loss = 0.57488126\n",
      "Iteration 4, loss = 0.55649764\n",
      "Iteration 5, loss = 0.54200105\n",
      "Iteration 6, loss = 0.52694793\n",
      "Iteration 7, loss = 0.51097616\n",
      "Iteration 8, loss = 0.49570936\n",
      "Iteration 9, loss = 0.46995626\n",
      "Iteration 10, loss = 0.45055278\n",
      "Iteration 11, loss = 0.43736771\n",
      "Iteration 12, loss = 0.43190333\n",
      "Iteration 13, loss = 0.40215087\n",
      "Iteration 14, loss = 0.37750309\n",
      "Iteration 15, loss = 0.34973615\n",
      "Iteration 16, loss = 0.32980012\n",
      "Iteration 17, loss = 0.31261986\n",
      "Iteration 18, loss = 0.29195215\n",
      "Iteration 19, loss = 0.27289586\n",
      "Iteration 20, loss = 0.25857880\n",
      "Iteration 21, loss = 0.24016809\n",
      "Iteration 22, loss = 0.23006969\n",
      "Iteration 23, loss = 0.20569809\n",
      "Iteration 24, loss = 0.19178704\n",
      "Iteration 25, loss = 0.18555150\n",
      "Iteration 26, loss = 0.17421476\n",
      "Iteration 27, loss = 0.16494094\n",
      "Iteration 28, loss = 0.15037308\n",
      "Iteration 29, loss = 0.14145178\n",
      "Iteration 30, loss = 0.13717608\n",
      "Iteration 31, loss = 0.12523073\n",
      "Iteration 32, loss = 0.11730392\n",
      "Iteration 33, loss = 0.11310933\n",
      "Iteration 34, loss = 0.10680722\n",
      "Iteration 35, loss = 0.10342547\n",
      "Iteration 36, loss = 0.10790747\n",
      "Iteration 37, loss = 0.10027633\n",
      "Iteration 38, loss = 0.09304177\n",
      "Iteration 39, loss = 0.08762436\n",
      "Iteration 40, loss = 0.08758994\n",
      "Iteration 41, loss = 0.08432340\n",
      "Iteration 42, loss = 0.08837360\n",
      "Iteration 43, loss = 0.07856058\n",
      "Iteration 44, loss = 0.07952762\n",
      "Iteration 45, loss = 0.07799066\n",
      "Iteration 46, loss = 0.07684923\n",
      "Iteration 47, loss = 0.07260622\n",
      "Iteration 48, loss = 0.07031167\n",
      "Iteration 49, loss = 0.07322610\n",
      "Iteration 50, loss = 0.06704849\n",
      "Iteration 51, loss = 0.06924968\n",
      "Iteration 52, loss = 0.06819631\n",
      "Iteration 53, loss = 0.06550298\n",
      "Iteration 54, loss = 0.07521475\n",
      "Iteration 55, loss = 0.06992437\n",
      "Iteration 56, loss = 0.06512995\n",
      "Iteration 57, loss = 0.07272678\n",
      "Iteration 58, loss = 0.06643990\n",
      "Iteration 59, loss = 0.06927767\n",
      "Iteration 60, loss = 0.06195810\n",
      "Iteration 61, loss = 0.05954119\n",
      "Iteration 62, loss = 0.06009850\n",
      "Iteration 63, loss = 0.08006889\n",
      "Iteration 64, loss = 0.06759191\n",
      "Iteration 65, loss = 0.06280845\n",
      "Iteration 66, loss = 0.06237400\n",
      "Iteration 67, loss = 0.08365141\n",
      "Iteration 68, loss = 0.06802378\n",
      "Iteration 69, loss = 0.06332642\n",
      "Iteration 70, loss = 0.07106606\n",
      "Iteration 71, loss = 0.05890802\n",
      "Iteration 72, loss = 0.05707541\n",
      "Iteration 73, loss = 0.06369312\n",
      "Iteration 74, loss = 0.07486053\n",
      "Iteration 75, loss = 0.05979815\n",
      "Iteration 76, loss = 0.07060015\n",
      "Iteration 77, loss = 0.05813850\n",
      "Iteration 78, loss = 0.05580016\n",
      "Iteration 79, loss = 0.07353425\n",
      "Iteration 80, loss = 0.05898650\n",
      "Iteration 81, loss = 0.05826942\n",
      "Iteration 82, loss = 0.07344799\n",
      "Iteration 83, loss = 0.06802613\n",
      "Iteration 84, loss = 0.05882545\n",
      "Iteration 85, loss = 0.05815590\n",
      "Iteration 86, loss = 0.05445725\n",
      "Iteration 87, loss = 0.05736374\n",
      "Iteration 88, loss = 0.06041049\n",
      "Iteration 89, loss = 0.05384745\n",
      "Iteration 90, loss = 0.06402757\n",
      "Iteration 91, loss = 0.06310689\n",
      "Iteration 92, loss = 0.06624522\n",
      "Iteration 93, loss = 0.06242020\n",
      "Iteration 94, loss = 0.05909483\n",
      "Iteration 95, loss = 0.05640933\n",
      "Iteration 96, loss = 0.05462230\n",
      "Iteration 97, loss = 0.05143973\n",
      "Iteration 98, loss = 0.05231679\n",
      "Iteration 99, loss = 0.05290378\n",
      "Iteration 100, loss = 0.05233118\n",
      "Iteration 101, loss = 0.05507804\n",
      "Iteration 102, loss = 0.05222776\n",
      "Iteration 103, loss = 0.05640372\n",
      "Iteration 104, loss = 0.05292914\n",
      "Iteration 105, loss = 0.06642949\n",
      "Iteration 106, loss = 0.05882089\n",
      "Iteration 107, loss = 0.05254260\n",
      "Iteration 108, loss = 0.05416966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63188253\n",
      "Iteration 2, loss = 0.59360666\n",
      "Iteration 3, loss = 0.56410380\n",
      "Iteration 4, loss = 0.54639128\n",
      "Iteration 5, loss = 0.52664587\n",
      "Iteration 6, loss = 0.50782382\n",
      "Iteration 7, loss = 0.48928894\n",
      "Iteration 8, loss = 0.46433566\n",
      "Iteration 9, loss = 0.43741742\n",
      "Iteration 10, loss = 0.41804722\n",
      "Iteration 11, loss = 0.39191364\n",
      "Iteration 12, loss = 0.38448173\n",
      "Iteration 13, loss = 0.34545838\n",
      "Iteration 14, loss = 0.33236163\n",
      "Iteration 15, loss = 0.30011719\n",
      "Iteration 16, loss = 0.27429785\n",
      "Iteration 17, loss = 0.26058066\n",
      "Iteration 18, loss = 0.24374045\n",
      "Iteration 19, loss = 0.22197279\n",
      "Iteration 20, loss = 0.20963496\n",
      "Iteration 21, loss = 0.19180026\n",
      "Iteration 22, loss = 0.18033351\n",
      "Iteration 23, loss = 0.16359713\n",
      "Iteration 24, loss = 0.14958958\n",
      "Iteration 25, loss = 0.14854843\n",
      "Iteration 26, loss = 0.13543884\n",
      "Iteration 27, loss = 0.13064389\n",
      "Iteration 28, loss = 0.11967133\n",
      "Iteration 29, loss = 0.11075499\n",
      "Iteration 30, loss = 0.10779968\n",
      "Iteration 31, loss = 0.10008874\n",
      "Iteration 32, loss = 0.09644339\n",
      "Iteration 33, loss = 0.09321748\n",
      "Iteration 34, loss = 0.08790443\n",
      "Iteration 35, loss = 0.08578384\n",
      "Iteration 36, loss = 0.08826477\n",
      "Iteration 37, loss = 0.08611305\n",
      "Iteration 38, loss = 0.07998340\n",
      "Iteration 39, loss = 0.07636209\n",
      "Iteration 40, loss = 0.07642591\n",
      "Iteration 41, loss = 0.07553597\n",
      "Iteration 42, loss = 0.07850151\n",
      "Iteration 43, loss = 0.07143376\n",
      "Iteration 44, loss = 0.07676206\n",
      "Iteration 45, loss = 0.07056092\n",
      "Iteration 46, loss = 0.07157003\n",
      "Iteration 47, loss = 0.06670317\n",
      "Iteration 48, loss = 0.06659393\n",
      "Iteration 49, loss = 0.06981533\n",
      "Iteration 50, loss = 0.06292044\n",
      "Iteration 51, loss = 0.06496776\n",
      "Iteration 52, loss = 0.06481178\n",
      "Iteration 53, loss = 0.06189029\n",
      "Iteration 54, loss = 0.07355862\n",
      "Iteration 55, loss = 0.06666771\n",
      "Iteration 56, loss = 0.06520888\n",
      "Iteration 57, loss = 0.07843158\n",
      "Iteration 58, loss = 0.06381833\n",
      "Iteration 59, loss = 0.07227795\n",
      "Iteration 60, loss = 0.06117113\n",
      "Iteration 61, loss = 0.05834447\n",
      "Iteration 62, loss = 0.05996940\n",
      "Iteration 63, loss = 0.08424142\n",
      "Iteration 64, loss = 0.06759042\n",
      "Iteration 65, loss = 0.06220884\n",
      "Iteration 66, loss = 0.06199624\n",
      "Iteration 67, loss = 0.08920502\n",
      "Iteration 68, loss = 0.06838536\n",
      "Iteration 69, loss = 0.06531905\n",
      "Iteration 70, loss = 0.07289915\n",
      "Iteration 71, loss = 0.06048895\n",
      "Iteration 72, loss = 0.05685734\n",
      "Iteration 73, loss = 0.06353277\n",
      "Iteration 74, loss = 0.07005877\n",
      "Iteration 75, loss = 0.06209488\n",
      "Iteration 76, loss = 0.07001445\n",
      "Iteration 77, loss = 0.05787791\n",
      "Iteration 78, loss = 0.05671749\n",
      "Iteration 79, loss = 0.07301428\n",
      "Iteration 80, loss = 0.06065952\n",
      "Iteration 81, loss = 0.05911103\n",
      "Iteration 82, loss = 0.07397205\n",
      "Iteration 83, loss = 0.07389102\n",
      "Iteration 84, loss = 0.06043630\n",
      "Iteration 85, loss = 0.05941361\n",
      "Iteration 86, loss = 0.05458840\n",
      "Iteration 87, loss = 0.05874885\n",
      "Iteration 88, loss = 0.05978214\n",
      "Iteration 89, loss = 0.05316064\n",
      "Iteration 90, loss = 0.06799696\n",
      "Iteration 91, loss = 0.06325021\n",
      "Iteration 92, loss = 0.06797606\n",
      "Iteration 93, loss = 0.06939083\n",
      "Iteration 94, loss = 0.06062104\n",
      "Iteration 95, loss = 0.05833842\n",
      "Iteration 96, loss = 0.05538082\n",
      "Iteration 97, loss = 0.05348000\n",
      "Iteration 98, loss = 0.05375794\n",
      "Iteration 99, loss = 0.05372239\n",
      "Iteration 100, loss = 0.05274974\n",
      "Iteration 101, loss = 0.05434645\n",
      "Iteration 102, loss = 0.05283379\n",
      "Iteration 103, loss = 0.05355760\n",
      "Iteration 104, loss = 0.05219710\n",
      "Iteration 105, loss = 0.06232467\n",
      "Iteration 106, loss = 0.05887634\n",
      "Iteration 107, loss = 0.05555535\n",
      "Iteration 108, loss = 0.05497669\n",
      "Iteration 109, loss = 0.05597006\n",
      "Iteration 110, loss = 0.05357688\n",
      "Iteration 111, loss = 0.05182797\n",
      "Iteration 112, loss = 0.05111739\n",
      "Iteration 113, loss = 0.05266376\n",
      "Iteration 114, loss = 0.05343948\n",
      "Iteration 115, loss = 0.05371685\n",
      "Iteration 116, loss = 0.05935840\n",
      "Iteration 117, loss = 0.05263863\n",
      "Iteration 118, loss = 0.05553786\n",
      "Iteration 119, loss = 0.05521927\n",
      "Iteration 120, loss = 0.05800951\n",
      "Iteration 121, loss = 0.05510636\n",
      "Iteration 122, loss = 0.05775708\n",
      "Iteration 123, loss = 0.06065181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63505078\n",
      "Iteration 2, loss = 0.59878557\n",
      "Iteration 3, loss = 0.57159176\n",
      "Iteration 4, loss = 0.55887822\n",
      "Iteration 5, loss = 0.54751666\n",
      "Iteration 6, loss = 0.53248250\n",
      "Iteration 7, loss = 0.52112674\n",
      "Iteration 8, loss = 0.50075595\n",
      "Iteration 9, loss = 0.48193815\n",
      "Iteration 10, loss = 0.46732832\n",
      "Iteration 11, loss = 0.44729461\n",
      "Iteration 12, loss = 0.43871541\n",
      "Iteration 13, loss = 0.41142814\n",
      "Iteration 14, loss = 0.40010355\n",
      "Iteration 15, loss = 0.37369485\n",
      "Iteration 16, loss = 0.34570853\n",
      "Iteration 17, loss = 0.33648585\n",
      "Iteration 18, loss = 0.31655543\n",
      "Iteration 19, loss = 0.29050275\n",
      "Iteration 20, loss = 0.28290890\n",
      "Iteration 21, loss = 0.26241383\n",
      "Iteration 22, loss = 0.25158391\n",
      "Iteration 23, loss = 0.23174609\n",
      "Iteration 24, loss = 0.21957473\n",
      "Iteration 25, loss = 0.20597974\n",
      "Iteration 26, loss = 0.19149619\n",
      "Iteration 27, loss = 0.18995140\n",
      "Iteration 28, loss = 0.17505696\n",
      "Iteration 29, loss = 0.15987447\n",
      "Iteration 30, loss = 0.15407176\n",
      "Iteration 31, loss = 0.14168934\n",
      "Iteration 32, loss = 0.13628086\n",
      "Iteration 33, loss = 0.12680415\n",
      "Iteration 34, loss = 0.12147857\n",
      "Iteration 35, loss = 0.11939811\n",
      "Iteration 36, loss = 0.12632562\n",
      "Iteration 37, loss = 0.11004560\n",
      "Iteration 38, loss = 0.10091550\n",
      "Iteration 39, loss = 0.10063795\n",
      "Iteration 40, loss = 0.10001798\n",
      "Iteration 41, loss = 0.09221738\n",
      "Iteration 42, loss = 0.09472954\n",
      "Iteration 43, loss = 0.08461256\n",
      "Iteration 44, loss = 0.09284688\n",
      "Iteration 45, loss = 0.08681360\n",
      "Iteration 46, loss = 0.08580841\n",
      "Iteration 47, loss = 0.07855648\n",
      "Iteration 48, loss = 0.07514564\n",
      "Iteration 49, loss = 0.07643730\n",
      "Iteration 50, loss = 0.07045210\n",
      "Iteration 51, loss = 0.07199724\n",
      "Iteration 52, loss = 0.06999677\n",
      "Iteration 53, loss = 0.06834895\n",
      "Iteration 54, loss = 0.07800771\n",
      "Iteration 55, loss = 0.07047290\n",
      "Iteration 56, loss = 0.06728660\n",
      "Iteration 57, loss = 0.08280278\n",
      "Iteration 58, loss = 0.06635500\n",
      "Iteration 59, loss = 0.07853533\n",
      "Iteration 60, loss = 0.06505344\n",
      "Iteration 61, loss = 0.06220511\n",
      "Iteration 62, loss = 0.06350869\n",
      "Iteration 63, loss = 0.08992987\n",
      "Iteration 64, loss = 0.06920976\n",
      "Iteration 65, loss = 0.06618695\n",
      "Iteration 66, loss = 0.06555735\n",
      "Iteration 67, loss = 0.08967498\n",
      "Iteration 68, loss = 0.06427123\n",
      "Iteration 69, loss = 0.06729272\n",
      "Iteration 70, loss = 0.06604730\n",
      "Iteration 71, loss = 0.05974048\n",
      "Iteration 72, loss = 0.05724162\n",
      "Iteration 73, loss = 0.06821020\n",
      "Iteration 74, loss = 0.07331979\n",
      "Iteration 75, loss = 0.06444765\n",
      "Iteration 76, loss = 0.07556902\n",
      "Iteration 77, loss = 0.06260284\n",
      "Iteration 78, loss = 0.05697250\n",
      "Iteration 79, loss = 0.07091316\n",
      "Iteration 80, loss = 0.05782335\n",
      "Iteration 81, loss = 0.05865321\n",
      "Iteration 82, loss = 0.06838717\n",
      "Iteration 83, loss = 0.07604745\n",
      "Iteration 84, loss = 0.06501645\n",
      "Iteration 85, loss = 0.05749974\n",
      "Iteration 86, loss = 0.05441152\n",
      "Iteration 87, loss = 0.05863563\n",
      "Iteration 88, loss = 0.06246372\n",
      "Iteration 89, loss = 0.05440848\n",
      "Iteration 90, loss = 0.06793426\n",
      "Iteration 91, loss = 0.06338161\n",
      "Iteration 92, loss = 0.07034890\n",
      "Iteration 93, loss = 0.06638639\n",
      "Iteration 94, loss = 0.06031893\n",
      "Iteration 95, loss = 0.05588501\n",
      "Iteration 96, loss = 0.05435791\n",
      "Iteration 97, loss = 0.05250634\n",
      "Iteration 98, loss = 0.05248037\n",
      "Iteration 99, loss = 0.05362759\n",
      "Iteration 100, loss = 0.05347568\n",
      "Iteration 101, loss = 0.05392779\n",
      "Iteration 102, loss = 0.05290863\n",
      "Iteration 103, loss = 0.05518439\n",
      "Iteration 104, loss = 0.05078364\n",
      "Iteration 105, loss = 0.06559889\n",
      "Iteration 106, loss = 0.05621532\n",
      "Iteration 107, loss = 0.05336528\n",
      "Iteration 108, loss = 0.05401907\n",
      "Iteration 109, loss = 0.05730130\n",
      "Iteration 110, loss = 0.05547907\n",
      "Iteration 111, loss = 0.05203328\n",
      "Iteration 112, loss = 0.05206704\n",
      "Iteration 113, loss = 0.05216139\n",
      "Iteration 114, loss = 0.05154548\n",
      "Iteration 115, loss = 0.05432195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62767703\n",
      "Iteration 2, loss = 0.59226747\n",
      "Iteration 3, loss = 0.56930353\n",
      "Iteration 4, loss = 0.55649936\n",
      "Iteration 5, loss = 0.54598855\n",
      "Iteration 6, loss = 0.53632473\n",
      "Iteration 7, loss = 0.52617000\n",
      "Iteration 8, loss = 0.51046857\n",
      "Iteration 9, loss = 0.49258757\n",
      "Iteration 10, loss = 0.47743313\n",
      "Iteration 11, loss = 0.46617062\n",
      "Iteration 12, loss = 0.45536249\n",
      "Iteration 13, loss = 0.43705841\n",
      "Iteration 14, loss = 0.42511905\n",
      "Iteration 15, loss = 0.40555645\n",
      "Iteration 16, loss = 0.37965237\n",
      "Iteration 17, loss = 0.36861820\n",
      "Iteration 18, loss = 0.34793521\n",
      "Iteration 19, loss = 0.33083520\n",
      "Iteration 20, loss = 0.31897982\n",
      "Iteration 21, loss = 0.30257594\n",
      "Iteration 22, loss = 0.29016383\n",
      "Iteration 23, loss = 0.27085182\n",
      "Iteration 24, loss = 0.25565089\n",
      "Iteration 25, loss = 0.24972738\n",
      "Iteration 26, loss = 0.23085308\n",
      "Iteration 27, loss = 0.22742567\n",
      "Iteration 28, loss = 0.21348861\n",
      "Iteration 29, loss = 0.19843657\n",
      "Iteration 30, loss = 0.18660938\n",
      "Iteration 31, loss = 0.17503044\n",
      "Iteration 32, loss = 0.16507137\n",
      "Iteration 33, loss = 0.15819258\n",
      "Iteration 34, loss = 0.14857424\n",
      "Iteration 35, loss = 0.14439989\n",
      "Iteration 36, loss = 0.14318285\n",
      "Iteration 37, loss = 0.13022035\n",
      "Iteration 38, loss = 0.12253243\n",
      "Iteration 39, loss = 0.12288596\n",
      "Iteration 40, loss = 0.11395656\n",
      "Iteration 41, loss = 0.10998556\n",
      "Iteration 42, loss = 0.11077158\n",
      "Iteration 43, loss = 0.10422411\n",
      "Iteration 44, loss = 0.10148242\n",
      "Iteration 45, loss = 0.10201668\n",
      "Iteration 46, loss = 0.09728580\n",
      "Iteration 47, loss = 0.08993093\n",
      "Iteration 48, loss = 0.08597374\n",
      "Iteration 49, loss = 0.08662542\n",
      "Iteration 50, loss = 0.07969810\n",
      "Iteration 51, loss = 0.08248766\n",
      "Iteration 52, loss = 0.08456600\n",
      "Iteration 53, loss = 0.07918941\n",
      "Iteration 54, loss = 0.08426318\n",
      "Iteration 55, loss = 0.07862680\n",
      "Iteration 56, loss = 0.07382862\n",
      "Iteration 57, loss = 0.08215312\n",
      "Iteration 58, loss = 0.07281046\n",
      "Iteration 59, loss = 0.08327265\n",
      "Iteration 60, loss = 0.07187598\n",
      "Iteration 61, loss = 0.06644908\n",
      "Iteration 62, loss = 0.06735812\n",
      "Iteration 63, loss = 0.09082953\n",
      "Iteration 64, loss = 0.07081024\n",
      "Iteration 65, loss = 0.07234353\n",
      "Iteration 66, loss = 0.06897471\n",
      "Iteration 67, loss = 0.08983136\n",
      "Iteration 68, loss = 0.06760873\n",
      "Iteration 69, loss = 0.07034175\n",
      "Iteration 70, loss = 0.06962058\n",
      "Iteration 71, loss = 0.06251517\n",
      "Iteration 72, loss = 0.06115865\n",
      "Iteration 73, loss = 0.07377906\n",
      "Iteration 74, loss = 0.07828314\n",
      "Iteration 75, loss = 0.06735925\n",
      "Iteration 76, loss = 0.07305770\n",
      "Iteration 77, loss = 0.06798902\n",
      "Iteration 78, loss = 0.05913974\n",
      "Iteration 79, loss = 0.07670880\n",
      "Iteration 80, loss = 0.06137945\n",
      "Iteration 81, loss = 0.06076319\n",
      "Iteration 82, loss = 0.06911088\n",
      "Iteration 83, loss = 0.08091469\n",
      "Iteration 84, loss = 0.06344474\n",
      "Iteration 85, loss = 0.05902950\n",
      "Iteration 86, loss = 0.05673404\n",
      "Iteration 87, loss = 0.06148601\n",
      "Iteration 88, loss = 0.06799708\n",
      "Iteration 89, loss = 0.05615807\n",
      "Iteration 90, loss = 0.06753152\n",
      "Iteration 91, loss = 0.06376482\n",
      "Iteration 92, loss = 0.06832990\n",
      "Iteration 93, loss = 0.06540332\n",
      "Iteration 94, loss = 0.06431182\n",
      "Iteration 95, loss = 0.05765723\n",
      "Iteration 96, loss = 0.05597604\n",
      "Iteration 97, loss = 0.05508218\n",
      "Iteration 98, loss = 0.05144293\n",
      "Iteration 99, loss = 0.05452739\n",
      "Iteration 100, loss = 0.05772982\n",
      "Iteration 101, loss = 0.05651695\n",
      "Iteration 102, loss = 0.05352900\n",
      "Iteration 103, loss = 0.05798624\n",
      "Iteration 104, loss = 0.05149539\n",
      "Iteration 105, loss = 0.06473788\n",
      "Iteration 106, loss = 0.05553619\n",
      "Iteration 107, loss = 0.05458640\n",
      "Iteration 108, loss = 0.05483198\n",
      "Iteration 109, loss = 0.06684894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62080459\n",
      "Iteration 2, loss = 0.58552437\n",
      "Iteration 3, loss = 0.56368895\n",
      "Iteration 4, loss = 0.54946859\n",
      "Iteration 5, loss = 0.54029844\n",
      "Iteration 6, loss = 0.53001747\n",
      "Iteration 7, loss = 0.51737250\n",
      "Iteration 8, loss = 0.50611667\n",
      "Iteration 9, loss = 0.49137973\n",
      "Iteration 10, loss = 0.47563500\n",
      "Iteration 11, loss = 0.46466413\n",
      "Iteration 12, loss = 0.45851892\n",
      "Iteration 13, loss = 0.45233064\n",
      "Iteration 14, loss = 0.42753759\n",
      "Iteration 15, loss = 0.40540903\n",
      "Iteration 16, loss = 0.39139463\n",
      "Iteration 17, loss = 0.37614011\n",
      "Iteration 18, loss = 0.35810287\n",
      "Iteration 19, loss = 0.34122492\n",
      "Iteration 20, loss = 0.33009995\n",
      "Iteration 21, loss = 0.31111878\n",
      "Iteration 22, loss = 0.30544031\n",
      "Iteration 23, loss = 0.28191823\n",
      "Iteration 24, loss = 0.26753519\n",
      "Iteration 25, loss = 0.26337974\n",
      "Iteration 26, loss = 0.24756876\n",
      "Iteration 27, loss = 0.23598737\n",
      "Iteration 28, loss = 0.22292963\n",
      "Iteration 29, loss = 0.20978920\n",
      "Iteration 30, loss = 0.20142756\n",
      "Iteration 31, loss = 0.18783252\n",
      "Iteration 32, loss = 0.17599945\n",
      "Iteration 33, loss = 0.17200497\n",
      "Iteration 34, loss = 0.16363749\n",
      "Iteration 35, loss = 0.15843512\n",
      "Iteration 36, loss = 0.15747537\n",
      "Iteration 37, loss = 0.14432383\n",
      "Iteration 38, loss = 0.13599021\n",
      "Iteration 39, loss = 0.13468868\n",
      "Iteration 40, loss = 0.12412015\n",
      "Iteration 41, loss = 0.11786673\n",
      "Iteration 42, loss = 0.12436943\n",
      "Iteration 43, loss = 0.11003132\n",
      "Iteration 44, loss = 0.10577724\n",
      "Iteration 45, loss = 0.10901037\n",
      "Iteration 46, loss = 0.10216982\n",
      "Iteration 47, loss = 0.09592597\n",
      "Iteration 48, loss = 0.09220753\n",
      "Iteration 49, loss = 0.09292026\n",
      "Iteration 50, loss = 0.08680884\n",
      "Iteration 51, loss = 0.08859171\n",
      "Iteration 52, loss = 0.08686505\n",
      "Iteration 53, loss = 0.08290571\n",
      "Iteration 54, loss = 0.08845447\n",
      "Iteration 55, loss = 0.08294639\n",
      "Iteration 56, loss = 0.07650233\n",
      "Iteration 57, loss = 0.08402010\n",
      "Iteration 58, loss = 0.07674016\n",
      "Iteration 59, loss = 0.08733699\n",
      "Iteration 60, loss = 0.07545735\n",
      "Iteration 61, loss = 0.06982770\n",
      "Iteration 62, loss = 0.07031812\n",
      "Iteration 63, loss = 0.09970644\n",
      "Iteration 64, loss = 0.07395806\n",
      "Iteration 65, loss = 0.06947442\n",
      "Iteration 66, loss = 0.06959149\n",
      "Iteration 67, loss = 0.08977240\n",
      "Iteration 68, loss = 0.06740223\n",
      "Iteration 69, loss = 0.07067945\n",
      "Iteration 70, loss = 0.07283916\n",
      "Iteration 71, loss = 0.06496911\n",
      "Iteration 72, loss = 0.06608553\n",
      "Iteration 73, loss = 0.07544288\n",
      "Iteration 74, loss = 0.07677228\n",
      "Iteration 75, loss = 0.06761110\n",
      "Iteration 76, loss = 0.07601609\n",
      "Iteration 77, loss = 0.06700638\n",
      "Iteration 78, loss = 0.05953863\n",
      "Iteration 79, loss = 0.07661813\n",
      "Iteration 80, loss = 0.06036530\n",
      "Iteration 81, loss = 0.06054332\n",
      "Iteration 82, loss = 0.06953208\n",
      "Iteration 83, loss = 0.07535382\n",
      "Iteration 84, loss = 0.06351387\n",
      "Iteration 85, loss = 0.05948346\n",
      "Iteration 86, loss = 0.05851093\n",
      "Iteration 87, loss = 0.06331408\n",
      "Iteration 88, loss = 0.07214666\n",
      "Iteration 89, loss = 0.05769442\n",
      "Iteration 90, loss = 0.07267128\n",
      "Iteration 91, loss = 0.06515325\n",
      "Iteration 92, loss = 0.07621065\n",
      "Iteration 93, loss = 0.06752456\n",
      "Iteration 94, loss = 0.06404982\n",
      "Iteration 95, loss = 0.05828112\n",
      "Iteration 96, loss = 0.05697070\n",
      "Iteration 97, loss = 0.05470171\n",
      "Iteration 98, loss = 0.05393610\n",
      "Iteration 99, loss = 0.05663328\n",
      "Iteration 100, loss = 0.05692895\n",
      "Iteration 101, loss = 0.05660042\n",
      "Iteration 102, loss = 0.05430937\n",
      "Iteration 103, loss = 0.06104611\n",
      "Iteration 104, loss = 0.05326896\n",
      "Iteration 105, loss = 0.06825114\n",
      "Iteration 106, loss = 0.05832220\n",
      "Iteration 107, loss = 0.05453355\n",
      "Iteration 108, loss = 0.05485535\n",
      "Iteration 109, loss = 0.06003739\n",
      "Iteration 110, loss = 0.05652918\n",
      "Iteration 111, loss = 0.05143070\n",
      "Iteration 112, loss = 0.05260771\n",
      "Iteration 113, loss = 0.05493060\n",
      "Iteration 114, loss = 0.05390983\n",
      "Iteration 115, loss = 0.05632871\n",
      "Iteration 116, loss = 0.05860247\n",
      "Iteration 117, loss = 0.05272847\n",
      "Iteration 118, loss = 0.05542396\n",
      "Iteration 119, loss = 0.05878657\n",
      "Iteration 120, loss = 0.06297501\n",
      "Iteration 121, loss = 0.05662191\n",
      "Iteration 122, loss = 0.06352988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62421642\n",
      "Iteration 2, loss = 0.58735852\n",
      "Iteration 3, loss = 0.56471730\n",
      "Iteration 4, loss = 0.55313498\n",
      "Iteration 5, loss = 0.54416635\n",
      "Iteration 6, loss = 0.53700991\n",
      "Iteration 7, loss = 0.52502248\n",
      "Iteration 8, loss = 0.51452636\n",
      "Iteration 9, loss = 0.50199976\n",
      "Iteration 10, loss = 0.48811486\n",
      "Iteration 11, loss = 0.47825365\n",
      "Iteration 12, loss = 0.47382768\n",
      "Iteration 13, loss = 0.47006893\n",
      "Iteration 14, loss = 0.44754664\n",
      "Iteration 15, loss = 0.42926664\n",
      "Iteration 16, loss = 0.41876284\n",
      "Iteration 17, loss = 0.40127379\n",
      "Iteration 18, loss = 0.38502208\n",
      "Iteration 19, loss = 0.37006049\n",
      "Iteration 20, loss = 0.36356314\n",
      "Iteration 21, loss = 0.34666027\n",
      "Iteration 22, loss = 0.34882969\n",
      "Iteration 23, loss = 0.31773746\n",
      "Iteration 24, loss = 0.30127783\n",
      "Iteration 25, loss = 0.29560160\n",
      "Iteration 26, loss = 0.28242071\n",
      "Iteration 27, loss = 0.26966644\n",
      "Iteration 28, loss = 0.25522374\n",
      "Iteration 29, loss = 0.24621435\n",
      "Iteration 30, loss = 0.23494120\n",
      "Iteration 31, loss = 0.22114010\n",
      "Iteration 32, loss = 0.20923869\n",
      "Iteration 33, loss = 0.20474988\n",
      "Iteration 34, loss = 0.19312183\n",
      "Iteration 35, loss = 0.18326513\n",
      "Iteration 36, loss = 0.18474636\n",
      "Iteration 37, loss = 0.16633750\n",
      "Iteration 38, loss = 0.15683500\n",
      "Iteration 39, loss = 0.16028411\n",
      "Iteration 40, loss = 0.14658913\n",
      "Iteration 41, loss = 0.13978051\n",
      "Iteration 42, loss = 0.13949073\n",
      "Iteration 43, loss = 0.13259976\n",
      "Iteration 44, loss = 0.12356660\n",
      "Iteration 45, loss = 0.12556634\n",
      "Iteration 46, loss = 0.11598388\n",
      "Iteration 47, loss = 0.11088716\n",
      "Iteration 48, loss = 0.10450834\n",
      "Iteration 49, loss = 0.10461815\n",
      "Iteration 50, loss = 0.09785450\n",
      "Iteration 51, loss = 0.09761483\n",
      "Iteration 52, loss = 0.09772582\n",
      "Iteration 53, loss = 0.09699708\n",
      "Iteration 54, loss = 0.09484346\n",
      "Iteration 55, loss = 0.09210444\n",
      "Iteration 56, loss = 0.08379929\n",
      "Iteration 57, loss = 0.08941532\n",
      "Iteration 58, loss = 0.08306958\n",
      "Iteration 59, loss = 0.09177809\n",
      "Iteration 60, loss = 0.07957485\n",
      "Iteration 61, loss = 0.07658079\n",
      "Iteration 62, loss = 0.07426477\n",
      "Iteration 63, loss = 0.10474742\n",
      "Iteration 64, loss = 0.07735337\n",
      "Iteration 65, loss = 0.07524223\n",
      "Iteration 66, loss = 0.07236194\n",
      "Iteration 67, loss = 0.09159429\n",
      "Iteration 68, loss = 0.07018083\n",
      "Iteration 69, loss = 0.07599953\n",
      "Iteration 70, loss = 0.07268551\n",
      "Iteration 71, loss = 0.06641132\n",
      "Iteration 72, loss = 0.06609575\n",
      "Iteration 73, loss = 0.07763550\n",
      "Iteration 74, loss = 0.07971100\n",
      "Iteration 75, loss = 0.06998299\n",
      "Iteration 76, loss = 0.07431613\n",
      "Iteration 77, loss = 0.06678765\n",
      "Iteration 78, loss = 0.06258830\n",
      "Iteration 79, loss = 0.07470191\n",
      "Iteration 80, loss = 0.06192986\n",
      "Iteration 81, loss = 0.06065991\n",
      "Iteration 82, loss = 0.06787251\n",
      "Iteration 83, loss = 0.07505455\n",
      "Iteration 84, loss = 0.06469586\n",
      "Iteration 85, loss = 0.05885063\n",
      "Iteration 86, loss = 0.05928796\n",
      "Iteration 87, loss = 0.06487293\n",
      "Iteration 88, loss = 0.07150866\n",
      "Iteration 89, loss = 0.05733079\n",
      "Iteration 90, loss = 0.07150128\n",
      "Iteration 91, loss = 0.06717959\n",
      "Iteration 92, loss = 0.08054059\n",
      "Iteration 93, loss = 0.06600865\n",
      "Iteration 94, loss = 0.06282917\n",
      "Iteration 95, loss = 0.05862310\n",
      "Iteration 96, loss = 0.05701697\n",
      "Iteration 97, loss = 0.05803460\n",
      "Iteration 98, loss = 0.05559528\n",
      "Iteration 99, loss = 0.05602157\n",
      "Iteration 100, loss = 0.05671155\n",
      "Iteration 101, loss = 0.05590582\n",
      "Iteration 102, loss = 0.05326137\n",
      "Iteration 103, loss = 0.06110091\n",
      "Iteration 104, loss = 0.05432934\n",
      "Iteration 105, loss = 0.06702630\n",
      "Iteration 106, loss = 0.05999387\n",
      "Iteration 107, loss = 0.05389209\n",
      "Iteration 108, loss = 0.05389873\n",
      "Iteration 109, loss = 0.06549573\n",
      "Iteration 110, loss = 0.05656914\n",
      "Iteration 111, loss = 0.05368075\n",
      "Iteration 112, loss = 0.05335669\n",
      "Iteration 113, loss = 0.05556628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62187898\n",
      "Iteration 2, loss = 0.58549064\n",
      "Iteration 3, loss = 0.56367245\n",
      "Iteration 4, loss = 0.55170740\n",
      "Iteration 5, loss = 0.54052095\n",
      "Iteration 6, loss = 0.53336262\n",
      "Iteration 7, loss = 0.52244177\n",
      "Iteration 8, loss = 0.51091881\n",
      "Iteration 9, loss = 0.49825247\n",
      "Iteration 10, loss = 0.48447181\n",
      "Iteration 11, loss = 0.47660125\n",
      "Iteration 12, loss = 0.47031108\n",
      "Iteration 13, loss = 0.47279186\n",
      "Iteration 14, loss = 0.44912178\n",
      "Iteration 15, loss = 0.42555835\n",
      "Iteration 16, loss = 0.41695029\n",
      "Iteration 17, loss = 0.40074721\n",
      "Iteration 18, loss = 0.38182994\n",
      "Iteration 19, loss = 0.37037585\n",
      "Iteration 20, loss = 0.36484243\n",
      "Iteration 21, loss = 0.34554053\n",
      "Iteration 22, loss = 0.34478940\n",
      "Iteration 23, loss = 0.31847893\n",
      "Iteration 24, loss = 0.30771630\n",
      "Iteration 25, loss = 0.29812931\n",
      "Iteration 26, loss = 0.28789205\n",
      "Iteration 27, loss = 0.27398177\n",
      "Iteration 28, loss = 0.26353877\n",
      "Iteration 29, loss = 0.25167992\n",
      "Iteration 30, loss = 0.24244089\n",
      "Iteration 31, loss = 0.22674842\n",
      "Iteration 32, loss = 0.21649513\n",
      "Iteration 33, loss = 0.21531667\n",
      "Iteration 34, loss = 0.19940045\n",
      "Iteration 35, loss = 0.19202608\n",
      "Iteration 36, loss = 0.19635523\n",
      "Iteration 37, loss = 0.18052554\n",
      "Iteration 38, loss = 0.16738319\n",
      "Iteration 39, loss = 0.16912533\n",
      "Iteration 40, loss = 0.15438302\n",
      "Iteration 41, loss = 0.14663571\n",
      "Iteration 42, loss = 0.14856380\n",
      "Iteration 43, loss = 0.13984612\n",
      "Iteration 44, loss = 0.13362059\n",
      "Iteration 45, loss = 0.13160160\n",
      "Iteration 46, loss = 0.12362524\n",
      "Iteration 47, loss = 0.11937497\n",
      "Iteration 48, loss = 0.11206358\n",
      "Iteration 49, loss = 0.11262066\n",
      "Iteration 50, loss = 0.10558773\n",
      "Iteration 51, loss = 0.10325092\n",
      "Iteration 52, loss = 0.10287360\n",
      "Iteration 53, loss = 0.10002206\n",
      "Iteration 54, loss = 0.09907552\n",
      "Iteration 55, loss = 0.09660196\n",
      "Iteration 56, loss = 0.08823769\n",
      "Iteration 57, loss = 0.09462296\n",
      "Iteration 58, loss = 0.08892545\n",
      "Iteration 59, loss = 0.09384478\n",
      "Iteration 60, loss = 0.08433055\n",
      "Iteration 61, loss = 0.08139252\n",
      "Iteration 62, loss = 0.07843829\n",
      "Iteration 63, loss = 0.10305703\n",
      "Iteration 64, loss = 0.08069055\n",
      "Iteration 65, loss = 0.07973882\n",
      "Iteration 66, loss = 0.07539853\n",
      "Iteration 67, loss = 0.08351880\n",
      "Iteration 68, loss = 0.07182328\n",
      "Iteration 69, loss = 0.07840587\n",
      "Iteration 70, loss = 0.08073490\n",
      "Iteration 71, loss = 0.07006119\n",
      "Iteration 72, loss = 0.06862137\n",
      "Iteration 73, loss = 0.08256164\n",
      "Iteration 74, loss = 0.07823056\n",
      "Iteration 75, loss = 0.07142112\n",
      "Iteration 76, loss = 0.08007988\n",
      "Iteration 77, loss = 0.06958658\n",
      "Iteration 78, loss = 0.06466734\n",
      "Iteration 79, loss = 0.07557265\n",
      "Iteration 80, loss = 0.06206645\n",
      "Iteration 81, loss = 0.06167244\n",
      "Iteration 82, loss = 0.07148882\n",
      "Iteration 83, loss = 0.07381981\n",
      "Iteration 84, loss = 0.06559229\n",
      "Iteration 85, loss = 0.06041139\n",
      "Iteration 86, loss = 0.05871687\n",
      "Iteration 87, loss = 0.06156833\n",
      "Iteration 88, loss = 0.06908575\n",
      "Iteration 89, loss = 0.05792014\n",
      "Iteration 90, loss = 0.06934483\n",
      "Iteration 91, loss = 0.06835085\n",
      "Iteration 92, loss = 0.07571126\n",
      "Iteration 93, loss = 0.07127692\n",
      "Iteration 94, loss = 0.06416114\n",
      "Iteration 95, loss = 0.06149652\n",
      "Iteration 96, loss = 0.05843098\n",
      "Iteration 97, loss = 0.05784457\n",
      "Iteration 98, loss = 0.05583317\n",
      "Iteration 99, loss = 0.05657946\n",
      "Iteration 100, loss = 0.05661187\n",
      "Iteration 101, loss = 0.05649676\n",
      "Iteration 102, loss = 0.05382283\n",
      "Iteration 103, loss = 0.06236601\n",
      "Iteration 104, loss = 0.05397760\n",
      "Iteration 105, loss = 0.06751703\n",
      "Iteration 106, loss = 0.05846286\n",
      "Iteration 107, loss = 0.05345708\n",
      "Iteration 108, loss = 0.05504332\n",
      "Iteration 109, loss = 0.06570290\n",
      "Iteration 110, loss = 0.05674398\n",
      "Iteration 111, loss = 0.05328220\n",
      "Iteration 112, loss = 0.05287769\n",
      "Iteration 113, loss = 0.05565023\n",
      "Iteration 114, loss = 0.05569742\n",
      "Iteration 115, loss = 0.05535473\n",
      "Iteration 116, loss = 0.06145507\n",
      "Iteration 117, loss = 0.05407875\n",
      "Iteration 118, loss = 0.05402466\n",
      "Iteration 119, loss = 0.05528545\n",
      "Iteration 120, loss = 0.06143296\n",
      "Iteration 121, loss = 0.05719160\n",
      "Iteration 122, loss = 0.06115926\n",
      "Iteration 123, loss = 0.05895371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61981437\n",
      "Iteration 2, loss = 0.58232934\n",
      "Iteration 3, loss = 0.55858465\n",
      "Iteration 4, loss = 0.54669530\n",
      "Iteration 5, loss = 0.53505638\n",
      "Iteration 6, loss = 0.52735019\n",
      "Iteration 7, loss = 0.51583062\n",
      "Iteration 8, loss = 0.50359330\n",
      "Iteration 9, loss = 0.49188492\n",
      "Iteration 10, loss = 0.47572171\n",
      "Iteration 11, loss = 0.46755962\n",
      "Iteration 12, loss = 0.46023211\n",
      "Iteration 13, loss = 0.46842424\n",
      "Iteration 14, loss = 0.43619472\n",
      "Iteration 15, loss = 0.41460719\n",
      "Iteration 16, loss = 0.40268943\n",
      "Iteration 17, loss = 0.38631558\n",
      "Iteration 18, loss = 0.37089289\n",
      "Iteration 19, loss = 0.35719872\n",
      "Iteration 20, loss = 0.34896548\n",
      "Iteration 21, loss = 0.32946562\n",
      "Iteration 22, loss = 0.33161567\n",
      "Iteration 23, loss = 0.30687828\n",
      "Iteration 24, loss = 0.29329683\n",
      "Iteration 25, loss = 0.28533092\n",
      "Iteration 26, loss = 0.27247656\n",
      "Iteration 27, loss = 0.26408050\n",
      "Iteration 28, loss = 0.24818606\n",
      "Iteration 29, loss = 0.24324274\n",
      "Iteration 30, loss = 0.23027327\n",
      "Iteration 31, loss = 0.21641494\n",
      "Iteration 32, loss = 0.20567607\n",
      "Iteration 33, loss = 0.20105062\n",
      "Iteration 34, loss = 0.18865622\n",
      "Iteration 35, loss = 0.18120133\n",
      "Iteration 36, loss = 0.18493773\n",
      "Iteration 37, loss = 0.17173829\n",
      "Iteration 38, loss = 0.15836008\n",
      "Iteration 39, loss = 0.15887142\n",
      "Iteration 40, loss = 0.14817567\n",
      "Iteration 41, loss = 0.14165765\n",
      "Iteration 42, loss = 0.13924284\n",
      "Iteration 43, loss = 0.13004594\n",
      "Iteration 44, loss = 0.12841364\n",
      "Iteration 45, loss = 0.12467949\n",
      "Iteration 46, loss = 0.11930064\n",
      "Iteration 47, loss = 0.11687363\n",
      "Iteration 48, loss = 0.10894674\n",
      "Iteration 49, loss = 0.11078675\n",
      "Iteration 50, loss = 0.10350072\n",
      "Iteration 51, loss = 0.10185092\n",
      "Iteration 52, loss = 0.10442275\n",
      "Iteration 53, loss = 0.09884876\n",
      "Iteration 54, loss = 0.09768041\n",
      "Iteration 55, loss = 0.09227665\n",
      "Iteration 56, loss = 0.08625310\n",
      "Iteration 57, loss = 0.09096358\n",
      "Iteration 58, loss = 0.08579233\n",
      "Iteration 59, loss = 0.09570614\n",
      "Iteration 60, loss = 0.08394595\n",
      "Iteration 61, loss = 0.08040065\n",
      "Iteration 62, loss = 0.07702302\n",
      "Iteration 63, loss = 0.09967102\n",
      "Iteration 64, loss = 0.08059673\n",
      "Iteration 65, loss = 0.08011574\n",
      "Iteration 66, loss = 0.07564204\n",
      "Iteration 67, loss = 0.09048429\n",
      "Iteration 68, loss = 0.07246394\n",
      "Iteration 69, loss = 0.07527462\n",
      "Iteration 70, loss = 0.07936633\n",
      "Iteration 71, loss = 0.07039151\n",
      "Iteration 72, loss = 0.06945466\n",
      "Iteration 73, loss = 0.08458077\n",
      "Iteration 74, loss = 0.08199259\n",
      "Iteration 75, loss = 0.07280226\n",
      "Iteration 76, loss = 0.07906818\n",
      "Iteration 77, loss = 0.07073300\n",
      "Iteration 78, loss = 0.06523387\n",
      "Iteration 79, loss = 0.07409970\n",
      "Iteration 80, loss = 0.06481542\n",
      "Iteration 81, loss = 0.06332984\n",
      "Iteration 82, loss = 0.07342231\n",
      "Iteration 83, loss = 0.08145399\n",
      "Iteration 84, loss = 0.07116470\n",
      "Iteration 85, loss = 0.06100299\n",
      "Iteration 86, loss = 0.06049358\n",
      "Iteration 87, loss = 0.06197167\n",
      "Iteration 88, loss = 0.06402569\n",
      "Iteration 89, loss = 0.05785395\n",
      "Iteration 90, loss = 0.07456121\n",
      "Iteration 91, loss = 0.06998701\n",
      "Iteration 92, loss = 0.08260734\n",
      "Iteration 93, loss = 0.06991802\n",
      "Iteration 94, loss = 0.06783837\n",
      "Iteration 95, loss = 0.06159959\n",
      "Iteration 96, loss = 0.05785962\n",
      "Iteration 97, loss = 0.05693996\n",
      "Iteration 98, loss = 0.05580821\n",
      "Iteration 99, loss = 0.05673217\n",
      "Iteration 100, loss = 0.06091951\n",
      "Iteration 101, loss = 0.05676029\n",
      "Iteration 102, loss = 0.05556225\n",
      "Iteration 103, loss = 0.05958517\n",
      "Iteration 104, loss = 0.05537757\n",
      "Iteration 105, loss = 0.06989399\n",
      "Iteration 106, loss = 0.06201834\n",
      "Iteration 107, loss = 0.05473692\n",
      "Iteration 108, loss = 0.05431326\n",
      "Iteration 109, loss = 0.06659587\n",
      "Iteration 110, loss = 0.05909455\n",
      "Iteration 111, loss = 0.05354406\n",
      "Iteration 112, loss = 0.05434091\n",
      "Iteration 113, loss = 0.05794614\n",
      "Iteration 114, loss = 0.05670734\n",
      "Iteration 115, loss = 0.05750475\n",
      "Iteration 116, loss = 0.05998711\n",
      "Iteration 117, loss = 0.05358727\n",
      "Iteration 118, loss = 0.05406876\n",
      "Iteration 119, loss = 0.05807316\n",
      "Iteration 120, loss = 0.06067178\n",
      "Iteration 121, loss = 0.05634663\n",
      "Iteration 122, loss = 0.06680464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61473615\n",
      "Iteration 2, loss = 0.58278086\n",
      "Iteration 3, loss = 0.55636595\n",
      "Iteration 4, loss = 0.54426512\n",
      "Iteration 5, loss = 0.53425013\n",
      "Iteration 6, loss = 0.52750728\n",
      "Iteration 7, loss = 0.51258348\n",
      "Iteration 8, loss = 0.50118886\n",
      "Iteration 9, loss = 0.49295891\n",
      "Iteration 10, loss = 0.47599467\n",
      "Iteration 11, loss = 0.46606050\n",
      "Iteration 12, loss = 0.45936101\n",
      "Iteration 13, loss = 0.46662542\n",
      "Iteration 14, loss = 0.43540209\n",
      "Iteration 15, loss = 0.41373420\n",
      "Iteration 16, loss = 0.40509978\n",
      "Iteration 17, loss = 0.38795131\n",
      "Iteration 18, loss = 0.37043565\n",
      "Iteration 19, loss = 0.35797341\n",
      "Iteration 20, loss = 0.34698781\n",
      "Iteration 21, loss = 0.32986394\n",
      "Iteration 22, loss = 0.33620384\n",
      "Iteration 23, loss = 0.30134648\n",
      "Iteration 24, loss = 0.29165485\n",
      "Iteration 25, loss = 0.27940050\n",
      "Iteration 26, loss = 0.26731649\n",
      "Iteration 27, loss = 0.25688840\n",
      "Iteration 28, loss = 0.24386867\n",
      "Iteration 29, loss = 0.23719275\n",
      "Iteration 30, loss = 0.22365806\n",
      "Iteration 31, loss = 0.20945405\n",
      "Iteration 32, loss = 0.19642668\n",
      "Iteration 33, loss = 0.19139321\n",
      "Iteration 34, loss = 0.18227136\n",
      "Iteration 35, loss = 0.17718353\n",
      "Iteration 36, loss = 0.17788743\n",
      "Iteration 37, loss = 0.16301073\n",
      "Iteration 38, loss = 0.15213856\n",
      "Iteration 39, loss = 0.15154059\n",
      "Iteration 40, loss = 0.14304071\n",
      "Iteration 41, loss = 0.13301378\n",
      "Iteration 42, loss = 0.13167968\n",
      "Iteration 43, loss = 0.12129209\n",
      "Iteration 44, loss = 0.11897993\n",
      "Iteration 45, loss = 0.12158480\n",
      "Iteration 46, loss = 0.11465523\n",
      "Iteration 47, loss = 0.10927228\n",
      "Iteration 48, loss = 0.10341636\n",
      "Iteration 49, loss = 0.10571983\n",
      "Iteration 50, loss = 0.09619433\n",
      "Iteration 51, loss = 0.09546068\n",
      "Iteration 52, loss = 0.09303986\n",
      "Iteration 53, loss = 0.09507502\n",
      "Iteration 54, loss = 0.09204098\n",
      "Iteration 55, loss = 0.09022769\n",
      "Iteration 56, loss = 0.08324714\n",
      "Iteration 57, loss = 0.09245808\n",
      "Iteration 58, loss = 0.08558117\n",
      "Iteration 59, loss = 0.08416871\n",
      "Iteration 60, loss = 0.07940340\n",
      "Iteration 61, loss = 0.07725829\n",
      "Iteration 62, loss = 0.07440821\n",
      "Iteration 63, loss = 0.10060519\n",
      "Iteration 64, loss = 0.07872605\n",
      "Iteration 65, loss = 0.07732647\n",
      "Iteration 66, loss = 0.07225456\n",
      "Iteration 67, loss = 0.08402590\n",
      "Iteration 68, loss = 0.06904134\n",
      "Iteration 69, loss = 0.07385726\n",
      "Iteration 70, loss = 0.07684982\n",
      "Iteration 71, loss = 0.06888920\n",
      "Iteration 72, loss = 0.06815728\n",
      "Iteration 73, loss = 0.09236823\n",
      "Iteration 74, loss = 0.08366845\n",
      "Iteration 75, loss = 0.07325533\n",
      "Iteration 76, loss = 0.07086468\n",
      "Iteration 77, loss = 0.06705872\n",
      "Iteration 78, loss = 0.06383026\n",
      "Iteration 79, loss = 0.07109979\n",
      "Iteration 80, loss = 0.06233367\n",
      "Iteration 81, loss = 0.06133587\n",
      "Iteration 82, loss = 0.07078217\n",
      "Iteration 83, loss = 0.07896210\n",
      "Iteration 84, loss = 0.06578394\n",
      "Iteration 85, loss = 0.05989624\n",
      "Iteration 86, loss = 0.05929928\n",
      "Iteration 87, loss = 0.06203712\n",
      "Iteration 88, loss = 0.06956736\n",
      "Iteration 89, loss = 0.05752883\n",
      "Iteration 90, loss = 0.07468795\n",
      "Iteration 91, loss = 0.06837280\n",
      "Iteration 92, loss = 0.08071203\n",
      "Iteration 93, loss = 0.06572072\n",
      "Iteration 94, loss = 0.06402276\n",
      "Iteration 95, loss = 0.05972382\n",
      "Iteration 96, loss = 0.05667907\n",
      "Iteration 97, loss = 0.05534109\n",
      "Iteration 98, loss = 0.05461648\n",
      "Iteration 99, loss = 0.05856509\n",
      "Iteration 100, loss = 0.05788790\n",
      "Iteration 101, loss = 0.05629937\n",
      "Iteration 102, loss = 0.05471108\n",
      "Iteration 103, loss = 0.05897577\n",
      "Iteration 104, loss = 0.05499052\n",
      "Iteration 105, loss = 0.07086867\n",
      "Iteration 106, loss = 0.05901260\n",
      "Iteration 107, loss = 0.05591525\n",
      "Iteration 108, loss = 0.05316049\n",
      "Iteration 109, loss = 0.06154542\n",
      "Iteration 110, loss = 0.05810059\n",
      "Iteration 111, loss = 0.05405771\n",
      "Iteration 112, loss = 0.05326413\n",
      "Iteration 113, loss = 0.05834528\n",
      "Iteration 114, loss = 0.05486656\n",
      "Iteration 115, loss = 0.06037373\n",
      "Iteration 116, loss = 0.05792259\n",
      "Iteration 117, loss = 0.05419085\n",
      "Iteration 118, loss = 0.05555091\n",
      "Iteration 119, loss = 0.05817118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61492221\n",
      "Iteration 2, loss = 0.58114382\n",
      "Iteration 3, loss = 0.55498587\n",
      "Iteration 4, loss = 0.54155848\n",
      "Iteration 5, loss = 0.53275037\n",
      "Iteration 6, loss = 0.52322921\n",
      "Iteration 7, loss = 0.51042223\n",
      "Iteration 8, loss = 0.49747757\n",
      "Iteration 9, loss = 0.49037931\n",
      "Iteration 10, loss = 0.47148941\n",
      "Iteration 11, loss = 0.46148215\n",
      "Iteration 12, loss = 0.45461928\n",
      "Iteration 13, loss = 0.46057189\n",
      "Iteration 14, loss = 0.42549199\n",
      "Iteration 15, loss = 0.40540162\n",
      "Iteration 16, loss = 0.39333405\n",
      "Iteration 17, loss = 0.37871128\n",
      "Iteration 18, loss = 0.35761530\n",
      "Iteration 19, loss = 0.34483422\n",
      "Iteration 20, loss = 0.33583522\n",
      "Iteration 21, loss = 0.31765847\n",
      "Iteration 22, loss = 0.32165002\n",
      "Iteration 23, loss = 0.28793590\n",
      "Iteration 24, loss = 0.27556860\n",
      "Iteration 25, loss = 0.26578217\n",
      "Iteration 26, loss = 0.25497309\n",
      "Iteration 27, loss = 0.24099306\n",
      "Iteration 28, loss = 0.22666793\n",
      "Iteration 29, loss = 0.22068477\n",
      "Iteration 30, loss = 0.21373920\n",
      "Iteration 31, loss = 0.19820044\n",
      "Iteration 32, loss = 0.18489005\n",
      "Iteration 33, loss = 0.18024577\n",
      "Iteration 34, loss = 0.17072254\n",
      "Iteration 35, loss = 0.17010347\n",
      "Iteration 36, loss = 0.16783659\n",
      "Iteration 37, loss = 0.15080671\n",
      "Iteration 38, loss = 0.14389453\n",
      "Iteration 39, loss = 0.14171475\n",
      "Iteration 40, loss = 0.13206467\n",
      "Iteration 41, loss = 0.12493468\n",
      "Iteration 42, loss = 0.12400472\n",
      "Iteration 43, loss = 0.11523286\n",
      "Iteration 44, loss = 0.11518233\n",
      "Iteration 45, loss = 0.11400929\n",
      "Iteration 46, loss = 0.10795009\n",
      "Iteration 47, loss = 0.10517563\n",
      "Iteration 48, loss = 0.09975901\n",
      "Iteration 49, loss = 0.09928195\n",
      "Iteration 50, loss = 0.09278703\n",
      "Iteration 51, loss = 0.09145277\n",
      "Iteration 52, loss = 0.08932432\n",
      "Iteration 53, loss = 0.09393689\n",
      "Iteration 54, loss = 0.09025764\n",
      "Iteration 55, loss = 0.08501869\n",
      "Iteration 56, loss = 0.08079175\n",
      "Iteration 57, loss = 0.08511154\n",
      "Iteration 58, loss = 0.07820678\n",
      "Iteration 59, loss = 0.08011003\n",
      "Iteration 60, loss = 0.07847602\n",
      "Iteration 61, loss = 0.07296696\n",
      "Iteration 62, loss = 0.07150584\n",
      "Iteration 63, loss = 0.09774160\n",
      "Iteration 64, loss = 0.07744738\n",
      "Iteration 65, loss = 0.08262850\n",
      "Iteration 66, loss = 0.07315145\n",
      "Iteration 67, loss = 0.08897607\n",
      "Iteration 68, loss = 0.06923872\n",
      "Iteration 69, loss = 0.07239066\n",
      "Iteration 70, loss = 0.07752502\n",
      "Iteration 71, loss = 0.06885949\n",
      "Iteration 72, loss = 0.06793376\n",
      "Iteration 73, loss = 0.08290210\n",
      "Iteration 74, loss = 0.08015356\n",
      "Iteration 75, loss = 0.07483366\n",
      "Iteration 76, loss = 0.07295585\n",
      "Iteration 77, loss = 0.06956744\n",
      "Iteration 78, loss = 0.06300361\n",
      "Iteration 79, loss = 0.07480141\n",
      "Iteration 80, loss = 0.06217844\n",
      "Iteration 81, loss = 0.05998324\n",
      "Iteration 82, loss = 0.06933896\n",
      "Iteration 83, loss = 0.07535919\n",
      "Iteration 84, loss = 0.06411076\n",
      "Iteration 85, loss = 0.05871274\n",
      "Iteration 86, loss = 0.05786102\n",
      "Iteration 87, loss = 0.06065231\n",
      "Iteration 88, loss = 0.06787315\n",
      "Iteration 89, loss = 0.05851444\n",
      "Iteration 90, loss = 0.07236090\n",
      "Iteration 91, loss = 0.06649824\n",
      "Iteration 92, loss = 0.07711968\n",
      "Iteration 93, loss = 0.06562162\n",
      "Iteration 94, loss = 0.06513739\n",
      "Iteration 95, loss = 0.05950449\n",
      "Iteration 96, loss = 0.05607800\n",
      "Iteration 97, loss = 0.05357853\n",
      "Iteration 98, loss = 0.05343236\n",
      "Iteration 99, loss = 0.05653661\n",
      "Iteration 100, loss = 0.05889847\n",
      "Iteration 101, loss = 0.05778116\n",
      "Iteration 102, loss = 0.05642172\n",
      "Iteration 103, loss = 0.05775441\n",
      "Iteration 104, loss = 0.05453460\n",
      "Iteration 105, loss = 0.06720063\n",
      "Iteration 106, loss = 0.06025859\n",
      "Iteration 107, loss = 0.05557325\n",
      "Iteration 108, loss = 0.05407699\n",
      "Iteration 109, loss = 0.06423677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61538904\n",
      "Iteration 2, loss = 0.57853589\n",
      "Iteration 3, loss = 0.55455138\n",
      "Iteration 4, loss = 0.54170424\n",
      "Iteration 5, loss = 0.53149156\n",
      "Iteration 6, loss = 0.52150943\n",
      "Iteration 7, loss = 0.50851969\n",
      "Iteration 8, loss = 0.49856259\n",
      "Iteration 9, loss = 0.48699132\n",
      "Iteration 10, loss = 0.46840936\n",
      "Iteration 11, loss = 0.45818853\n",
      "Iteration 12, loss = 0.44840198\n",
      "Iteration 13, loss = 0.45703877\n",
      "Iteration 14, loss = 0.42082744\n",
      "Iteration 15, loss = 0.39841859\n",
      "Iteration 16, loss = 0.38505886\n",
      "Iteration 17, loss = 0.37127426\n",
      "Iteration 18, loss = 0.35174107\n",
      "Iteration 19, loss = 0.33811081\n",
      "Iteration 20, loss = 0.32870920\n",
      "Iteration 21, loss = 0.31437331\n",
      "Iteration 22, loss = 0.31272275\n",
      "Iteration 23, loss = 0.28143307\n",
      "Iteration 24, loss = 0.26919809\n",
      "Iteration 25, loss = 0.26170086\n",
      "Iteration 26, loss = 0.24685785\n",
      "Iteration 27, loss = 0.23702328\n",
      "Iteration 28, loss = 0.22463469\n",
      "Iteration 29, loss = 0.21705362\n",
      "Iteration 30, loss = 0.20791825\n",
      "Iteration 31, loss = 0.19451753\n",
      "Iteration 32, loss = 0.18285585\n",
      "Iteration 33, loss = 0.17459245\n",
      "Iteration 34, loss = 0.16801306\n",
      "Iteration 35, loss = 0.16331321\n",
      "Iteration 36, loss = 0.16561752\n",
      "Iteration 37, loss = 0.14901793\n",
      "Iteration 38, loss = 0.14183343\n",
      "Iteration 39, loss = 0.13978914\n",
      "Iteration 40, loss = 0.13149430\n",
      "Iteration 41, loss = 0.12401255\n",
      "Iteration 42, loss = 0.12023054\n",
      "Iteration 43, loss = 0.11270679\n",
      "Iteration 44, loss = 0.11130384\n",
      "Iteration 45, loss = 0.11253621\n",
      "Iteration 46, loss = 0.10983740\n",
      "Iteration 47, loss = 0.10167683\n",
      "Iteration 48, loss = 0.09849983\n",
      "Iteration 49, loss = 0.09933753\n",
      "Iteration 50, loss = 0.09234105\n",
      "Iteration 51, loss = 0.08979090\n",
      "Iteration 52, loss = 0.09004892\n",
      "Iteration 53, loss = 0.08535788\n",
      "Iteration 54, loss = 0.09010216\n",
      "Iteration 55, loss = 0.08615282\n",
      "Iteration 56, loss = 0.07917407\n",
      "Iteration 57, loss = 0.08463361\n",
      "Iteration 58, loss = 0.07689844\n",
      "Iteration 59, loss = 0.08087541\n",
      "Iteration 60, loss = 0.07669537\n",
      "Iteration 61, loss = 0.07168756\n",
      "Iteration 62, loss = 0.07063455\n",
      "Iteration 63, loss = 0.09123664\n",
      "Iteration 64, loss = 0.07370185\n",
      "Iteration 65, loss = 0.07349478\n",
      "Iteration 66, loss = 0.06901361\n",
      "Iteration 67, loss = 0.08224679\n",
      "Iteration 68, loss = 0.06873372\n",
      "Iteration 69, loss = 0.07825985\n",
      "Iteration 70, loss = 0.07768925\n",
      "Iteration 71, loss = 0.06660402\n",
      "Iteration 72, loss = 0.06460174\n",
      "Iteration 73, loss = 0.07459819\n",
      "Iteration 74, loss = 0.07809535\n",
      "Iteration 75, loss = 0.07239545\n",
      "Iteration 76, loss = 0.07293933\n",
      "Iteration 77, loss = 0.06590015\n",
      "Iteration 78, loss = 0.06083922\n",
      "Iteration 79, loss = 0.07378531\n",
      "Iteration 80, loss = 0.06176360\n",
      "Iteration 81, loss = 0.05926711\n",
      "Iteration 82, loss = 0.07080342\n",
      "Iteration 83, loss = 0.07924614\n",
      "Iteration 84, loss = 0.06423111\n",
      "Iteration 85, loss = 0.05819743\n",
      "Iteration 86, loss = 0.05746722\n",
      "Iteration 87, loss = 0.06009283\n",
      "Iteration 88, loss = 0.07541773\n",
      "Iteration 89, loss = 0.05799381\n",
      "Iteration 90, loss = 0.07074421\n",
      "Iteration 101, loss = 0.05605586\n",
      "Iteration 102, loss = 0.05408003\n",
      "Iteration 103, loss = 0.05787829\n",
      "Iteration 104, loss = 0.05181212\n",
      "Iteration 105, loss = 0.06451288\n",
      "Iteration 106, loss = 0.06092933\n",
      "Iteration 107, loss = 0.05281560\n",
      "Iteration 108, loss = 0.05416811\n",
      "Iteration 109, loss = 0.06297447\n",
      "Iteration 110, loss = 0.05683214\n",
      "Iteration 111, loss = 0.05136897\n",
      "Iteration 112, loss = 0.05144887\n",
      "Iteration 113, loss = 0.05416554\n",
      "Iteration 114, loss = 0.05312762\n",
      "Iteration 115, loss = 0.05385337\n",
      "Iteration 116, loss = 0.05989999\n",
      "Iteration 117, loss = 0.05272886\n",
      "Iteration 118, loss = 0.05246944\n",
      "Iteration 119, loss = 0.05434851\n",
      "Iteration 120, loss = 0.06212242\n",
      "Iteration 121, loss = 0.05461025\n",
      "Iteration 122, loss = 0.06057402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61042650\n",
      "Iteration 2, loss = 0.57633492\n",
      "Iteration 3, loss = 0.55224089\n",
      "Iteration 4, loss = 0.53989581\n",
      "Iteration 5, loss = 0.53037203\n",
      "Iteration 6, loss = 0.52005239\n",
      "Iteration 7, loss = 0.50631681\n",
      "Iteration 8, loss = 0.49594459\n",
      "Iteration 9, loss = 0.48672169\n",
      "Iteration 10, loss = 0.46703889\n",
      "Iteration 11, loss = 0.45669533\n",
      "Iteration 12, loss = 0.45153882\n",
      "Iteration 13, loss = 0.45541892\n",
      "Iteration 14, loss = 0.42007533\n",
      "Iteration 15, loss = 0.39610760\n",
      "Iteration 16, loss = 0.38342730\n",
      "Iteration 17, loss = 0.36941878\n",
      "Iteration 18, loss = 0.35278173\n",
      "Iteration 19, loss = 0.33530753\n",
      "Iteration 20, loss = 0.32415555\n",
      "Iteration 21, loss = 0.30953847\n",
      "Iteration 22, loss = 0.30881063\n",
      "Iteration 23, loss = 0.27907929\n",
      "Iteration 24, loss = 0.26256172\n",
      "Iteration 25, loss = 0.25912065\n",
      "Iteration 26, loss = 0.24294834\n",
      "Iteration 27, loss = 0.23622577\n",
      "Iteration 28, loss = 0.22197697\n",
      "Iteration 29, loss = 0.21487517\n",
      "Iteration 30, loss = 0.20740994\n",
      "Iteration 31, loss = 0.19194451\n",
      "Iteration 32, loss = 0.17995448\n",
      "Iteration 33, loss = 0.17398809\n",
      "Iteration 34, loss = 0.16621171\n",
      "Iteration 35, loss = 0.16030055\n",
      "Iteration 36, loss = 0.16640597\n",
      "Iteration 37, loss = 0.14608879\n",
      "Iteration 38, loss = 0.13872189\n",
      "Iteration 39, loss = 0.13634982\n",
      "Iteration 40, loss = 0.12786469\n",
      "Iteration 41, loss = 0.11977477\n",
      "Iteration 42, loss = 0.11612874\n",
      "Iteration 43, loss = 0.11228029\n",
      "Iteration 44, loss = 0.10986669\n",
      "Iteration 45, loss = 0.11112365\n",
      "Iteration 46, loss = 0.10764889\n",
      "Iteration 47, loss = 0.10075936\n",
      "Iteration 48, loss = 0.09494480\n",
      "Iteration 49, loss = 0.09634449\n",
      "Iteration 50, loss = 0.08897048\n",
      "Iteration 51, loss = 0.08746939\n",
      "Iteration 52, loss = 0.08867969\n",
      "Iteration 53, loss = 0.08747652\n",
      "Iteration 54, loss = 0.08875987\n",
      "Iteration 55, loss = 0.08547999\n",
      "Iteration 56, loss = 0.07704971\n",
      "Iteration 57, loss = 0.08585754\n",
      "Iteration 58, loss = 0.07753460\n",
      "Iteration 59, loss = 0.08241192\n",
      "Iteration 60, loss = 0.07592301\n",
      "Iteration 61, loss = 0.07027954\n",
      "Iteration 62, loss = 0.06949491\n",
      "Iteration 63, loss = 0.09219767\n",
      "Iteration 64, loss = 0.07269742\n",
      "Iteration 65, loss = 0.06835900\n",
      "Iteration 66, loss = 0.06981640\n",
      "Iteration 67, loss = 0.07904160\n",
      "Iteration 68, loss = 0.06701364\n",
      "Iteration 69, loss = 0.07626611\n",
      "Iteration 70, loss = 0.07456242\n",
      "Iteration 71, loss = 0.06585716\n",
      "Iteration 72, loss = 0.06567334\n",
      "Iteration 73, loss = 0.07449111\n",
      "Iteration 74, loss = 0.07200497\n",
      "Iteration 75, loss = 0.06602619\n",
      "Iteration 76, loss = 0.07219140\n",
      "Iteration 77, loss = 0.06290299\n",
      "Iteration 78, loss = 0.06094569\n",
      "Iteration 79, loss = 0.07030656\n",
      "Iteration 80, loss = 0.06011564\n",
      "Iteration 81, loss = 0.06014684\n",
      "Iteration 82, loss = 0.06584058\n",
      "Iteration 83, loss = 0.08074495\n",
      "Iteration 84, loss = 0.06470781\n",
      "Iteration 85, loss = 0.05822358\n",
      "Iteration 86, loss = 0.05638711\n",
      "Iteration 87, loss = 0.05871033\n",
      "Iteration 88, loss = 0.07634445\n",
      "Iteration 89, loss = 0.05798192\n",
      "Iteration 90, loss = 0.06817759\n",
      "Iteration 91, loss = 0.06225892\n",
      "Iteration 92, loss = 0.07385170\n",
      "Iteration 93, loss = 0.06151631\n",
      "Iteration 94, loss = 0.06086549\n",
      "Iteration 95, loss = 0.05634702\n",
      "Iteration 96, loss = 0.05537668\n",
      "Iteration 97, loss = 0.05314973\n",
      "Iteration 98, loss = 0.05201231\n",
      "Iteration 99, loss = 0.05443085\n",
      "Iteration 100, loss = 0.05299834\n",
      "Iteration 101, loss = 0.05506960\n",
      "Iteration 102, loss = 0.05417920\n",
      "Iteration 103, loss = 0.05740569\n",
      "Iteration 104, loss = 0.05175835\n",
      "Iteration 105, loss = 0.06648597\n",
      "Iteration 106, loss = 0.05848075\n",
      "Iteration 107, loss = 0.05371408\n",
      "Iteration 108, loss = 0.05359292\n",
      "Iteration 109, loss = 0.05883018\n",
      "Iteration 110, loss = 0.05480389\n",
      "Iteration 111, loss = 0.05252142\n",
      "Iteration 112, loss = 0.05201948\n",
      "Iteration 113, loss = 0.05419867\n",
      "Iteration 114, loss = 0.05417243\n",
      "Iteration 115, loss = 0.05433022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61772458\n",
      "Iteration 2, loss = 0.58130444\n",
      "Iteration 3, loss = 0.56375810\n",
      "Iteration 4, loss = 0.55268824\n",
      "Iteration 5, loss = 0.54545870\n",
      "Iteration 6, loss = 0.53737152\n",
      "Iteration 7, loss = 0.52601880\n",
      "Iteration 8, loss = 0.52130260\n",
      "Iteration 9, loss = 0.50816214\n",
      "Iteration 10, loss = 0.49680186\n",
      "Iteration 11, loss = 0.49197075\n",
      "Iteration 12, loss = 0.49107284\n",
      "Iteration 13, loss = 0.47700258\n",
      "Iteration 14, loss = 0.45961338\n",
      "Iteration 15, loss = 0.44574730\n",
      "Iteration 16, loss = 0.43842034\n",
      "Iteration 17, loss = 0.42445906\n",
      "Iteration 18, loss = 0.41246746\n",
      "Iteration 19, loss = 0.40089318\n",
      "Iteration 20, loss = 0.39272783\n",
      "Iteration 21, loss = 0.38001265\n",
      "Iteration 22, loss = 0.37522625\n",
      "Iteration 23, loss = 0.35703541\n",
      "Iteration 24, loss = 0.34917330\n",
      "Iteration 25, loss = 0.33740392\n",
      "Iteration 26, loss = 0.32623019\n",
      "Iteration 27, loss = 0.31680642\n",
      "Iteration 28, loss = 0.30553923\n",
      "Iteration 29, loss = 0.29557573\n",
      "Iteration 30, loss = 0.29079093\n",
      "Iteration 31, loss = 0.27871296\n",
      "Iteration 32, loss = 0.26437538\n",
      "Iteration 33, loss = 0.25944188\n",
      "Iteration 34, loss = 0.25012735\n",
      "Iteration 35, loss = 0.24559912\n",
      "Iteration 36, loss = 0.24358065\n",
      "Iteration 37, loss = 0.23422141\n",
      "Iteration 38, loss = 0.21875275\n",
      "Iteration 39, loss = 0.21280965\n",
      "Iteration 40, loss = 0.20281630\n",
      "Iteration 41, loss = 0.19393726\n",
      "Iteration 42, loss = 0.19186038\n",
      "Iteration 43, loss = 0.18574439\n",
      "Iteration 44, loss = 0.17729635\n",
      "Iteration 45, loss = 0.17666937\n",
      "Iteration 46, loss = 0.16932752\n",
      "Iteration 47, loss = 0.16233950\n",
      "Iteration 48, loss = 0.15611247\n",
      "Iteration 49, loss = 0.15274941\n",
      "Iteration 50, loss = 0.14949033\n",
      "Iteration 51, loss = 0.14430317\n",
      "Iteration 52, loss = 0.14340460\n",
      "Iteration 53, loss = 0.14104863\n",
      "Iteration 54, loss = 0.13299592\n",
      "Iteration 55, loss = 0.12942001\n",
      "Iteration 56, loss = 0.12256829\n",
      "Iteration 57, loss = 0.13394497\n",
      "Iteration 58, loss = 0.12104895\n",
      "Iteration 59, loss = 0.12090322\n",
      "Iteration 60, loss = 0.11340198\n",
      "Iteration 61, loss = 0.10696546\n",
      "Iteration 62, loss = 0.10610190\n",
      "Iteration 63, loss = 0.11311563\n",
      "Iteration 64, loss = 0.10403260\n",
      "Iteration 65, loss = 0.09963956\n",
      "Iteration 66, loss = 0.09714396\n",
      "Iteration 67, loss = 0.10379820\n",
      "Iteration 68, loss = 0.09285236\n",
      "Iteration 69, loss = 0.09496729\n",
      "Iteration 70, loss = 0.09000459\n",
      "Iteration 71, loss = 0.08765441\n",
      "Iteration 72, loss = 0.08663048\n",
      "Iteration 73, loss = 0.09537383\n",
      "Iteration 74, loss = 0.09216361\n",
      "Iteration 75, loss = 0.08736084\n",
      "Iteration 76, loss = 0.08995015\n",
      "Iteration 77, loss = 0.08880291\n",
      "Iteration 78, loss = 0.07927978\n",
      "Iteration 79, loss = 0.08340034\n",
      "Iteration 80, loss = 0.07678969\n",
      "Iteration 81, loss = 0.07422778\n",
      "Iteration 82, loss = 0.07906320\n",
      "Iteration 83, loss = 0.08296852\n",
      "Iteration 84, loss = 0.07754069\n",
      "Iteration 85, loss = 0.07185834\n",
      "Iteration 86, loss = 0.06987169\n",
      "Iteration 87, loss = 0.07321839\n",
      "Iteration 88, loss = 0.07864652\n",
      "Iteration 89, loss = 0.07047948\n",
      "Iteration 90, loss = 0.08568363\n",
      "Iteration 91, loss = 0.07682165\n",
      "Iteration 92, loss = 0.07918774\n",
      "Iteration 93, loss = 0.07169111\n",
      "Iteration 94, loss = 0.07118166\n",
      "Iteration 95, loss = 0.06591687\n",
      "Iteration 96, loss = 0.06527222\n",
      "Iteration 97, loss = 0.06299900\n",
      "Iteration 98, loss = 0.06179284\n",
      "Iteration 99, loss = 0.06275157\n",
      "Iteration 100, loss = 0.06309422\n",
      "Iteration 101, loss = 0.06398606\n",
      "Iteration 102, loss = 0.06072908\n",
      "Iteration 103, loss = 0.06373818\n",
      "Iteration 104, loss = 0.06020857\n",
      "Iteration 105, loss = 0.06552953\n",
      "Iteration 106, loss = 0.06489218\n",
      "Iteration 107, loss = 0.05976571\n",
      "Iteration 108, loss = 0.05841919\n",
      "Iteration 109, loss = 0.06762669\n",
      "Iteration 110, loss = 0.06151778\n",
      "Iteration 111, loss = 0.05792861\n",
      "Iteration 112, loss = 0.05683726\n",
      "Iteration 113, loss = 0.06133355\n",
      "Iteration 114, loss = 0.06016572\n",
      "Iteration 115, loss = 0.06016825\n",
      "Iteration 116, loss = 0.06648162\n",
      "Iteration 117, loss = 0.05856710\n",
      "Iteration 118, loss = 0.05783650\n",
      "Iteration 119, loss = 0.06049994\n",
      "Iteration 120, loss = 0.06186833\n",
      "Iteration 121, loss = 0.06006604\n",
      "Iteration 122, loss = 0.06089144\n",
      "Iteration 123, loss = 0.06862396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlps, accuracies = run_experiment_across_layers(mlp_probe_experiment, train_detect_neg_consistency, train_labels_det_neg_consistency, test_detect_neg_consistency, test_labels_det_neg_consistency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = generate_classification_report_all_layers(test_detect_neg_consistency, test_labels_det_neg_consistency, mlps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.78      0.72       954\n",
      "           1       0.63      0.48      0.55       721\n",
      "\n",
      "    accuracy                           0.65      1675\n",
      "   macro avg       0.65      0.63      0.63      1675\n",
      "weighted avg       0.65      0.65      0.65      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.69       954\n",
      "           1       0.59      0.53      0.56       721\n",
      "\n",
      "    accuracy                           0.64      1675\n",
      "   macro avg       0.63      0.63      0.63      1675\n",
      "weighted avg       0.64      0.64      0.64      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71       954\n",
      "           1       0.61      0.62      0.61       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.66      0.66      1675\n",
      "weighted avg       0.67      0.67      0.67      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[:3]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71       954\n",
      "           1       0.62      0.60      0.61       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.66      0.66      1675\n",
      "weighted avg       0.67      0.67      0.67      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72       954\n",
      "           1       0.62      0.56      0.59       721\n",
      "\n",
      "    accuracy                           0.66      1675\n",
      "   macro avg       0.66      0.65      0.65      1675\n",
      "weighted avg       0.66      0.66      0.66      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72       954\n",
      "           1       0.63      0.54      0.58       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.65      0.65      1675\n",
      "weighted avg       0.66      0.67      0.66      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[3:6]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72       954\n",
      "           1       0.62      0.53      0.57       721\n",
      "\n",
      "    accuracy                           0.66      1675\n",
      "   macro avg       0.65      0.64      0.64      1675\n",
      "weighted avg       0.65      0.66      0.65      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.76      0.72       954\n",
      "           1       0.62      0.53      0.57       721\n",
      "\n",
      "    accuracy                           0.66      1675\n",
      "   macro avg       0.65      0.64      0.64      1675\n",
      "weighted avg       0.66      0.66      0.65      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.73       954\n",
      "           1       0.64      0.55      0.59       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.66      0.66      1675\n",
      "weighted avg       0.67      0.67      0.67      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[6:9]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71       954\n",
      "           1       0.62      0.64      0.63       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.67      0.67      0.67      1675\n",
      "weighted avg       0.68      0.67      0.67      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72       954\n",
      "           1       0.63      0.57      0.60       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.66      0.66      1675\n",
      "weighted avg       0.67      0.67      0.67      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71       954\n",
      "           1       0.61      0.58      0.59       721\n",
      "\n",
      "    accuracy                           0.66      1675\n",
      "   macro avg       0.65      0.65      0.65      1675\n",
      "weighted avg       0.66      0.66      0.66      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[9:12]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72       954\n",
      "           1       0.63      0.57      0.59       721\n",
      "\n",
      "    accuracy                           0.67      1675\n",
      "   macro avg       0.66      0.66      0.66      1675\n",
      "weighted avg       0.67      0.67      0.67      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[12:]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Piero Gobetti used to communicate in <mask>.',\n",
       " 'The original language of Thulladha Manamum Thullum is <mask>.',\n",
       " 'Fiat Siena is produced by <mask>.',\n",
       " 'ferric phosphate consists of <mask>.',\n",
       " 'Carlos Fuentes used to communicate in <mask>.',\n",
       " 'National Assembly of Hungary is a legal term in <mask>.',\n",
       " 'Dennistoun Glacier is located in <mask>.',\n",
       " 'Sasha Krasny died in the city of <mask>.',\n",
       " 'Pakistan shares border with <mask>.',\n",
       " 'Dream with Me was written in <mask>.',\n",
       " 'Hamilton Central railway station is named after <mask>.',\n",
       " 'West Royalty, Prince Edward Island is located in <mask>.',\n",
       " 'Dyfed Archaeological Trust works in the field of <mask>.',\n",
       " 'Florian Henckel von Donnersmarck used to communicate in <mask>.',\n",
       " 'Edmund Hobhouse was born in the city of <mask>.',\n",
       " 'The official language of Idaho is <mask>.',\n",
       " 'Sir Thomas Buxton, 1st Baronet used to work in <mask>.',\n",
       " 'Pro-feminism is part of <mask>.',\n",
       " 'Yotam Halperin used to communicate in <mask>.',\n",
       " 'Newfoundland expedition is located in <mask>.',\n",
       " 'Nebraska Attorney General is a legal term in <mask>.',\n",
       " 'Albert Camus used to work in <mask>.',\n",
       " 'The official language of Malawi is <mask>.',\n",
       " 'Boston Underground Film Festival is located in <mask>.',\n",
       " 'Wismar is located in <mask>.',\n",
       " 'Boeing 777 is produced by <mask>.',\n",
       " 'Paul Cameron works in the field of <mask>.',\n",
       " 'Alfa Romeo SZ is produced by <mask>.',\n",
       " 'Google Pay Send is developed by <mask>.',\n",
       " 'The original language of White Ladder is <mask>.',\n",
       " 'Canada Permanent Resident Card is a legal term in <mask>.',\n",
       " 'The official language of Australia is <mask>.',\n",
       " 'The capital of Swedish Empire is <mask>.',\n",
       " 'Bern is the capital of <mask>.',\n",
       " 'Michelangelo Pisani di Massa e di Mormile was born in the city of <mask>.',\n",
       " 'The headquarter of AFC Bournemouth is in <mask>.',\n",
       " 'Toulouges is located in <mask>.',\n",
       " 'German Football Association is a member of <mask>.',\n",
       " 'Senate of the Philippines is a legal term in <mask>.',\n",
       " 'Belmopan is the capital of <mask>.',\n",
       " 'The native language of Jean Lefebvre is <mask>.',\n",
       " 'Microsoft Mail is developed by <mask>.',\n",
       " 'The original language of Vishwaroopam is <mask>.',\n",
       " 'The native language of Andrey Malakhov is <mask>.',\n",
       " 'Jeep Compass is produced by <mask>.',\n",
       " 'The native language of Julie Delpy is <mask>.',\n",
       " 'Lage Raho Munna Bhai was created in <mask>.',\n",
       " 'Nintendo Entertainment Analysis & Development is owned by <mask>.',\n",
       " 'Kemal Alispahi was born in the city of <mask>.',\n",
       " 'CBS This Morning was originally aired on <mask>.',\n",
       " 'The official language of Benelux is <mask>.',\n",
       " 'Askola is located in <mask>.',\n",
       " 'Ebenezer Howard used to communicate in <mask>.',\n",
       " 'The headquarter of Sydney Olympic FC is in <mask>.',\n",
       " 'Suzuki GSV-R is produced by <mask>.',\n",
       " 'Emilio Lussu used to communicate in <mask>.',\n",
       " 'Montmartre is part of <mask>.',\n",
       " 'Korea Football Association is a member of <mask>.',\n",
       " 'Adam McLean was born in the city of <mask>.',\n",
       " 'Australian Signals Directorate is a legal term in <mask>.',\n",
       " 'Volvo XC90 is produced by <mask>.',\n",
       " 'Strafgesetzbuch is a legal term in <mask>.',\n",
       " 'Rabies vaccine is a subclass of <mask>.',\n",
       " 'Gloucestershire Airport is named after <mask>.',\n",
       " 'The Waverly Wonders was originally aired on <mask>.',\n",
       " 'Bad Vilbel is located in <mask>.',\n",
       " 'Ralph McInerny used to communicate in <mask>.',\n",
       " 'Sergey Uvarov used to communicate in <mask>.',\n",
       " 'The native language of Jean Allemane is <mask>.',\n",
       " 'Cocaine is <mask>.',\n",
       " 'Garcia River is a <mask>.',\n",
       " 'Aleksey Brusilov used to communicate in <mask>.',\n",
       " 'The headquarter of The Weather Channel is in <mask>.',\n",
       " 'The official language of Khabarovsk Krai is <mask>.',\n",
       " 'Rob Owen is a <mask> by profession.',\n",
       " 'Audi R8R is produced by <mask>.',\n",
       " 'Zimbabwe Football Association is a member of <mask>.',\n",
       " 'Valeurs Actuelles was written in <mask>.',\n",
       " 'Saturn is named after <mask>.',\n",
       " 'Immortal Game is located in <mask>.',\n",
       " 'Spain is located in <mask>.',\n",
       " 'Agnolo Firenzuola used to communicate in <mask>.',\n",
       " 'Nissan 180SX is produced by <mask>.',\n",
       " 'The official language of Denmark is <mask>.',\n",
       " 'The capital of Mississippi is <mask>.',\n",
       " 'Windsor Link Railway is located in <mask>.',\n",
       " 'Alberto Mazzucato used to communicate in <mask>.',\n",
       " 'United States of America shares border with <mask>.',\n",
       " 'Esko Aho works for <mask>.',\n",
       " 'nephron is part of <mask>.',\n",
       " 'Toyota Aygo is produced by <mask>.',\n",
       " 'The native language of Jean Giraudoux is <mask>.',\n",
       " 'Theodore the Studite was born in <mask>.',\n",
       " 'Pearl High School shooting is located in <mask>.',\n",
       " 'Seoul International Cartoon and Animation Festival is located in <mask>.',\n",
       " 'The native language of Viacheslav Belavkin is <mask>.',\n",
       " 'Robert Bunsen works in the field of <mask>.',\n",
       " 'The original language of The Voice Israel is <mask>.',\n",
       " 'Colchester Community Stadium is owned by <mask>.',\n",
       " 'Yekaterina Furtseva died in <mask>.',\n",
       " 'Solicitor-General of Australia is a legal term in <mask>.',\n",
       " 'Toshiko Akiyoshi plays <mask>.',\n",
       " 'The native language of Maurice Faure is <mask>.',\n",
       " 'Paraguayan Football Association is a member of <mask>.',\n",
       " 'The official language of Catalonia is <mask>.',\n",
       " 'Lund Cathedral is located in <mask>.',\n",
       " 'Boeing 367-80 is produced by <mask>.',\n",
       " 'The official language of Cuba is <mask>.',\n",
       " 'The official language of La Chaux-de-Fonds is <mask>.',\n",
       " 'Brooklyn shares border with <mask>.',\n",
       " 'Blind Willie McTell plays <mask>.',\n",
       " 'Jacques Cousteau died in <mask>.',\n",
       " 'Pedro de Ribera used to work in <mask>.',\n",
       " 'Windows RT is developed by <mask>.',\n",
       " 'Internet Explorer 5 is developed by <mask>.',\n",
       " 'The official language of Prince Edward Island is <mask>.',\n",
       " 'John Dalton works in the field of <mask>.',\n",
       " 'Monarchy of Australia is a legal term in <mask>.',\n",
       " 'Bologna Process is named after <mask>.',\n",
       " 'Q was written in <mask>.',\n",
       " 'Google Labs is owned by <mask>.',\n",
       " 'Vientiane is the capital of <mask>.',\n",
       " 'Jules Massenet used to communicate in <mask>.',\n",
       " 'Bentley Arnage is produced by <mask>.',\n",
       " 'The Caterpillar Wish was created in <mask>.',\n",
       " 'Paul Antoine Fleuriot de Langle used to communicate in <mask>.',\n",
       " 'The capital of Anand district is <mask>.',\n",
       " 'Leon Athanese Gosselin was born in the city of <mask>.',\n",
       " 'Volvo 300 Series is produced by <mask>.',\n",
       " 'trampolining is a subclass of <mask>.',\n",
       " 'Salomon Konijn was born in the city of <mask>.',\n",
       " 'The native language of Jean-Baptiste Maunier is <mask>.',\n",
       " 'The original language of Die Tageszeitung is <mask>.',\n",
       " 'The official language of South Tyrol is <mask>.',\n",
       " 'Marc Sangnier died in <mask>.',\n",
       " 'Richard Vale was born in the city of <mask>.',\n",
       " 'Nissan R89C is produced by <mask>.',\n",
       " 'basal metabolism is a subclass of <mask>.',\n",
       " 'The native language of Lannick Gautry is <mask>.',\n",
       " 'Ultimate Christmas was written in <mask>.',\n",
       " 'The capital of Colonial Brazil is <mask>.',\n",
       " 'Sabinus of Spoleto has the position of <mask>.',\n",
       " 'Matinee Theater was originally aired on <mask>.',\n",
       " 'The official language of Donetsk Oblast is <mask>.',\n",
       " 'Dundee Airport is named after <mask>.',\n",
       " 'David Glacier is located in <mask>.',\n",
       " 'Mercury Montclair is produced by <mask>.',\n",
       " 'The capital of Estonia is <mask>.',\n",
       " 'Jernimo de Ayanz y Beaumont was of <mask> nationality.',\n",
       " 'Leinster shares border with <mask>.',\n",
       " 'Conrad Graf died in <mask>.',\n",
       " 'The headquarter of Heilmann & Littmann is in <mask>.',\n",
       " 'George Enescu used to communicate in <mask>.',\n",
       " 'feminist movement is part of <mask>.',\n",
       " 'The native language of Nikolay Dobrolyubov is <mask>.',\n",
       " 'Amaia Salamanca used to communicate in <mask>.',\n",
       " 'BBC Radio 1 is owned by <mask>.',\n",
       " 'New Zealand Football is a member of <mask>.',\n",
       " 'Kyle Rayner is a <mask>.',\n",
       " 'labour law is part of <mask>.',\n",
       " 'Olof Bj%C3%B6rner was born in the city of <mask>.',\n",
       " 'Neny Island is located in <mask>.',\n",
       " 'The native language of Pierre Reverdy is <mask>.',\n",
       " 'Brandon Jenkins (football player) plays in <mask> position.',\n",
       " 'Galway Airport is named after <mask>.',\n",
       " 'Andrew Carnegie used to communicate in <mask>.',\n",
       " 'Louisiana Film Prize is located in <mask>.',\n",
       " 'Asclepiades of Bithynia works in the field of <mask>.',\n",
       " 'Jupiter Glacier is located in <mask>.',\n",
       " 'The native language of Andreas Karkavitsas is <mask>.',\n",
       " 'Skelton Glacier is located in <mask>.',\n",
       " 'Lancia Lybra is produced by <mask>.',\n",
       " 'Honda City is produced by <mask>.',\n",
       " 'Sunset was written in <mask>.',\n",
       " 'Cardiff Blitz is located in <mask>.',\n",
       " 'Pirri plays in <mask> position.',\n",
       " 'Physical Graffiti was written in <mask>.',\n",
       " 'Football Federation Australia is a member of <mask>.',\n",
       " 'Magnum, P.I. was originally aired on <mask>.',\n",
       " 'National Council of Switzerland is a legal term in <mask>.',\n",
       " 'Lancia Musa is produced by <mask>.',\n",
       " 'Microsoft Word is developed by <mask>.',\n",
       " 'Angelica Balabanoff used to communicate in <mask>.',\n",
       " 'Stephen Orgel used to communicate in <mask>.',\n",
       " 'Cabinet of the Federal Republic of Germany is a legal term in <mask>.',\n",
       " 'Google Account is owned by <mask>.',\n",
       " 'The motion picture industry is accociated with a disctrict named <mask>.',\n",
       " 'Schytt Glacier is a <mask>.',\n",
       " 'The capital of Tripolitania is <mask>.',\n",
       " 'Toyota Soarer is produced by <mask>.',\n",
       " 'Jack Be Nimble was written in <mask>.',\n",
       " 'Charles Dixon was born in the city of <mask>.',\n",
       " 'The original language of Low-Life is <mask>.',\n",
       " 'Windows Installer is developed by <mask>.',\n",
       " 'The original language of Liquid Sky is <mask>.',\n",
       " 'Mario Monticelli used to communicate in <mask>.',\n",
       " 'Google Map Maker is owned by <mask>.',\n",
       " 'The official language of Bermuda is <mask>.',\n",
       " 'Giuseppe Tornatore used to communicate in <mask>.',\n",
       " \"Gaston d'Audiffret-Pasquier used to work in <mask>.\",\n",
       " 'The native language of Swedes is <mask>.',\n",
       " 'Son Amores was created in <mask>.',\n",
       " 'Galatasaray High School is located in <mask>.',\n",
       " 'Rocket Power was originally aired on <mask>.',\n",
       " 'Nissan Pivo is produced by <mask>.',\n",
       " 'Nokia N82 is produced by <mask>.',\n",
       " 'Toyota Paseo is produced by <mask>.',\n",
       " 'The original language of The Man Who Shot Liberty Valance is <mask>.',\n",
       " 'President of Nigeria is a legal term in <mask>.',\n",
       " 'The original language of Crossing Jordan is <mask>.',\n",
       " 'Jorum Glacier is located in <mask>.',\n",
       " 'The official language of Zimbabwe is <mask>.',\n",
       " 'The headquarter of Scotiabank is in <mask>.',\n",
       " 'Lot-et-Garonne is named after <mask>.',\n",
       " 'Renault Talisman is produced by <mask>.',\n",
       " 'Derek was written in <mask>.',\n",
       " 'Solicitor General for Scotland is a legal term in <mask>.',\n",
       " \"The capital of Finnish Socialist Workers' Republic is <mask>.\",\n",
       " 'Delaware shares border with <mask>.',\n",
       " 'Lagos is the capital of <mask>.',\n",
       " 'Bubble Guppies was originally aired on <mask>.',\n",
       " 'SEAT 133 is produced by <mask>.',\n",
       " 'The native language of Michel Robin is <mask>.',\n",
       " 'The capital of Afghan Transitional Administration is <mask>.',\n",
       " 'Edward Matthew Ward was born in <mask>.',\n",
       " 'Western Asia is located in <mask>.',\n",
       " 'Manila is the capital of <mask>.',\n",
       " 'The official language of Kingdom of the Netherlands is <mask>.',\n",
       " 'Premier of Alberta is a legal term in <mask>.',\n",
       " 'Exeter River is a <mask>.',\n",
       " 'Constitution of Texas is a legal term in <mask>.',\n",
       " 'The native language of Pierre Bayle is <mask>.',\n",
       " 'Cumhuriyet was written in <mask>.',\n",
       " 'Plamen Oresharski used to communicate in <mask>.',\n",
       " 'President of Syria is a legal term in <mask>.',\n",
       " 'L. Ron Hubbard is affiliated with the <mask> religion.',\n",
       " 'Run Lola Run was written in <mask>.',\n",
       " 'Pierre Mauroy used to work in <mask>.',\n",
       " 'Nokia 5800 XpressMusic is produced by <mask>.',\n",
       " 'president of Tajikistan is a legal term in <mask>.',\n",
       " 'K.S.V. Roeselare is located in <mask>.',\n",
       " 'The native language of Nikolay Akimov is <mask>.',\n",
       " 'Koos Vorrink died in the city of <mask>.',\n",
       " 'Lancia 037 is produced by <mask>.',\n",
       " 'Chevrolet Trax is produced by <mask>.',\n",
       " 'flag of Texas is a legal term in <mask>.',\n",
       " 'Mizoram is located in <mask>.',\n",
       " 'Fiat Ritmo is produced by <mask>.',\n",
       " 'The native language of Ginette Leclerc is <mask>.',\n",
       " 'The capital of Hampton County is <mask>.',\n",
       " 'The original language of Paul Clifford is <mask>.',\n",
       " 'Chevrolet Vega is produced by <mask>.',\n",
       " 'Illinois Attorney General is a legal term in <mask>.',\n",
       " 'Standing NATO Maritime Group One is part of <mask>.',\n",
       " 'Sallust used to communicate in <mask>.',\n",
       " 'Present Tense was written in <mask>.',\n",
       " 'Georges Hugon was born in the city of <mask>.',\n",
       " 'Potassium superoxide consists of <mask>.',\n",
       " 'Agostino Rocca used to communicate in <mask>.',\n",
       " 'C preprocessor is part of <mask>.',\n",
       " 'George Scharf died in <mask>.',\n",
       " 'The native language of Bernard Giraudeau is <mask>.',\n",
       " 'The native language of Irina Shayk is <mask>.',\n",
       " 'Vicente Espinel used to communicate in <mask>.',\n",
       " 'Le Quotidien de Paris was written in <mask>.',\n",
       " 'Adobe Premiere Pro is developed by <mask>.',\n",
       " 'stunt performer is a subclass of <mask>.',\n",
       " 'Nature was written in <mask>.',\n",
       " 'Adalard of Corbie has the position of <mask>.',\n",
       " 'U and V-class destroyer is a subclass of <mask>.',\n",
       " 'Eusebio Di Francesco plays in <mask> position.',\n",
       " 'The original language of Vathiyar is <mask>.',\n",
       " 'Lexus SC is produced by <mask>.',\n",
       " 'Niger shares border with <mask>.',\n",
       " 'The capital of Vilnius County is <mask>.',\n",
       " 'Cebu City is the capital of <mask>.',\n",
       " 'Annie Ernaux used to communicate in <mask>.',\n",
       " 'Sony E-mount is produced by <mask>.',\n",
       " 'Internet Explorer 11 is developed by <mask>.',\n",
       " 'Montour County is located in <mask>.',\n",
       " 'The native language of Jacques Roubaud is <mask>.',\n",
       " 'bpost is located in <mask>.',\n",
       " 'The Mattei Affair was created in <mask>.',\n",
       " 'Scamper is <mask>.',\n",
       " 'Norman Petty plays <mask>.',\n",
       " 'The official language of Chechen Republic is <mask>.',\n",
       " 'On Parole was written in <mask>.',\n",
       " 'Reading Rainbow was originally aired on <mask>.',\n",
       " 'Henri Victor Regnault works in the field of <mask>.',\n",
       " 'Liam Gallagher plays <mask>.',\n",
       " 'The official language of Montenegro is <mask>.',\n",
       " 'Laurence Sterne died in <mask>.',\n",
       " 'Pink Flag was written in <mask>.',\n",
       " 'Paolo Sammarco plays in <mask> position.',\n",
       " 'Giovanni Bertati used to communicate in <mask>.',\n",
       " 'Battambang Province is located in <mask>.',\n",
       " 'Morice River is a <mask>.',\n",
       " 'Claude Piel was born in the city of <mask>.',\n",
       " 'The capital of Serbia and Montenegro is <mask>.',\n",
       " 'Berlin is the capital of <mask>.',\n",
       " 'The headquarter of Communist Party of Indonesia is in <mask>.',\n",
       " 'Magdalena Mielcarz was born in <mask>.',\n",
       " 'President of Kenya is a legal term in <mask>.',\n",
       " 'Alfa Romeo MiTo is produced by <mask>.',\n",
       " 'The headquarter of Crystal Palace Baltimore is in <mask>.',\n",
       " 'Hiroshima Prefecture is named after <mask>.',\n",
       " 'Charles Fleetwood died in <mask>.',\n",
       " 'general aviation is a subclass of <mask>.',\n",
       " 'BMW X3 is produced by <mask>.',\n",
       " 'The headquarter of Montreal Light, Heat & Power is in <mask>.',\n",
       " 'Bad Wildungen is located in <mask>.',\n",
       " 'Ferrari Monza is produced by <mask>.',\n",
       " 'The capital of United Kingdom of Great Britain and Ireland is <mask>.',\n",
       " 'The original language of Cosmic Voyage is <mask>.',\n",
       " 'Tosk Albanian is part of <mask>.',\n",
       " 'Renault Estafette is produced by <mask>.',\n",
       " 'Nokia Internet tablet is produced by <mask>.',\n",
       " 'Honda Super Cub is produced by <mask>.',\n",
       " 'puff pastry was created in <mask>.',\n",
       " 'Government of Russia is a legal term in <mask>.',\n",
       " 'Beckingen is located in <mask>.',\n",
       " 'Hindu denomination is part of <mask>.',\n",
       " 'The original language of Love Actually is <mask>.',\n",
       " 'The headquarter of Alberta Government Telephones is in <mask>.',\n",
       " 'The High Chaparral was originally aired on <mask>.',\n",
       " 'Gerard Mortier used to work in <mask>.',\n",
       " 'Eastern Europe consists of <mask>.',\n",
       " 'The native language of Gustave de Beaumont is <mask>.',\n",
       " 'Chicago Golden Gloves is located in <mask>.',\n",
       " 'Kieler Nachrichten was written in <mask>.',\n",
       " 'Massachusetts Senate is a legal term in <mask>.',\n",
       " 'Adrien Albert Marie de Mun used to work in <mask>.',\n",
       " 'Stewart G. Honeck was born in the city of <mask>.',\n",
       " 'Typing is for <mask>.',\n",
       " 'The official language of Republic of Venice is <mask>.',\n",
       " 'Virtual Telecommunications Access Method is developed by <mask>.',\n",
       " 'The native language of Michel Sardou is <mask>.',\n",
       " 'Toyota Sienna is produced by <mask>.',\n",
       " 'Toyota Matrix is produced by <mask>.',\n",
       " 'Listening requires <mask>.',\n",
       " 'Barcelona May Days is located in <mask>.',\n",
       " 'Lieutenant Governor of California is a legal term in <mask>.',\n",
       " 'HuffPost was written in <mask>.',\n",
       " 'Final Fantasy Legend III is developed by <mask>.',\n",
       " 'Butler Glacier is located in <mask>.',\n",
       " 'The native language of Pierre Bourdieu is <mask>.',\n",
       " 'Aviation archaeology is a subclass of <mask>.',\n",
       " 'Samurai Pizza Cats was created in <mask>.',\n",
       " 'The native language of Dino Dvornik is <mask>.',\n",
       " 'The native language of Laurent Lafitte is <mask>.',\n",
       " 'Novosibirsk Reservoir is a <mask>.',\n",
       " 'Henri Perreyve was born in the city of <mask>.',\n",
       " 'Vclav Havel died in the city of <mask>.',\n",
       " 'For Your Pleasure was written in <mask>.',\n",
       " 'frame drum is a subclass of <mask>.',\n",
       " 'The capital of Hiroshima Prefecture is <mask>.',\n",
       " 'Ferrari 158 is produced by <mask>.',\n",
       " 'Rock and Roll Music was written in <mask>.',\n",
       " 'Charles Frodsham was born in the city of <mask>.',\n",
       " 'The official language of Kuusamo is <mask>.',\n",
       " 'Idol 2009 is part of <mask>.',\n",
       " 'Oslo Airport, Fornebu is named after <mask>.',\n",
       " 'Phillips Brooks died in <mask>.',\n",
       " 'Nokia Lumia 920 is produced by <mask>.',\n",
       " 'randomized algorithm is a subclass of <mask>.',\n",
       " 'Sina Weibo was written in <mask>.',\n",
       " 'Robert Bourne was born in the city of <mask>.',\n",
       " 'Yaropolk I of Kiev was born in <mask>.',\n",
       " \"The original language of La Fontaine's Fables is <mask>.\",\n",
       " 'The native language of Symeon of Polotsk is <mask>.',\n",
       " 'Robert Garnier used to communicate in <mask>.',\n",
       " 'Horses run <mask>.',\n",
       " 'The capital of Arizona is <mask>.',\n",
       " 'Francis Davis was born in the city of <mask>.',\n",
       " 'The capital of Sweden is <mask>.',\n",
       " 'The official language of Canton of Geneva is <mask>.',\n",
       " 'Kunming is the capital of <mask>.',\n",
       " 'Eupen-Malmedy is located in <mask>.',\n",
       " 'Ferruccio Busoni used to communicate in <mask>.',\n",
       " 'The capital of Guadalajara Province is <mask>.',\n",
       " 'Tbilisi is the capital of <mask>.',\n",
       " 'Chevrolet Corvette (C4) is produced by <mask>.',\n",
       " \"The official language of Mongolian People's Republic is <mask>.\",\n",
       " 'Audi A7 is produced by <mask>.',\n",
       " 'Puerto Rican Football Federation is a member of <mask>.',\n",
       " 'Clang is developed by <mask>.',\n",
       " 'electrostatics is part of <mask>.',\n",
       " 'Biological Chemistry was written in <mask>.',\n",
       " 'NTL Ireland was founded in <mask>.',\n",
       " 'The capital of Cumbria is <mask>.',\n",
       " 'RT America was written in <mask>.',\n",
       " 'Bernard Musson used to communicate in <mask>.',\n",
       " 'The capital of Antwerp is <mask>.',\n",
       " 'The capital of Fatimid caliphate is <mask>.',\n",
       " 'Electricidad was written in <mask>.',\n",
       " 'The headquarter of Green Party of England and Wales is in <mask>.',\n",
       " 'Treorchy is a <mask>.',\n",
       " 'The native language of William-Adolphe Bouguereau is <mask>.',\n",
       " 'The official language of San Marino is <mask>.',\n",
       " 'Stamatios Kleanthis died in <mask>.',\n",
       " 'Jean Tiberi used to work in <mask>.',\n",
       " 'Sofia is the capital of <mask>.',\n",
       " 'Adobe GoLive is developed by <mask>.',\n",
       " 'Toyota Cressida is produced by <mask>.',\n",
       " 'Jean-Jacques Grunenwald used to work in <mask>.',\n",
       " 'HSBC Bank Malta is located in <mask>.',\n",
       " 'The native language of Pierre Bec is <mask>.',\n",
       " 'Susan McClary used to communicate in <mask>.',\n",
       " 'Virginia shares border with <mask>.',\n",
       " 'Kondotty is located in <mask>.',\n",
       " 'The original language of Mad TV is <mask>.',\n",
       " 'The official language of Czech Republic is <mask>.',\n",
       " 'Airbus A380 is produced by <mask>.',\n",
       " 'Angola shares border with <mask>.',\n",
       " 'Realdo Colombo died in <mask>.',\n",
       " 'Al Cotey was born in the city of <mask>.',\n",
       " 'steel consists of <mask>.',\n",
       " 'The original language of Westworld is <mask>.',\n",
       " 'Sands of Beirut is located in <mask>.',\n",
       " 'Glastonbury Lake Village is located in <mask>.',\n",
       " 'Lancaster railway station is named after <mask>.',\n",
       " 'mathematical logic is part of <mask>.',\n",
       " 'The headquarter of Tusker F.C. is in <mask>.',\n",
       " 'rpd Sos was born in the city of <mask>.',\n",
       " 'The official language of President of the Basque Autonomous Community is <mask>.',\n",
       " 'Leonid Sabaneyev was born in <mask>.',\n",
       " 'John Frederick Maurice was born in <mask>.',\n",
       " 'borosilicate glass is a subclass of <mask>.',\n",
       " 'The official language of Montserrat is <mask>.',\n",
       " 'Teres Ridge is located in <mask>.',\n",
       " 'Aktiengesellschaft is a legal term in <mask>.',\n",
       " 'Vietnam Football Funny is a member of <mask>.',\n",
       " 'The official language of Somalia is <mask>.',\n",
       " 'Pori Jazz plays <mask> music.',\n",
       " 'Stefanie van Vliet was born in the city of <mask>.',\n",
       " 'George-Daniel de Monfreid used to communicate in <mask>.',\n",
       " 'Siege of Sarajevo is located in <mask>.',\n",
       " 'Mauritius Police Force is a legal term in <mask>.',\n",
       " 'The original language of Southcliffe is <mask>.',\n",
       " 'Keltie Glacier is located in <mask>.',\n",
       " 'The native language of Paul Collin is <mask>.',\n",
       " 'Penna Ahobilam is located in <mask>.',\n",
       " 'Sanford and Son was originally aired on <mask>.',\n",
       " 'Constitution of Arkansas is a legal term in <mask>.',\n",
       " 'Easter Vigil is named after <mask>.',\n",
       " 'The David Letterman Show was originally aired on <mask>.',\n",
       " 'The capital of Wellington Region is <mask>.',\n",
       " 'Jean Crotti used to communicate in <mask>.',\n",
       " 'George Cadogan, 5th Earl Cadogan used to work in <mask>.',\n",
       " 'Black Mountain Side was written in <mask>.',\n",
       " 'Turkish Military Academy is located in <mask>.',\n",
       " 'Nino Rota used to communicate in <mask>.',\n",
       " 'Yelena Belyakova was born in the city of <mask>.',\n",
       " 'The headquarter of Royal Society of Edinburgh is in <mask>.',\n",
       " 'Windows NT is developed by <mask>.',\n",
       " 'Internet censorship is a subclass of <mask>.',\n",
       " 'China Railways CRH3 is produced by <mask>.',\n",
       " 'The official language of Porvoo is <mask>.',\n",
       " 'Jean-Charles Pichegru died in <mask>.',\n",
       " 'The official language of South Carolina is <mask>.',\n",
       " 'City of London is the capital of <mask>.',\n",
       " 'Spyros Sofos was born in the city of <mask>.',\n",
       " 'Suzanne Malherbe used to communicate in <mask>.',\n",
       " 'Audi RSQ is produced by <mask>.',\n",
       " 'IBM ViaVoice is developed by <mask>.',\n",
       " 'The headquarter of Czechoslovak Television is in <mask>.',\n",
       " 'Nissan Be-1 is produced by <mask>.',\n",
       " 'The official language of Haiti is <mask>.',\n",
       " 'Cormorant is a <mask>.',\n",
       " 'Jean Bourdichon used to communicate in <mask>.',\n",
       " 'El Tarf Province is located in <mask>.',\n",
       " 'The native language of Alain Savary is <mask>.',\n",
       " 'Toyota Corolla E140 is produced by <mask>.',\n",
       " 'Pulp Fiction was written in <mask>.',\n",
       " 'Vidovdan is a legal term in <mask>.',\n",
       " 'The original language of Easy Rider is <mask>.',\n",
       " 'Le Petit Parisien was written in <mask>.',\n",
       " 'The official language of South Ossetia is <mask>.',\n",
       " 'Super Audio CD is owned by <mask>.',\n",
       " 'Italian Chemical Society works in the field of <mask>.',\n",
       " 'Antonio Vivaldi plays <mask>.',\n",
       " 'Thomas Whitelegg died in <mask>.',\n",
       " 'Libyan Football Federation is a member of <mask>.',\n",
       " 'Middle Irish is a subclass of <mask>.',\n",
       " 'Resource Access Control Facility is developed by <mask>.',\n",
       " \"The Supremes A' Go-Go is represented by music label <mask>.\",\n",
       " 'The official language of Central Visayas is <mask>.',\n",
       " 'The original language of The Sarah Jane Adventures is <mask>.',\n",
       " 'The original language of Amar en tiempos revueltos is <mask>.',\n",
       " 'The native language of Jules Roy is <mask>.',\n",
       " 'Miles Fitzalan-Howard, 17th Duke of Norfolk was born in <mask>.',\n",
       " 'quantum optics is part of <mask>.',\n",
       " 'Renault R26 is produced by <mask>.',\n",
       " 'The capital of Gironde is <mask>.',\n",
       " 'Antonio Soler used to communicate in <mask>.',\n",
       " '1996 Summer Olympics is located in <mask>.',\n",
       " 'Bionz is produced by <mask>.',\n",
       " 'Great Tenasserim River is a <mask>.',\n",
       " 'President of Cuba is a legal term in <mask>.',\n",
       " 'The capital of Central Bohemian Region is <mask>.',\n",
       " 'Monster Rancher was created in <mask>.',\n",
       " 'Hey Dude was originally aired on <mask>.',\n",
       " 'The official language of Wappo people is <mask>.',\n",
       " '1st Foreign Regiment is a <mask>.',\n",
       " 'John Roland Abbey died in the city of <mask>.',\n",
       " 'Wirth Peninsula is located in <mask>.',\n",
       " 'Anthrax vaccines is a subclass of <mask>.',\n",
       " 'Bruce County is located in <mask>.',\n",
       " 'Henri Storck used to communicate in <mask>.',\n",
       " 'Kraisak Choonhavan was born in the city of <mask>.',\n",
       " 'Michel Peissel used to communicate in <mask>.',\n",
       " 'Google Health is developed by <mask>.',\n",
       " 'Spirit River Formation is a <mask>.',\n",
       " 'Nissan Cima is produced by <mask>.',\n",
       " 'Aeronautical Information Service works in the field of <mask>.',\n",
       " 'Justice Minister of Denmark is a legal term in <mask>.',\n",
       " 'The official language of Yalta is <mask>.',\n",
       " 'Federal Palace of Switzerland is located in <mask>.',\n",
       " 'Uyu River is a <mask>.',\n",
       " 'Sheridan Morley died in <mask>.',\n",
       " 'The official language of Illinois is <mask>.',\n",
       " 'BBC One is owned by <mask>.',\n",
       " 'Football Federation of Ukraine is a member of <mask>.',\n",
       " 'sociological theory is part of <mask>.',\n",
       " 'animal husbandry is part of <mask>.',\n",
       " 'The capital of King County is <mask>.',\n",
       " 'Samoa maintains diplomatic relations with <mask>.',\n",
       " 'Active Directory is developed by <mask>.',\n",
       " 'The headquarter of Shanghai Jiao Tong University is in <mask>.',\n",
       " 'Matapdia River is a <mask>.',\n",
       " 'Thomas Wroth was born in the city of <mask>.',\n",
       " 'Prime Minister of Malaysia is a legal term in <mask>.',\n",
       " 'Saint Lucia Football Association is a member of <mask>.',\n",
       " 'Video CD is owned by <mask>.',\n",
       " 'The Gold-Bug was written in <mask>.',\n",
       " 'Nagarik was created in <mask>.',\n",
       " 'The headquarter of James Purdey & Sons is in <mask>.',\n",
       " 'The capital of Ottoman Empire is <mask>.',\n",
       " 'Parviz Davoodi was born in <mask>.',\n",
       " 'Uppdrag granskning was created in <mask>.',\n",
       " 'Final Fantasy X is developed by <mask>.',\n",
       " 'Ostankino Tower is located in <mask>.',\n",
       " 'Otway Massif is located in <mask>.',\n",
       " 'Sergey Ryazansky used to communicate in <mask>.',\n",
       " 'Mexico City and <mask> are twin cities.',\n",
       " 'The native language of Vladimir Mayakovsky is <mask>.',\n",
       " 'Marcel Tournier died in <mask>.',\n",
       " 'The capital of Republic of Genova is <mask>.',\n",
       " 'The original language of Front Page Challenge is <mask>.',\n",
       " 'Windows Media Encoder is developed by <mask>.',\n",
       " 'Frantiek Neuwirt was born in the city of <mask>.',\n",
       " 'London Independent Film Festival is located in <mask>.',\n",
       " 'The capital of Province of Venice is <mask>.',\n",
       " 'ferrous sulfate consists of <mask>.',\n",
       " 'Microsoft SQL Server is developed by <mask>.',\n",
       " 'Kentucky Senate is a legal term in <mask>.',\n",
       " 'jollof rice consists of <mask>.',\n",
       " '1992 Summer Olympics is located in <mask>.',\n",
       " 'COBOL is a <mask>.',\n",
       " 'The native language of Pierre-Joseph Cambon is <mask>.',\n",
       " 'Maxence Caron used to communicate in <mask>.',\n",
       " 'The original language of Brookside is <mask>.',\n",
       " 'Prime Minister of Iraq is a legal term in <mask>.',\n",
       " 'Honda S600 is produced by <mask>.',\n",
       " 'The headquarter of RB Leipzig is in <mask>.',\n",
       " 'LGM-30 Minuteman is produced by <mask>.',\n",
       " 'BMW X1 is produced by <mask>.',\n",
       " 'Adobe Digital Editions is developed by <mask>.',\n",
       " 'Danish Maastricht Treaty referendum is a <mask>.',\n",
       " 'Vilenica Cave is a <mask>.',\n",
       " 'Lszl Z. Bit was born in the city of <mask>.',\n",
       " 'Chester railway station is named after <mask>.',\n",
       " 'BBC Radio 2 is owned by <mask>.',\n",
       " 'John Broadwood died in <mask>.',\n",
       " 'Paul Deschanel used to work in <mask>.',\n",
       " 'Robert Lecou was born in the city of <mask>.',\n",
       " 'Kansas shares border with <mask>.',\n",
       " 'Stolen Apples is represented by music label <mask>.',\n",
       " 'The official language of Kyto Prefecture is <mask>.',\n",
       " 'The original language of Moondru Per Moondru Kaadhal is <mask>.',\n",
       " 'The native language of Ger van Elk is <mask>.',\n",
       " 'York Cemetery, York is a <mask>.',\n",
       " 'BMW Z3 is produced by <mask>.',\n",
       " 'Beirut is the capital of <mask>.',\n",
       " 'Chevrolet Delray is produced by <mask>.',\n",
       " 'Art blog is a subclass of <mask>.',\n",
       " 'President of the Executive Yuan is a legal term in <mask>.',\n",
       " 'Toyota Highlander is produced by <mask>.',\n",
       " 'People can hill <mask>.',\n",
       " 'Daniel Alfredson was born in <mask>.',\n",
       " 'Football Association of Thailand is a member of <mask>.',\n",
       " 'Google Code Search is owned by <mask>.',\n",
       " 'Brown University is located in <mask>.',\n",
       " 'Virginia General Assembly is a legal term in <mask>.',\n",
       " 'Fiat Idea is produced by <mask>.',\n",
       " 'Goffredo Petrassi died in <mask>.',\n",
       " 'Ferrari 288 GTO is produced by <mask>.',\n",
       " 'Bern Theatre is located in <mask>.',\n",
       " 'Wisconsin is a <mask>.',\n",
       " 'Kampong Thom Province is located in <mask>.',\n",
       " 'Marin Barbu was born in <mask>.',\n",
       " 'Ferrari 212 Inter is produced by <mask>.',\n",
       " 'The official language of Altai Republic is <mask>.',\n",
       " 'Medical Investigation was originally aired on <mask>.',\n",
       " 'Rafael Jofresa used to communicate in <mask>.',\n",
       " 'Roger Caillois used to communicate in <mask>.',\n",
       " 'Prachin Buri River is located in <mask>.',\n",
       " 'The capital of Metropolis of Lyon is <mask>.',\n",
       " 'Siemens Desiro is produced by <mask>.',\n",
       " 'Skylar Grey plays <mask>.',\n",
       " 'Anastasia Volochkova used to communicate in <mask>.',\n",
       " 'Maryland General Assembly is a legal term in <mask>.',\n",
       " 'Laisse tomber les filles was written in <mask>.',\n",
       " 'Binary Synchronous Communications is developed by <mask>.',\n",
       " 'Bulls have <mask>.',\n",
       " 'tin(II) oxide consists of <mask>.',\n",
       " 'The original language of The Telegraph is <mask>.',\n",
       " 'Grand Rapids Hotel is a <mask>.',\n",
       " 'The capital of Nord-Pas-de-Calais is <mask>.',\n",
       " 'Spain shares border with <mask>.',\n",
       " 'Ukraine maintains diplomatic relations with <mask>.',\n",
       " 'manual transmission is a subclass of <mask>.',\n",
       " 'Crimes Act 1914 is a legal term in <mask>.',\n",
       " 'IBM 407 is produced by <mask>.',\n",
       " 'Google Maps is owned by <mask>.',\n",
       " 'Kitchen Princess was created in <mask>.',\n",
       " 'Azumanga Daioh was created in <mask>.',\n",
       " 'Pitchfork was written in <mask>.',\n",
       " 'Croatia is a member of <mask>.',\n",
       " 'The headquarter of Pontifical Catholic University of Chile is in <mask>.',\n",
       " 'Theobald of Bec has the position of <mask>.',\n",
       " 'The capital of Allied-occupied Germany is <mask>.',\n",
       " 'Australian Federal Police is a legal term in <mask>.',\n",
       " 'Live After Death was written in <mask>.',\n",
       " 'Marc Ribot plays <mask>.',\n",
       " 'Finnish Civil War is located in <mask>.',\n",
       " 'Mongolian Football Federation is a member of <mask>.',\n",
       " 'The capital of Stockholm County is <mask>.',\n",
       " 'Sometimes learning causes <mask>.',\n",
       " 'The capital of Lancashire is <mask>.',\n",
       " 'civil aviation is a subclass of <mask>.',\n",
       " 'Windows Phone 7 is produced by <mask>.',\n",
       " 'female education is part of <mask>.',\n",
       " '2012 Assam violence is located in <mask>.',\n",
       " 'J-pop was created in <mask>.',\n",
       " 'polio vaccine is a subclass of <mask>.',\n",
       " 'Ruth Fuller Sasaki was born in the city of <mask>.',\n",
       " 'New Delhi is the capital of <mask>.',\n",
       " 'Honnavar is located in <mask>.',\n",
       " 'Yamaha Royal Star Venture is produced by <mask>.',\n",
       " 'Libertad Digital was written in <mask>.',\n",
       " 'Intel i960 is developed by <mask>.',\n",
       " 'Yellow Sea is a <mask>.',\n",
       " 'The official language of Byelorussian Soviet Socialist Republic is <mask>.',\n",
       " 'Renault 19 is produced by <mask>.',\n",
       " 'sulfur dichloride consists of <mask>.',\n",
       " 'Nigeria shares border with <mask>.',\n",
       " 'The official language of Mississippi is <mask>.',\n",
       " 'The native language of Louis Nicolas Vauquelin is <mask>.',\n",
       " 'Suzuki APV is produced by <mask>.',\n",
       " 'Wild Dances was written in <mask>.',\n",
       " 'Cecilia Bartoli used to communicate in <mask>.',\n",
       " 'Norman Lamont used to work in <mask>.',\n",
       " 'iron(III) oxide consists of <mask>.',\n",
       " 'Zagreb Film Festival is located in <mask>.',\n",
       " 'Honda RS125R is produced by <mask>.',\n",
       " 'IBM 402 is produced by <mask>.',\n",
       " 'Ceramic is made of <mask>.',\n",
       " 'Nivelle Offensive is located in <mask>.',\n",
       " 'Late Night with David Letterman was originally aired on <mask>.',\n",
       " 'Google Translate is owned by <mask>.',\n",
       " 'The capital of Mary Region is <mask>.',\n",
       " 'Yasir Butt was born in the city of <mask>.',\n",
       " 'William Wills, 1st Baron Winterstoke used to work in <mask>.',\n",
       " 'The Sopranos was originally aired on <mask>.',\n",
       " 'The capital of Anatolia Eyalet is <mask>.',\n",
       " 'The X Factor was created in <mask>.',\n",
       " 'Witold Nazarewicz was born in the city of <mask>.',\n",
       " 'Jacques Poulin used to communicate in <mask>.',\n",
       " 'Senate of Australia is a legal term in <mask>.',\n",
       " 'The Man-Machine was written in <mask>.',\n",
       " 'Istanbul Technical University is located in <mask>.',\n",
       " 'The capital of State of Brazil is <mask>.',\n",
       " 'The native language of Michel Godard is <mask>.',\n",
       " 'subgenus is part of <mask>.',\n",
       " 'BMW 3 Series is produced by <mask>.',\n",
       " 'Cleveland Stadium is owned by <mask>.',\n",
       " 'The native language of Yuri Knorozov is <mask>.',\n",
       " 'Dimitri Kirsanoff died in <mask>.',\n",
       " 'Music is a <mask>.',\n",
       " 'Nino Bravo used to communicate in <mask>.',\n",
       " 'Chrysler 300C is produced by <mask>.',\n",
       " 'Honda CX series is produced by <mask>.',\n",
       " 'Panathinaikos B.C. is located in <mask>.',\n",
       " 'Crying is part of <mask>.',\n",
       " 'The capital of Saskatchewan is <mask>.',\n",
       " 'zinc sulfide consists of <mask>.',\n",
       " 'The official language of France is <mask>.',\n",
       " 'David Island is located in <mask>.',\n",
       " 'BMW M62 is produced by <mask>.',\n",
       " 'Fiat 4 HP is produced by <mask>.',\n",
       " 'Thomas Gilovich works in the field of <mask>.',\n",
       " 'Soviet invasion of Poland is located in <mask>.',\n",
       " 'Slovenia shares border with <mask>.',\n",
       " 'Lizards have a <mask>.',\n",
       " 'The capital of Connecticut is <mask>.',\n",
       " 'The capital of City of Hawkesbury is <mask>.',\n",
       " 'The capital of Colorado is <mask>.',\n",
       " 'double track is a subclass of <mask>.',\n",
       " 'Kampala is the capital of <mask>.',\n",
       " 'President of Moldova is a legal term in <mask>.',\n",
       " 'Audi S4 is produced by <mask>.',\n",
       " 'Audi A8 is produced by <mask>.',\n",
       " 'Dakota War of 1862 is located in <mask>.',\n",
       " 'The official language of Eswatini is <mask>.',\n",
       " 'Munich is the capital of <mask>.',\n",
       " 'Joaquin Mir Trinxet used to communicate in <mask>.',\n",
       " 'Anaxagoras works in the field of <mask>.',\n",
       " 'Turner & Hooch was written in <mask>.',\n",
       " 'The headquarter of University of Detroit Mercy is in <mask>.',\n",
       " 'The official language of Burundi is <mask>.',\n",
       " 'EA-18G Growler is developed by <mask>.',\n",
       " 'The original language of Flo is <mask>.',\n",
       " 'Google Voice is owned by <mask>.',\n",
       " 'iGoogle is owned by <mask>.',\n",
       " 'The official language of Liestal is <mask>.',\n",
       " 'Friction produces <mask>.',\n",
       " 'Kyto Prefecture is located in <mask>.',\n",
       " 'The native language of Chen Xiaoxu is <mask>.',\n",
       " 'Evgeny Lifshitz died in <mask>.',\n",
       " 'Roland Freisler died in <mask>.',\n",
       " 'Adobe Fireworks is developed by <mask>.',\n",
       " 'George Mavrotas was born in the city of <mask>.',\n",
       " 'Burma Rifles is a <mask>.',\n",
       " 'Governor-General of Australia is a legal term in <mask>.',\n",
       " 'The native language of Nadezhda Krupskaya is <mask>.',\n",
       " 'Vimala Raman used to communicate in <mask>.',\n",
       " 'Bill Haley & His Comets is represented by music label <mask>.',\n",
       " 'fuel pump is a subclass of <mask>.',\n",
       " 'Domenico Passignano died in the city of <mask>.',\n",
       " 'Abdel Sattar Sabry plays in <mask> position.',\n",
       " 'Seattle International Film Festival is located in <mask>.',\n",
       " 'Chevrolet Nomad is produced by <mask>.',\n",
       " 'Regina Markov was born in the city of <mask>.',\n",
       " 'The native language of Ingrian Finns is <mask>.',\n",
       " 'WIRED Science was originally aired on <mask>.',\n",
       " 'Marga van Praag was born in the city of <mask>.',\n",
       " 'The native language of Bernard Musson is <mask>.',\n",
       " 'Pallacanestro Virtus Roma is located in <mask>.',\n",
       " 'Photinus of Sirmium has the position of <mask>.',\n",
       " 'Singapore maintains diplomatic relations with <mask>.',\n",
       " 'A Dangerous Life was created in <mask>.',\n",
       " 'semiotics is part of <mask>.',\n",
       " 'The native language of Vincent Rottiers is <mask>.',\n",
       " 'Saudi Arabia is located in <mask>.',\n",
       " 'Napoleon III used to work in <mask>.',\n",
       " 'Kip King was born in the city of <mask>.',\n",
       " 'Prime Minister of Turkey is a legal term in <mask>.',\n",
       " 'Rich Text Format is developed by <mask>.',\n",
       " 'The official language of Bali is <mask>.',\n",
       " 'The headquarter of Compagnie des Transports Strasbourgeois is in <mask>.',\n",
       " 'Paulus Manutius used to work in <mask>.',\n",
       " 'George Osborne used to work in <mask>.',\n",
       " 'The official language of Oklahoma is <mask>.',\n",
       " 'The official language of Le Locle is <mask>.',\n",
       " 'Lopo Soares de Albergaria was born in <mask>.',\n",
       " 'Moscow theater hostage crisis is located in <mask>.',\n",
       " 'Svenska Hollywoodfruar was created in <mask>.',\n",
       " 'Secretos was written in <mask>.',\n",
       " 'caipirinha was created in <mask>.',\n",
       " 'Honda NSX is produced by <mask>.',\n",
       " 'The native language of Fyodor Pavlovich Reshetnikov is <mask>.',\n",
       " 'CNN Center is located in <mask>.',\n",
       " 'neurosurgery is part of <mask>.',\n",
       " 'The official language of Paris Commune is <mask>.',\n",
       " '2011 German Masters is located in <mask>.',\n",
       " 'Pierre Chenal used to communicate in <mask>.',\n",
       " 'Torgeir Micaelsen was born in the city of <mask>.',\n",
       " 'Suzuki Jimny is produced by <mask>.',\n",
       " 'Bronx County Courthouse is a <mask>.',\n",
       " 'The original language of Galpa Holeo Satyi is <mask>.',\n",
       " 'Vermont Senate is a legal term in <mask>.',\n",
       " 'Nissan Serena is produced by <mask>.',\n",
       " 'Clayton-Marsh Creek-Greenville Fault is a <mask>.',\n",
       " 'The capital of Islamic State of Afghanistan is <mask>.',\n",
       " 'IBM 704 is produced by <mask>.',\n",
       " 'Russian Wikipedia was written in <mask>.',\n",
       " 'propylene consists of <mask>.',\n",
       " 'The native language of Aleksandr Ptushko is <mask>.',\n",
       " 'Jan Antonn Duchoslav was born in the city of <mask>.',\n",
       " 'The official language of Bellinzona is <mask>.',\n",
       " 'Dainik Bhaskar was written in <mask>.',\n",
       " 'Saint George River is a <mask>.',\n",
       " 'The Next Day was written in <mask>.',\n",
       " 'WWE NXT is owned by <mask>.',\n",
       " 'Battle of San Marcial is located in <mask>.',\n",
       " 'Congress of Guatemala is a legal term in <mask>.',\n",
       " 'Prime Minister of Portugal is a legal term in <mask>.',\n",
       " 'New Amsterdam is named after <mask>.',\n",
       " 'The native language of Christophe Moreau is <mask>.',\n",
       " 'The original language of Adventure Time is <mask>.',\n",
       " 'Food Safari was created in <mask>.',\n",
       " 'Kenelm Digby died in <mask>.',\n",
       " 'The original language of Brice de Nice is <mask>.',\n",
       " 'Lexus is owned by <mask>.',\n",
       " 'The original language of Air Mata Iboe is <mask>.',\n",
       " 'Steve Biko used to communicate in <mask>.',\n",
       " 'The official language of Brunei is <mask>.',\n",
       " 'The capital of Queensland is <mask>.',\n",
       " 'arithmetic is part of <mask>.',\n",
       " 'The native language of Roland Armontel is <mask>.',\n",
       " 'The A-Team was originally aired on <mask>.',\n",
       " 'The official language of Haparanda Municipality is <mask>.',\n",
       " 'Delaware Byways is located in <mask>.',\n",
       " 'Innocent XI died in <mask>.',\n",
       " 'Nepal shares border with <mask>.',\n",
       " 'flag of Italy is a legal term in <mask>.',\n",
       " 'The native language of Polaire is <mask>.',\n",
       " 'Apocalypse Meow was created in <mask>.',\n",
       " 'The native language of Adolphe Pinard is <mask>.',\n",
       " 'Adelaide Airport is named after <mask>.',\n",
       " 'blood soup is a subclass of <mask>.',\n",
       " 'Sound Transit was founded in <mask>.',\n",
       " 'IBM 8100 is produced by <mask>.',\n",
       " 'Claude-Thomas Dupuy was born in <mask>.',\n",
       " 'Ivory Coast shares border with <mask>.',\n",
       " 'The official language of Zabaykalsky Krai is <mask>.',\n",
       " 'Adam Bartsch used to work in <mask>.',\n",
       " 'Charlie Palmieri plays <mask>.',\n",
       " 'Romain Grosjean used to communicate in <mask>.',\n",
       " 'The native language of Emmanuelle Arsan is <mask>.',\n",
       " \"women's basketball is a subclass of <mask>.\",\n",
       " 'Some closets is made of <mask>.',\n",
       " 'Alexandre Lagoya plays <mask>.',\n",
       " 'Tomaso Albinoni plays <mask>.',\n",
       " 'Olivier Dahan used to communicate in <mask>.',\n",
       " 'The official language of Sweden is <mask>.',\n",
       " 'Samuil Marshak died in <mask>.',\n",
       " 'Nissan President is produced by <mask>.',\n",
       " 'The original language of La Stampa is <mask>.',\n",
       " 'Debenham Glacier is located in <mask>.',\n",
       " 'Leicestershire shares border with <mask>.',\n",
       " 'Arutz Sheva was written in <mask>.',\n",
       " 'Portsmouth Airport, Hampshire is named after <mask>.',\n",
       " 'Holy Name Cathedral, Chicago is located in <mask>.',\n",
       " 'The original language of Film International is <mask>.',\n",
       " 'South Carolina shares border with <mask>.',\n",
       " 'Jean-Baptiste Marchand used to communicate in <mask>.',\n",
       " 'Catscratch was originally aired on <mask>.',\n",
       " 'The original language of The Wardstone Chronicles is <mask>.',\n",
       " 'Belgium shares border with <mask>.',\n",
       " 'Sudan Football Association is a member of <mask>.',\n",
       " 'Creswell, Derbyshire is a <mask>.',\n",
       " 'Mississippi shares border with <mask>.',\n",
       " 'Douglas X-3 Stiletto is produced by <mask>.',\n",
       " 'Windows Server 2008 R2 is developed by <mask>.',\n",
       " 'The original language of Spectacular! is <mask>.',\n",
       " 'Honda Valkyrie is produced by <mask>.',\n",
       " 'The native language of Valentin Glushko is <mask>.',\n",
       " 'Vincent Strambi died in <mask>.',\n",
       " 'The official language of Gothenburg Municipality is <mask>.',\n",
       " 'Roy Orbison used to communicate in <mask>.',\n",
       " 'Hellenic Film Academy Awards is located in <mask>.',\n",
       " 'Vienna is the capital of <mask>.',\n",
       " 'How I Met Your Mother was originally aired on <mask>.',\n",
       " 'The capital of Germany is <mask>.',\n",
       " 'Kuwait is located in <mask>.',\n",
       " 'Toyota Carina is produced by <mask>.',\n",
       " 'The capital of Henrico County is <mask>.',\n",
       " 'The headquarter of University of Otago is in <mask>.',\n",
       " 'Star of David is named after <mask>.',\n",
       " 'Audi Q3 is produced by <mask>.',\n",
       " 'Ketil Hvoslef was born in the city of <mask>.',\n",
       " 'Irvine High School is located in <mask>.',\n",
       " 'The official language of South Georgia and the South Sandwich Islands is <mask>.',\n",
       " 'The official language of Grand-Saconnex is <mask>.',\n",
       " 'The official language of Chardonne is <mask>.',\n",
       " 'Roland Orzabal plays <mask>.',\n",
       " 'Vadis Odjidja-Ofoe plays in <mask> position.',\n",
       " 'Allan Warren used to communicate in <mask>.',\n",
       " 'BBC Red Button is owned by <mask>.',\n",
       " 'iPhone 1 is produced by <mask>.',\n",
       " 'optical telescope is a subclass of <mask>.',\n",
       " 'Polish Press Agency was founded in <mask>.',\n",
       " 'The capital of French Algeria is <mask>.',\n",
       " 'Charles Coll was born in the city of <mask>.',\n",
       " 'Revolution Analytics is owned by <mask>.',\n",
       " 'zinc chloride consists of <mask>.',\n",
       " 'Theodore J. van den Broek was born in the city of <mask>.',\n",
       " 'Zenon Nowosz was born in the city of <mask>.',\n",
       " 'The official language of Panama is <mask>.',\n",
       " 'Claude Weisz was born in the city of <mask>.',\n",
       " 'Honda FCX Clarity is produced by <mask>.',\n",
       " 'Edmonton Gardens is owned by <mask>.',\n",
       " 'Albert Richardson was born in <mask>.',\n",
       " 'Petras Klimas used to communicate in <mask>.',\n",
       " 'Victoria Land is named after <mask>.',\n",
       " 'Parliament of Finland is a legal term in <mask>.',\n",
       " 'Phoenix Sky Harbor International Airport is owned by <mask>.',\n",
       " 'organizational theory is part of <mask>.',\n",
       " 'atmosphere of Earth is part of <mask>.',\n",
       " 'Scotland Today was created in <mask>.',\n",
       " 'The original language of Anand Math is <mask>.',\n",
       " 'Cathal Goulding was born in <mask>.',\n",
       " 'tobacco smoking is a subclass of <mask>.',\n",
       " 'A Quick One was written in <mask>.',\n",
       " 'digital art is a subclass of <mask>.',\n",
       " 'Muscat is the capital of <mask>.',\n",
       " 'Alain Deloche was born in the city of <mask>.',\n",
       " 'Franz Werfel used to work in <mask>.',\n",
       " 'The headquarter of Alberta University of the Arts is in <mask>.',\n",
       " 'The native language of Vasily Kamensky is <mask>.',\n",
       " 'fish sauce consists of <mask>.',\n",
       " 'Texas House of Representatives is a legal term in <mask>.',\n",
       " 'The native language of Xavier Beauvois is <mask>.',\n",
       " 'Gmina Warlubie is located in <mask>.',\n",
       " 'The Confederation of the Rhine was established by <mask>.',\n",
       " 'The native language of Agoston Haraszthy is <mask>.',\n",
       " 'The native language of Jean Follain is <mask>.',\n",
       " 'Idol 2011 is part of <mask>.',\n",
       " 'Nokia X is produced by <mask>.',\n",
       " 'Lupin III was created in <mask>.',\n",
       " 'Jean-Baptiste Henri Lacordaire used to work in <mask>.',\n",
       " 'Taiwan (ROC) Resident Certificate is a legal term in <mask>.',\n",
       " 'Je te rends ton amour was written in <mask>.',\n",
       " 'Nash embedding theorem is a <mask>.',\n",
       " 'The headquarter of Goa ISL team is in <mask>.',\n",
       " 'Gulf War is located in <mask>.',\n",
       " 'The native language of Marcel Marceau is <mask>.',\n",
       " 'Christine Angot used to communicate in <mask>.',\n",
       " 'Sword is made of <mask>.',\n",
       " 'Croom, County Limerick is located in <mask>.',\n",
       " 'Sam Benedict was originally aired on <mask>.',\n",
       " 'Fiat Strada is produced by <mask>.',\n",
       " 'Le Gaulois was written in <mask>.',\n",
       " 'The capital of Veneto is <mask>.',\n",
       " 'The native language of Pierre Lescure is <mask>.',\n",
       " 'Victoria International Airport is named after <mask>.',\n",
       " 'Avaldsnes is a <mask>.',\n",
       " 'Galina Starovoytova used to work in <mask>.',\n",
       " 'The native language of Vera Brezhneva is <mask>.',\n",
       " 'The headquarter of American Bar Association is in <mask>.',\n",
       " 'Raquel Morell used to communicate in <mask>.',\n",
       " 'Jeep Commander is produced by <mask>.',\n",
       " 'The headquarter of Common Purpose UK is in <mask>.',\n",
       " 'The original language of Commentarii de Bello Gallico is <mask>.',\n",
       " 'Renault Sport Spider is produced by <mask>.',\n",
       " 'Snickers consists of <mask>.',\n",
       " 'Yahoo! HotJobs is owned by <mask>.',\n",
       " 'The original language of Redfern Now is <mask>.',\n",
       " \"Giovanni di Bicci de' Medici died in <mask>.\",\n",
       " 'Nintendo 3DS is developed by <mask>.',\n",
       " 'Naruto Strait is located in <mask>.',\n",
       " 'advanced mathematics is a subclass of <mask>.',\n",
       " 'The native language of Paul Bonifas is <mask>.',\n",
       " 'Adobe Muse is developed by <mask>.',\n",
       " 'The headquarter of Valencia CF Mestalla is in <mask>.',\n",
       " \"Ippolito d'Este has the position of <mask>.\",\n",
       " 'IA-64 is developed by <mask>.',\n",
       " 'David Isberg plays <mask>.',\n",
       " 'Alien Storm is developed by <mask>.',\n",
       " 'The capital of Mobile County is <mask>.',\n",
       " 'Newfoundland Act is a legal term in <mask>.',\n",
       " 'Brighton Centre is located in <mask>.',\n",
       " 'The official language of Northern Mariana Islands is <mask>.',\n",
       " 'Sandy Glacier is a <mask>.',\n",
       " 'LVMH was founded in <mask>.',\n",
       " 'Mera Joota Hai Japani was written in <mask>.',\n",
       " 'natural resource management is a subclass of <mask>.',\n",
       " 'Hilir Perak is located in <mask>.',\n",
       " 'Lilongwe is the capital of <mask>.',\n",
       " 'Brad Delson plays <mask>.',\n",
       " 'Freeman Dyson used to communicate in <mask>.',\n",
       " 'The official language of Tonga is <mask>.',\n",
       " 'folk art is a subclass of <mask>.',\n",
       " 'The original language of The Shield is <mask>.',\n",
       " 'The native language of Jean-Christophe Bouvet is <mask>.',\n",
       " 'Kurt Lewin works in the field of <mask>.',\n",
       " 'Geaca is located in <mask>.',\n",
       " 'West Sussex shares border with <mask>.',\n",
       " 'Veronika Voss was created in <mask>.',\n",
       " 'Peruvian Football Federation is a member of <mask>.',\n",
       " 'Damascus International Film Festival is located in <mask>.',\n",
       " 'Power, Corruption & Lies was written in <mask>.',\n",
       " 'tree stump is part of <mask>.',\n",
       " 'The capital of Canton of Geneva is <mask>.',\n",
       " 'The original language of Ram Rajya is <mask>.',\n",
       " 'Andreapolsky District is located in <mask>.',\n",
       " 'focaccia was created in <mask>.',\n",
       " 'Toyota Toyoace is produced by <mask>.',\n",
       " 'The native language of Fernand Gravey is <mask>.',\n",
       " 'IBM Rational Rose XDE is developed by <mask>.',\n",
       " 'Chrysler 200 is produced by <mask>.',\n",
       " 'The headquarter of Sydney Theatre Company is in <mask>.',\n",
       " 'The native language of Louis Friant is <mask>.',\n",
       " 'The original language of The Naked Sun is <mask>.',\n",
       " 'The original language of Komban is <mask>.',\n",
       " 'Delta Goodrem was created in <mask>.',\n",
       " 'Robocopy is developed by <mask>.',\n",
       " 'The official language of manually coded English is <mask>.',\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_neg_consistency[\"masked_non_negated\"][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Library of Alexandria',\n",
       " 'Bruno Racine',\n",
       " 'Salimuzzaman Siddiqui',\n",
       " \"Kay O'Brien\",\n",
       " 'Antim Peak',\n",
       " 'Adobe Media Player',\n",
       " 'Cauchy stress tensor',\n",
       " 'Raymond Queneau',\n",
       " 'Sweden',\n",
       " 'Chrysler K platform',\n",
       " 'El Siglo Futuro',\n",
       " 'Johan Gustaf Sandberg',\n",
       " 'Fiat Uno',\n",
       " 'Louisiana Voodoo',\n",
       " 'Jean-Baptiste Henri Lacordaire',\n",
       " 'aerial photography',\n",
       " 'Chrysalis',\n",
       " 'Charles Augustin Sainte-Beuve',\n",
       " 'Super Monaco GP',\n",
       " 'Abraham Blooteling',\n",
       " 'Edward Bulwer-Lytton',\n",
       " \"Pourvu qu'elles soient douces\",\n",
       " 'The Bill Cosby Show',\n",
       " 'Pauline Mills McGibbon',\n",
       " 'Maryland General Assembly',\n",
       " 'Average Joe',\n",
       " 'Singapore International Film Festival',\n",
       " 'Munich Re',\n",
       " 'Delhi Technological University',\n",
       " 'Ukrainian Railways',\n",
       " 'President of Vietnam',\n",
       " 'BBC Persian Television',\n",
       " 'Mishor Rohoshyo',\n",
       " 'Super Monaco GP',\n",
       " 'Western Canada Concept',\n",
       " 'Sardinian Judicati',\n",
       " 'Sony Crackle',\n",
       " 'Christian Wilhelm Blomstrand',\n",
       " 'Florence Regional Airport',\n",
       " 'Little Child',\n",
       " 'nickel silver',\n",
       " 'Jan van Krimpen',\n",
       " 'Uppslagsverket Finland',\n",
       " 'Norman Lamont',\n",
       " 'rpd Sos',\n",
       " 'Albert Camus',\n",
       " 'Journaled File System',\n",
       " 'Albert Camus',\n",
       " 'Saravannan Meenatchi',\n",
       " 'Volvo Amazon',\n",
       " 'Lilongwe',\n",
       " 'Vicente Espinel',\n",
       " 'Suzuki Hayabusa',\n",
       " 'Google Guice',\n",
       " 'Nevada Attorney General',\n",
       " 'Odessa',\n",
       " 'Dumoulin Islands',\n",
       " 'David Beckham',\n",
       " 'Witold Nazarewicz',\n",
       " 'Fifty Shades of Grey',\n",
       " 'Nikolay Alexandrovich Milyutin',\n",
       " 'Montmartre',\n",
       " \"2013 Internazionali BNL d'Italia\",\n",
       " 'Sexual Sterilization Act of Alberta',\n",
       " 'Albert Sarraut',\n",
       " 'cocaine',\n",
       " '2009 Lahore police academy attacks',\n",
       " 'Press TV',\n",
       " 'Final Fantasy III',\n",
       " 'Kuwait',\n",
       " 'mer Kaner',\n",
       " 'natural resource management',\n",
       " 'Jessy Kramer',\n",
       " 'Autonomous Republic of Crimea',\n",
       " 'heavy bomber',\n",
       " 'Fresno County',\n",
       " 'Centre Pierre Charbonneau',\n",
       " 'Caroline Beil',\n",
       " 'life insurance',\n",
       " 'Guyana',\n",
       " 'Mermaid Melody Pichi Pichi Pitch',\n",
       " 'Office Online',\n",
       " 'The Washington Post',\n",
       " 'Google Map Maker',\n",
       " 'Soviet Union',\n",
       " 'Airbus A330',\n",
       " 'Boeing 367-80',\n",
       " 'Giuseppe Bossi',\n",
       " 'Abbasid Caliphate',\n",
       " 'Macau International Movie Festival',\n",
       " 'Agoston Haraszthy',\n",
       " 'Hendrikus Colijn',\n",
       " 'Taiwan',\n",
       " 'Uutisvuoto',\n",
       " 'Sonic Drift 2',\n",
       " 'Suzuki Cultus Crescent',\n",
       " 'Ramil Guliyev',\n",
       " 'IBM Workplace OS',\n",
       " 'Samhain',\n",
       " 'Sporting Clube de Goa',\n",
       " 'iPhone 3GS',\n",
       " 'Boston Underground Film Festival',\n",
       " 'Jesse Saunders',\n",
       " 'Jean Claudio',\n",
       " 'Pierre Boucher',\n",
       " 'Canadian Multiculturalism Act',\n",
       " 'Yerevan',\n",
       " 'Julie Myerson',\n",
       " 'Georges Courteline',\n",
       " 'Loukas Sideras',\n",
       " 'Nazi Germany',\n",
       " '36 Vayadhinile',\n",
       " 'Germany in the Eurovision Song Contest 2010',\n",
       " 'Anti-Oedipus',\n",
       " 'Swing Girls',\n",
       " 'Prime Minister of Denmark',\n",
       " 'fossil fuel',\n",
       " 'Bolton',\n",
       " 'Mercury Villager',\n",
       " 'Tinker Glacier',\n",
       " 'Sergei Nilus',\n",
       " 'Rififi',\n",
       " 'SA Tennis Open',\n",
       " 'Ferrari 365',\n",
       " 'BT Tower',\n",
       " 'Bzura',\n",
       " 'Jeep Commander',\n",
       " 'Ross Island',\n",
       " 'Lawrence Townsend',\n",
       " 'Ouchy',\n",
       " 'Paul Morand',\n",
       " 'Intervi',\n",
       " 'Amar en tiempos revueltos',\n",
       " 'Taito Corporation',\n",
       " 'Highland',\n",
       " 'Cathaoirleach',\n",
       " 'Daniele Franceschini',\n",
       " 'Rome Film Festival',\n",
       " 'Shape Magazine',\n",
       " 'Jakub Kubicki',\n",
       " 'bridges',\n",
       " 'Pascal Gentil',\n",
       " 'Cairo',\n",
       " 'El Salvador',\n",
       " 'Royal Borough of Kensington and Chelsea',\n",
       " 'Abhiyum Naanum',\n",
       " 'Stieg Larsson',\n",
       " 'Dutch Language Union',\n",
       " 'Amiga 500',\n",
       " 'Paul Ramadier',\n",
       " 'Pierre Pelot',\n",
       " 'Petula Clark',\n",
       " 'Arasangam',\n",
       " 'Governor of Oregon',\n",
       " 'Chevrolet Montana',\n",
       " 'Alberto Mazzucato',\n",
       " 'sword',\n",
       " 'Len Tuckey',\n",
       " 'Jacques Toubon',\n",
       " 'Saratov Oblast',\n",
       " 'Intel iAPX 432',\n",
       " 'Massey University',\n",
       " 'Sundsvall Municipality',\n",
       " 'Kylie Minogue',\n",
       " 'Brave New World',\n",
       " 'Somalia',\n",
       " 'Paul Epworth',\n",
       " 'oral and maxillofacial surgery',\n",
       " 'George Walden',\n",
       " 'deep sea fish',\n",
       " 'Hiroshima Prefecture',\n",
       " 'Ion Cristoiu',\n",
       " 'Renault Fuego',\n",
       " 'Burlington International Airport',\n",
       " 'Jacques Ignatius de Roore',\n",
       " 'Il Postino: The Postman',\n",
       " 'Adobe After Effects',\n",
       " 'Adobe AIR',\n",
       " 'Internet Explorer 5',\n",
       " 'Franz-Olivier Giesbert',\n",
       " 'BMW E24',\n",
       " 'Pierre Beauchamp',\n",
       " 'Porsche 993',\n",
       " 'Marcel Oopa',\n",
       " 'organizational studies',\n",
       " 'Qatar Airways',\n",
       " 'Nenets Autonomous Okrug',\n",
       " 'Alberto Mazzucato',\n",
       " \"Michel d'Ornano\",\n",
       " 'Antonio Marcello Barberini',\n",
       " 'Carl Linnaeus',\n",
       " 'Pink Flag',\n",
       " 'Doha',\n",
       " 'Vladimir Vinogradov',\n",
       " 'Shinnan Glacier',\n",
       " 'Robert McCrindle',\n",
       " 'Yoav Gelber',\n",
       " 'Korea Football Association',\n",
       " 'Vogue Paris',\n",
       " 'Don Francisco Presenta',\n",
       " 'Lyubov Orlova',\n",
       " 'Edward J. King',\n",
       " 'Pennsylvania Station',\n",
       " 'drums',\n",
       " 'political philosophy',\n",
       " 'Google Marketing Platform',\n",
       " 'Jules Massenet',\n",
       " 'Indre-et-Loire',\n",
       " 'Organisation for Economic Co-operation and Development',\n",
       " 'United States Marine Corps Scout Sniper',\n",
       " '68th Venice International Film Festival',\n",
       " 'New Literary History',\n",
       " 'Weird Tales',\n",
       " 'Judah Loew ben Bezalel',\n",
       " 'Emilio Pujol',\n",
       " 'Minneapolis City Hall',\n",
       " 'The English Illustrated Magazine',\n",
       " 'NOS Journaal',\n",
       " 'Volvo 300 Series',\n",
       " 'Apple Pay',\n",
       " 'Landtag of Bavaria',\n",
       " 'Vincent Strambi',\n",
       " 'James Purdey & Sons',\n",
       " 'Ajanta Caves',\n",
       " 'cyberterrorism',\n",
       " 'Gustav Eduard von Hindersin',\n",
       " 'B-type asteroid',\n",
       " 'A Ver-o-Mar',\n",
       " 'Sardinian Judicati',\n",
       " 'NOW Lebanon',\n",
       " 'Sonata Software',\n",
       " 'Rutgers Glacier',\n",
       " 'Constitution of Alaska',\n",
       " 'Internet Explorer',\n",
       " 'Nissan Murano',\n",
       " 'Grupo Globo',\n",
       " 'Shawbury',\n",
       " 'Wiener AC',\n",
       " 'Maurice Joly',\n",
       " 'Jean Yanne',\n",
       " 'Collective Security Treaty Organisation',\n",
       " 'Felix von Kraus',\n",
       " 'Georges Chamarat',\n",
       " 'Royal National Theatre',\n",
       " 'Anastasia Myskina',\n",
       " 'James A. MacAlister',\n",
       " 'Yves Roucaute',\n",
       " 'Typhoon Pamela',\n",
       " 'John Coltrane',\n",
       " 'Latin poetry',\n",
       " 'International Istanbul Film Festival',\n",
       " 'Brussels-Capital Region',\n",
       " 'laminated bow',\n",
       " 'love',\n",
       " 'Doa Bekleriz',\n",
       " 'Ed Rossbach',\n",
       " 'tickling',\n",
       " 'congress of Berlin',\n",
       " 'CGTN Spanish',\n",
       " 'Ellen Wilkinson',\n",
       " 'Mas Canciones',\n",
       " 'Samsung NX-mount',\n",
       " 'Figure It Out',\n",
       " 'Toyota Platz',\n",
       " 'Honda CBR600F3',\n",
       " 'optics',\n",
       " 'Auguste Chevalier',\n",
       " 'Suzuki Cultus Crescent',\n",
       " 'Bye Bye Brasil',\n",
       " 'Sjogren Glacier',\n",
       " 'Copenhagen',\n",
       " 'Latvia',\n",
       " 'Kevin Higgins',\n",
       " 'Estonian Wikipedia',\n",
       " 'Lancia Flavia',\n",
       " 'Schwules Museum',\n",
       " 'Dimitrie Paciurea',\n",
       " 'Franz Rosei',\n",
       " 'Final Fantasy Legend III',\n",
       " 'Indonesia',\n",
       " 'Sebastian Coe',\n",
       " 'flat bone',\n",
       " 'Pride Toronto',\n",
       " 'Chevrolet Citation',\n",
       " 'Khorasan Province',\n",
       " 'Liberia',\n",
       " 'Khone Phapheng Falls',\n",
       " 'Lazarev Ice Shelf',\n",
       " 'This Old House',\n",
       " 'United Kingdom',\n",
       " 'Armand de Kersaint',\n",
       " 'Tate Liverpool',\n",
       " 'New Statesman',\n",
       " 'Gaetano Moroni',\n",
       " 'Guadalajara Province',\n",
       " 'Mysore Airport',\n",
       " 'frame drum',\n",
       " 'RAN Remote Area Nurse',\n",
       " 'Dodge WC series',\n",
       " 'Alexios II Komnenos',\n",
       " 'Jan ulk',\n",
       " 'Nagarik',\n",
       " 'Google Translate',\n",
       " 'Andreapolsky District',\n",
       " 'Squad',\n",
       " 'Herpa Wings',\n",
       " 'Mount Holly Cemetery',\n",
       " 'Kazimierz Wyka',\n",
       " 'Walter Long, 1st Viscount Long',\n",
       " 'United Launch Alliance',\n",
       " 'Arthur Godfrey and His Friends',\n",
       " 'Lake Geneva',\n",
       " 'Martin Koolhoven',\n",
       " 'Stockholm Municipality',\n",
       " 'bottle',\n",
       " 'Samsung',\n",
       " 'Lodhran Tehsil',\n",
       " 'Oleta River',\n",
       " 'Brown, Shipley & Co.',\n",
       " 'John Robert Godley',\n",
       " 'Rundell and Bridge',\n",
       " 'Leicester City F.C.',\n",
       " 'Outer islands of Mauritius',\n",
       " 'Christine Angot',\n",
       " 'Pinoy Idol',\n",
       " 'Baal Shem of London',\n",
       " 'The Mountain of the Cannibal God',\n",
       " '2006 Mumbai train bombings',\n",
       " 'Austin',\n",
       " 'Umberto Guidoni',\n",
       " 'Lancaster railway station',\n",
       " \"Jacques Charles Dupont de l'Eure\",\n",
       " 'Edward Heyman',\n",
       " 'Yunjin Kim',\n",
       " 'Windows Aero',\n",
       " 'Unix shell',\n",
       " 'Adventure Time',\n",
       " 'Muscat',\n",
       " 'Congress of Guatemala',\n",
       " 'Turkish Wikipedia',\n",
       " 'childhood cancer',\n",
       " 'systems science',\n",
       " 'Abdel Sattar Sabry',\n",
       " 'Gerard Hemsworth',\n",
       " 'Malay Archipelago',\n",
       " 'Industrial Design Centre',\n",
       " 'Christophe Moreau',\n",
       " 'Piove di Sacco',\n",
       " 'Google Hummingbird',\n",
       " 'Laitila',\n",
       " 'Tusker F.C.',\n",
       " 'Xbox',\n",
       " 'Royal Numismatic Society',\n",
       " 'Sergei Diaghilev',\n",
       " 'Adelaide Airport',\n",
       " 'Section One of the Canadian Charter of Rights and Freedoms',\n",
       " 'McFarlane Strait',\n",
       " 'Claude Cerval',\n",
       " 'Fiat Siena',\n",
       " 'Windows RT',\n",
       " 'President of Egypt',\n",
       " 'Fiat Idea',\n",
       " 'Le chalet',\n",
       " 'NFL on NBC',\n",
       " 'John Webster',\n",
       " 'Francesco Satolli',\n",
       " 'Nissan Terrano II',\n",
       " 'Heilmann & Littmann',\n",
       " 'Knesset',\n",
       " 'Jerusalem of Gold',\n",
       " 'Anne Fontaine',\n",
       " 'Vasili III of Russia',\n",
       " 'Windows 8',\n",
       " 'Bach Ice Shelf',\n",
       " 'County Leitrim',\n",
       " 'President of Algeria',\n",
       " 'Guangdong',\n",
       " 'Intel Quark',\n",
       " 'C-40 Clipper',\n",
       " 'Halifax City Hall',\n",
       " 'Dig Out Your Soul',\n",
       " 'Volvo Penta',\n",
       " 'The Hunchback of Notre Dame',\n",
       " 'Lemland',\n",
       " 'Nissan NX',\n",
       " 'C%C3%A9sar de Oliveira',\n",
       " 'Marcelin Pleynet',\n",
       " 'Dookudu',\n",
       " 'Glasgow City Council',\n",
       " 'Mauna Kea',\n",
       " 'Honda ST1300',\n",
       " 'Hirvensalmi',\n",
       " 'MSN Music',\n",
       " 'Gustavo Laureano',\n",
       " 'Giacomo Balla',\n",
       " 'Jim Wright',\n",
       " 'Wilfred Thesiger',\n",
       " 'Sommariva del Bosco',\n",
       " '2011 German Masters',\n",
       " 'Georges Ernest Boulanger',\n",
       " 'Robert Merle',\n",
       " 'Athens',\n",
       " 'Trampled Under Foot',\n",
       " 'Tusculanae Disputationes',\n",
       " 'submarine',\n",
       " 'Phobos Ridge',\n",
       " 'BMW M62',\n",
       " 'C. N. Annadurai',\n",
       " 'palatal consonant',\n",
       " 'Boyana Glacier',\n",
       " 'Irina Shayk',\n",
       " 'Hama Governorate',\n",
       " 'nutdriver',\n",
       " 'Uganda Anti-Homosexuality Act, 2014',\n",
       " 'Aleksandar Zograf',\n",
       " 'IBM Db2',\n",
       " 'Honda Wave series',\n",
       " 'Vlastimil Pt%C3%A1k',\n",
       " 'Beylerbeyi Palace',\n",
       " 'Ferrari 250 GTO',\n",
       " 'Hollywood Reel Independent Film Festival',\n",
       " 'Jean-Paul Sartre',\n",
       " 'The Dune Encyclopedia',\n",
       " 'Hawaii',\n",
       " 'IBM 729',\n",
       " 'Roadster (bicycle)',\n",
       " 'La Tropicale Amissa Bongo',\n",
       " 'Charles Cooper',\n",
       " 'Valencian Community',\n",
       " 'Marcel Marceau',\n",
       " 'Kuusamo',\n",
       " 'Chevrolet Tahoe',\n",
       " 'Alien Storm',\n",
       " 'Giunta Pisano',\n",
       " 'Oklahoma County',\n",
       " 'Gerard Mortier',\n",
       " 'Independent State of Croatia',\n",
       " 'Volvo S40',\n",
       " 'Gmina Stara Kornica',\n",
       " 'Aberdeen Airport',\n",
       " 'Eschborn-Frankfurt City Loop',\n",
       " 'Jupiter Glacier',\n",
       " 'Pentium III',\n",
       " 'Kyoto Protocol',\n",
       " 'Brazos Wind Farm',\n",
       " 'Xbox Game Studios',\n",
       " 'Adentro',\n",
       " 'Pierre Pelot',\n",
       " 'Antoine Bourseiller',\n",
       " 'Sughd Province',\n",
       " 'Sundbyberg Municipality',\n",
       " 'North East Lincolnshire',\n",
       " 'Holy See',\n",
       " 'Being Julia',\n",
       " 'Leser Landshuth',\n",
       " 'Panther Hollow Bridge',\n",
       " 'zoology',\n",
       " 'Sergo Kldiashvili',\n",
       " 'Shree Pundalik',\n",
       " 'Tasmanian House of Assembly',\n",
       " 'Uyu River',\n",
       " 'Austrian Football Association',\n",
       " 'iPad 3',\n",
       " 'Hermitage Water',\n",
       " 'Maurice Merleau-Ponty',\n",
       " 'Utah State Route 152',\n",
       " 'Spike Hughes',\n",
       " 'Valeriu Stoica',\n",
       " 'England',\n",
       " 'stochastic',\n",
       " 'Denmark',\n",
       " 'Panteion University',\n",
       " 'Piatt County',\n",
       " 'Hans Nielsen Hauge',\n",
       " 'Seville Fair',\n",
       " 'Udmurt Autonomous Oblast',\n",
       " 'Komi Republic',\n",
       " 'Jan ulk',\n",
       " 'Adam McLean',\n",
       " 'Bigbury-on-Sea',\n",
       " 'Jean Rostand',\n",
       " 'Pascale Ogier',\n",
       " 'Adobe Flash Builder',\n",
       " 'extreme poverty',\n",
       " 'Oklahoma',\n",
       " 'Meyrin',\n",
       " 'Henry Manners, 8th Duke of Rutland',\n",
       " 'Jean Yanne',\n",
       " 'odor',\n",
       " 'John V of Portugal',\n",
       " 'Pierre de Marca',\n",
       " 'Spanish Empire',\n",
       " 'Lyndon B. Johnson Space Center',\n",
       " 'Costa Rica',\n",
       " 'University of Paris',\n",
       " 'Bruno Racine',\n",
       " 'Amar Shonar Bangla',\n",
       " 'Matinee Theater',\n",
       " 'The Leys School',\n",
       " 'Barbados Football Association',\n",
       " 'Francis Robortello',\n",
       " 'Turkey',\n",
       " 'League of Communists of Yugoslavia',\n",
       " 'reggae',\n",
       " 'Matinee Theater',\n",
       " 'Dharmatma',\n",
       " 'Holland, Manitoba',\n",
       " 'Thomas Whitelegg',\n",
       " 'Snezhinsk',\n",
       " 'John Edwin',\n",
       " 'Macon Coliseum',\n",
       " 'Louis Godin',\n",
       " 'Unix shell',\n",
       " 'Beijing College Student Film Festival',\n",
       " 'Pennsylvania Station',\n",
       " 'Montreal World Film Festival',\n",
       " 'Pentium',\n",
       " 'Jacques Auguste de Thou',\n",
       " 'Norwegian News Agency',\n",
       " 'Ferrari 599 GTB Fiorano',\n",
       " 'North Holland',\n",
       " 'Shoranur',\n",
       " 'Dominic Guard',\n",
       " 'laser printer',\n",
       " 'Minsk National Airport',\n",
       " 'Great Tenasserim River',\n",
       " 'Dominique Besnehard',\n",
       " 'Nebraska',\n",
       " 'Henri Estienne',\n",
       " 'frame drum',\n",
       " 'Gunnar Staalesen',\n",
       " 'Honda CB750',\n",
       " 'Madhur Bhandarkar',\n",
       " 'Nokia Networks',\n",
       " 'dashboard',\n",
       " 'The Astronomical Journal',\n",
       " 'First Czechoslovak Republic',\n",
       " 'Porsche Panamera',\n",
       " 'Intel i860',\n",
       " 'matrix norm',\n",
       " 'Porfiry Ivanov',\n",
       " 'Seventh Star',\n",
       " 'Asikkala',\n",
       " 'Anna Kournikova',\n",
       " 'iTunes Radio',\n",
       " 'Chengdu',\n",
       " 'National Assembly of Quebec',\n",
       " 'Renault 25',\n",
       " 'Dwynwen',\n",
       " 'Hikaru Utada',\n",
       " 'Fiat 147',\n",
       " 'Hrtkovci',\n",
       " 'Petr Kroutil',\n",
       " 'Lahti',\n",
       " 'Auguste de Marmont',\n",
       " 'Siikajoki',\n",
       " 'Karl Michael Ziehrer',\n",
       " 'Belfast Film Festival',\n",
       " 'Jan Davidsz. de Heem',\n",
       " 'American English',\n",
       " 'parking orbit',\n",
       " 'Belgium',\n",
       " 'Damascus',\n",
       " 'Ukraine',\n",
       " 'Andrew Lang',\n",
       " 'Canadian Wildlife Service',\n",
       " 'Royal Montreal Golf Club',\n",
       " 'Virginia',\n",
       " 'Massachusetts Attorney General',\n",
       " 'Seattle Lesbian & Gay Film Festival',\n",
       " 'procreating',\n",
       " 'La Ribera Baixa',\n",
       " 'Rama Prabha',\n",
       " 'Alan Thomson',\n",
       " 'Patna',\n",
       " 'Yovkov Point',\n",
       " 'Nicholas Fairbairn',\n",
       " 'Love Deluxe',\n",
       " 'Stockholm County Council',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'The Barbara Stanwyck Show',\n",
       " \"Eugene O'Neill\",\n",
       " 'Michel Ordener',\n",
       " 'Kazan Governorate',\n",
       " 'Giuseppe Bossi',\n",
       " 'European Physical Society',\n",
       " 'Sam Benedict',\n",
       " 'psychopathology',\n",
       " 'Elvis Presley',\n",
       " 'Hamburg International Film Festival',\n",
       " 'Albania',\n",
       " 'Hugh Fraser, 1st Baron Fraser of Allander',\n",
       " 'Juan J. Campanella',\n",
       " 'County Armagh',\n",
       " 'Holstein Kiel',\n",
       " 'Dimitrios Gounaris',\n",
       " 'Casimir Delavigne',\n",
       " 'Angola',\n",
       " 'Jean Chiappe',\n",
       " 'One Day International',\n",
       " 'Osvaldo Genazzani',\n",
       " 'Sacramento',\n",
       " 'quick bread',\n",
       " 'Everard Home',\n",
       " 'hydrogen fluoride',\n",
       " 'Boston and Maine Corporation',\n",
       " 'Mimar Sinan Fine Arts University',\n",
       " 'Anne Curry',\n",
       " 'Beautiful Loser',\n",
       " 'Microsoft Schedule Plus',\n",
       " 'The Vicomte of Bragelonne: Ten Years Later',\n",
       " 'County Kilkenny',\n",
       " 'Socialist Federal Republic of Yugoslavia',\n",
       " 'Niger',\n",
       " 'success',\n",
       " 'ethnolinguistics',\n",
       " 'Bruno Tabacci',\n",
       " 'Strafgesetzbuch',\n",
       " 'Dumoulin Islands',\n",
       " 'geodetic astronomy',\n",
       " 'Colombian Football Federation',\n",
       " 'Oslo Airport, Fornebu',\n",
       " 'Estonia',\n",
       " 'LGM-30 Minuteman',\n",
       " 'La Revue du vin de France',\n",
       " 'Kungumam',\n",
       " 'Nicky Ryan',\n",
       " 'Nissan Terrano II',\n",
       " 'Thailand',\n",
       " 'Chuya River',\n",
       " 'Solicitor-General of Australia',\n",
       " 'Ismail Haniyeh',\n",
       " 'University of Paris',\n",
       " 'Edward Matthew Ward',\n",
       " 'Paolo Sammarco',\n",
       " 'Hadzhidimovo',\n",
       " 'Adliswil',\n",
       " 'Anand Math',\n",
       " 'Knut Reiersrud',\n",
       " 'Emergency!',\n",
       " 'Henry Doulton',\n",
       " 'Riiser-Larsen Ice Shelf',\n",
       " '30th Street Station',\n",
       " 'Gothenburg Film Festival',\n",
       " 'closets',\n",
       " 'GQ',\n",
       " 'Hugh Fraser, 1st Baron Fraser of Allander',\n",
       " 'CBS Evening News',\n",
       " 'Pierre Lecomte du Noy',\n",
       " 'Y Gododdin',\n",
       " 'La Chaux-de-Fonds',\n",
       " 'Cardiff Airport',\n",
       " 'Concise Oxford English Dictionary',\n",
       " 'President of Austria',\n",
       " 'Rachel Whiteread',\n",
       " 'Francisco Franco',\n",
       " 'Boeing Defense, Space & Security',\n",
       " 'Naryn River',\n",
       " 'Ellsworth Land',\n",
       " 'Nicolas Oudinot',\n",
       " 'Ecuador',\n",
       " 'Gyles Brandreth',\n",
       " 'Bernard Musson',\n",
       " 'Evert Willem Beth',\n",
       " 'Tetro',\n",
       " 'Carla Kihlstedt',\n",
       " 'Nagaland',\n",
       " 'Chinese characters',\n",
       " 'Boldklubben Frem',\n",
       " 'Can You Feel the Love Tonight',\n",
       " 'Ernest Picard',\n",
       " 'Max Velthuijs',\n",
       " 'Neighborhoods in Detroit',\n",
       " 'Staffordshire',\n",
       " 'Chicago Golden Gloves',\n",
       " 'Emmanuelle Bercot',\n",
       " 'Queens',\n",
       " 'monarch of Norway',\n",
       " 'Mukkam',\n",
       " 'Bethlehem Steel',\n",
       " 'Cornwall Civic Complex',\n",
       " 'Brown Peninsula',\n",
       " 'Nissan Cherry',\n",
       " 'olives',\n",
       " 'Honda Accord',\n",
       " 'Sophisticated Beggar',\n",
       " 'Thomas Gilovich',\n",
       " 'Everard Home',\n",
       " 'The Strand Magazine',\n",
       " 'Jessy Kramer',\n",
       " 'Stuttgart Airport',\n",
       " 'Andersson Island',\n",
       " 'Mukhed',\n",
       " 'hoatzins',\n",
       " 'Illertissen',\n",
       " 'Croom, County Limerick',\n",
       " 'kindness',\n",
       " 'Xbox',\n",
       " 'Nissan Laurel',\n",
       " 'Nissan Armada',\n",
       " 'Lutz Heck',\n",
       " 'Abraham Hume, 2nd Baronet',\n",
       " 'Prime Minister of Portugal',\n",
       " 'World Championship Wrestling',\n",
       " 'Garkan Rural District',\n",
       " 'Dodge Coronet',\n",
       " 'Cheers',\n",
       " 'Istanbul University',\n",
       " 'Panagarh',\n",
       " 'Georges Couthon',\n",
       " 'Ernani',\n",
       " 'Attingal',\n",
       " 'Honda NS500',\n",
       " 'Maryland House of Delegates',\n",
       " 'Luxembourg',\n",
       " 'Ludivine Sagnier',\n",
       " 'Myanmar Football Federation',\n",
       " 'zanzibar',\n",
       " 'Moritz von Schwind',\n",
       " 'Dino Dvornik',\n",
       " 'Microsoft InfoPath',\n",
       " 'The Invisible Man',\n",
       " 'Windows Genuine Advantage',\n",
       " 'Microsoft Office XP',\n",
       " 'Colombes',\n",
       " 'Jean De Briac',\n",
       " 'Honda CBR250R',\n",
       " 'Peabody Veterans Memorial High School',\n",
       " 'Switzerland',\n",
       " 'Paulus Manutius',\n",
       " 'Zagreb Pride',\n",
       " 'Rabinal',\n",
       " 'Socialist Republic of Serbia',\n",
       " 'Bahujan Vikas Aaghadi',\n",
       " 'Andrea Bianchi',\n",
       " 'Face Dances',\n",
       " 'County Carlow',\n",
       " 'Toyota Raum',\n",
       " 'Parliament of Navarre',\n",
       " 'The Tall Blond Man with One Black Shoe',\n",
       " 'Albany River',\n",
       " 'Governor of Tennessee',\n",
       " 'Marisa Masullo',\n",
       " 'Microsoft OneNote',\n",
       " 'Maharashtra',\n",
       " 'dreaming',\n",
       " 'Scotland',\n",
       " 'Margherita Buy',\n",
       " 'Kansas Attorney General',\n",
       " 'Jiangsu Broadcasting Corporation',\n",
       " 'Fiat Tipo',\n",
       " 'Wolfgang Sawallisch',\n",
       " 'isotope of nickel',\n",
       " 'Kazimierz Flatau',\n",
       " 'Viktor Chernomyrdin',\n",
       " 'Alphonse Pyramus de Candolle',\n",
       " 'Jean-Baptiste Robert Lindet',\n",
       " 'IBM Tivoli Storage Manager',\n",
       " 'Paul Klebnikov',\n",
       " 'Dudley railway station',\n",
       " 'Sughd Province',\n",
       " 'head teacher',\n",
       " 'limnology',\n",
       " 'Jules Favre',\n",
       " 'Zadar Airport',\n",
       " 'SportsCentury',\n",
       " 'Snezhinsk',\n",
       " 'John Dalton',\n",
       " 'Paul Crauchet',\n",
       " 'Moscow State Pedagogical University',\n",
       " 'Khartoum',\n",
       " 'AGM-86 ALCM',\n",
       " 'flag of Russia',\n",
       " 'flag of Russia',\n",
       " 'Taobao',\n",
       " 'Henri Perreyve',\n",
       " 'Mac OS 9',\n",
       " 'Russian Football Union',\n",
       " 'Microsoft Store',\n",
       " 'Philippines',\n",
       " 'Algeria',\n",
       " 'information theory',\n",
       " 'Boeing Helicopters',\n",
       " 'Julien Boisselier',\n",
       " 'Reinfeldt Cabinet',\n",
       " 'Canberra International Film Festival',\n",
       " 'Intel Core',\n",
       " 'Michigan Supreme Court',\n",
       " 'Mama Corsica',\n",
       " 'Karkkila',\n",
       " 'Manhattanhenge',\n",
       " 'Ginette Garcin',\n",
       " 'Monas, National Monument of Indonesia',\n",
       " 'Le Juste Prix',\n",
       " 'Grand Duchy of Lithuania',\n",
       " 'Murder Call',\n",
       " 'Torgeir Micaelsen',\n",
       " \"Howlin' Wolf\",\n",
       " \"First Barons' War\",\n",
       " 'Andrew Lloyd Webber',\n",
       " 'Nottingham Crown Court',\n",
       " 'Giuseppe Peano',\n",
       " 'Eyemouth disaster',\n",
       " 'Jean-Yves Daniel-Lesur',\n",
       " 'Minsk',\n",
       " 'Hello, Larry',\n",
       " 'FC Dynamo Kyiv',\n",
       " 'Richard Corbould',\n",
       " 'exome',\n",
       " 'Infiniti Q50',\n",
       " 'tofu',\n",
       " 'Parattai Engira Azhagu Sundaram',\n",
       " 'Nagaland',\n",
       " '16th London Turkish Film Festival',\n",
       " 'Bangarapet',\n",
       " \"Paddy O'Hanlon\",\n",
       " 'Pierre Beauchamp',\n",
       " 'disruptive coloration',\n",
       " 'Hammond Civic Center',\n",
       " 'Jonas Berggren',\n",
       " 'Leone Caetani',\n",
       " 'Mnir Gle',\n",
       " 'Dolwyddelan',\n",
       " 'Benjamin Paul Akers',\n",
       " 'Tarnobrzeg Voivodeship',\n",
       " 'John V. Evans',\n",
       " 'Port of Antwerp',\n",
       " 'thoracic surgery',\n",
       " 'telepathy',\n",
       " 'Brisbane Times',\n",
       " 'John Savage',\n",
       " '2015 Southeast Asian Games',\n",
       " 'Ottawa Citizen',\n",
       " 'Zaire',\n",
       " 'Jacques Mauclair',\n",
       " 'Kondotty',\n",
       " 'Google Reader',\n",
       " 'Croatian National Bank',\n",
       " 'Dodge Omni',\n",
       " 'Berlin International Film Festival',\n",
       " 'File Explorer',\n",
       " 'Commonwealth of Independent States',\n",
       " 'Mac OS X 10.1',\n",
       " 'Boston Massacre',\n",
       " 'Orpheus Island National Park',\n",
       " 'Canadian Human Rights Act',\n",
       " 'Chevrolet Delray',\n",
       " 'Latvia',\n",
       " 'kabsa',\n",
       " 'MS-DOS',\n",
       " 'Constitution of Sweden',\n",
       " 'Cine Blitz',\n",
       " 'Kemal Alispahi',\n",
       " 'ibm',\n",
       " 'Whitehall Glacier',\n",
       " 'Glen Ballard',\n",
       " 'The Literary Review',\n",
       " 'Beaglehole Glacier',\n",
       " 'Dengeki Daisy',\n",
       " 'Oregon House of Representatives',\n",
       " 'Harrison Allen',\n",
       " 'Aragon',\n",
       " 'Kotis Point',\n",
       " 'Dublin',\n",
       " 'Llanpumsaint',\n",
       " 'Las Vegas',\n",
       " 'Western Australia',\n",
       " 'Montevideo',\n",
       " 'Bernard Musson',\n",
       " 'Ernest Jones',\n",
       " 'Ferrari 312PB',\n",
       " 'Pandora Hearts',\n",
       " 'Australian Capital Territory',\n",
       " 'Brian May',\n",
       " 'Brisbane',\n",
       " 'Minister of International Trade',\n",
       " 'Honda Ascot',\n",
       " 'Outlook.com',\n",
       " 'Cardiff Airport',\n",
       " 'Toyota Aurion',\n",
       " 'Ireland',\n",
       " 'Thomas Palaiologos',\n",
       " 'City of Newcastle',\n",
       " 'Taking Chances',\n",
       " 'Giovanni Ricordi',\n",
       " 'Zomba, Malawi',\n",
       " 'Humphry Davy',\n",
       " 'Fernand David',\n",
       " 'decisions',\n",
       " 'The New School for Jazz and Contemporary Music',\n",
       " 'Ilmajoki',\n",
       " 'Algerian wine',\n",
       " 'Merikarvia',\n",
       " 'Nippon Cultural Broadcasting',\n",
       " 'SpongeBob SquarePants, season 2',\n",
       " 'Jean Crotti',\n",
       " 'bacterial pneumonia',\n",
       " 'Dmitry Puchkov',\n",
       " 'France Info',\n",
       " 'Open University of Catalonia',\n",
       " 'Washington, D.C.',\n",
       " 'Odessa',\n",
       " 'Christina Rossetti',\n",
       " 'hill',\n",
       " 'lipoid nephrosis',\n",
       " 'Don Giovanni',\n",
       " 'Windows Easy Transfer',\n",
       " 'Amsterdam',\n",
       " 'pound cake',\n",
       " 'trampolining',\n",
       " 'Suzuki Ertiga',\n",
       " 'Octave Uzanne',\n",
       " 'Ballydehob',\n",
       " 'Galina Starovoytova',\n",
       " 'Staffordshire',\n",
       " 'Microsoft Visual Studio',\n",
       " 'Josep Maria Jujol',\n",
       " 'Moudon',\n",
       " 'Jacobs University Bremen',\n",
       " 'Port of Antwerp',\n",
       " 'Nissan Pulsar',\n",
       " 'Northeastern University',\n",
       " 'Nando Parrado',\n",
       " 'Gauteng',\n",
       " 'Julius Goldzier',\n",
       " 'Nissan Teana',\n",
       " \"Massimo D'Alema\",\n",
       " 'Ukrainian Soviet Socialist Republic',\n",
       " 'Edward Heyman',\n",
       " 'Open University of Catalonia',\n",
       " 'Bill Barminski',\n",
       " 'Amazon Echo',\n",
       " 'Alfred Scott-Gatty',\n",
       " 'Jean Lefebvre',\n",
       " 'Legislative Yuan',\n",
       " 'Luftfahrt-Bundesamt',\n",
       " 'Strom Glacier',\n",
       " 'ISPF',\n",
       " 'Guatemala City',\n",
       " 'Queen Maud Land',\n",
       " 'Christian Bouchet',\n",
       " 'Two English Girls',\n",
       " 'Shelbourne F.C.',\n",
       " 'Johann Stamitz',\n",
       " 'Buenos Aires',\n",
       " 'Low-Life',\n",
       " 'Paolo Bertoli',\n",
       " 'Byrakur',\n",
       " 'Your Highness',\n",
       " 'Gerasimov Institute of Cinematography',\n",
       " 'Inuyasha',\n",
       " 'The Soul Cages',\n",
       " 'The American Historical Review',\n",
       " 'Wild & Crazy Kids',\n",
       " 'Pierre-Jules Cavelier',\n",
       " 'samurai cinema',\n",
       " 'Aspen Shortsfest',\n",
       " 'Attached Support Processor',\n",
       " 'Tourgasm',\n",
       " 'George Zarkadakis',\n",
       " 'contemplating',\n",
       " 'Ignaz von Born',\n",
       " 'Jean-Louis Barrault',\n",
       " 'Tuusula',\n",
       " 'Coluche',\n",
       " 'Matias Kupiainen',\n",
       " 'Sreekanth',\n",
       " 'Zhovti Vody',\n",
       " 'Preet Vihar',\n",
       " 'This Is War',\n",
       " 'BMW Motorrad',\n",
       " 'Western Tokyo',\n",
       " 'Thailand',\n",
       " 'Jacob Katz',\n",
       " 'Kurikka',\n",
       " 'Hey Dude',\n",
       " 'Byelorussian Soviet Socialist Republic',\n",
       " 'President of Argentina',\n",
       " 'Medaram',\n",
       " 'Javier de Burgos',\n",
       " 'Anatoly Sagalevich',\n",
       " 'Battle of Waterloo',\n",
       " 'referee',\n",
       " 'Berlin Codex',\n",
       " 'Jean-Yves Daniel-Lesur',\n",
       " 'Dodge 600',\n",
       " 'Budapest',\n",
       " 'Press TV',\n",
       " 'Kotis Point',\n",
       " 'Novo Sarajevo',\n",
       " 'BMW M70',\n",
       " 'TU Wien',\n",
       " 'Third Watch',\n",
       " 'logic',\n",
       " 'Montmartre',\n",
       " 'Panama',\n",
       " 'Bayerischer Rundfunk',\n",
       " 'Sahneh County',\n",
       " 'La dame blanche',\n",
       " 'Baron Cajetan von Felder',\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_neg_detect[\"content\"][:5000]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation consistency given entire sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detect_neg_consistency_ful, train_labels_det_neg_consistency = get_hidden_states_many_examples(model_wrapped, train_dataset_neg_consistency, n=23000, batch_size=100, query_column=\"masked_non_negated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detect_neg_consistency_ful, test_labels_det_neg_consistency = get_hidden_states_many_examples(model_wrapped, test_dataset_neg_consistency, n=5000, batch_size=100, query_column=\"masked_non_negated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56997413\n",
      "Iteration 2, loss = 0.50857677\n",
      "Iteration 3, loss = 0.47857090\n",
      "Iteration 4, loss = 0.46597085\n",
      "Iteration 5, loss = 0.45632371\n",
      "Iteration 6, loss = 0.44836501\n",
      "Iteration 7, loss = 0.44234312\n",
      "Iteration 8, loss = 0.44229012\n",
      "Iteration 9, loss = 0.43114708\n",
      "Iteration 10, loss = 0.42617749\n",
      "Iteration 11, loss = 0.42134867\n",
      "Iteration 12, loss = 0.43761238\n",
      "Iteration 13, loss = 0.41449206\n",
      "Iteration 14, loss = 0.40155792\n",
      "Iteration 15, loss = 0.39121109\n",
      "Iteration 16, loss = 0.39435257\n",
      "Iteration 17, loss = 0.37953237\n",
      "Iteration 18, loss = 0.37719408\n",
      "Iteration 19, loss = 0.36938035\n",
      "Iteration 20, loss = 0.36610236\n",
      "Iteration 21, loss = 0.35348845\n",
      "Iteration 22, loss = 0.34967081\n",
      "Iteration 23, loss = 0.35020981\n",
      "Iteration 24, loss = 0.33993237\n",
      "Iteration 25, loss = 0.32915414\n",
      "Iteration 26, loss = 0.31814559\n",
      "Iteration 27, loss = 0.31470023\n",
      "Iteration 28, loss = 0.30567636\n",
      "Iteration 29, loss = 0.29465151\n",
      "Iteration 30, loss = 0.29191080\n",
      "Iteration 31, loss = 0.28544499\n",
      "Iteration 32, loss = 0.27415920\n",
      "Iteration 33, loss = 0.26764915\n",
      "Iteration 34, loss = 0.25946420\n",
      "Iteration 35, loss = 0.25065006\n",
      "Iteration 36, loss = 0.26072738\n",
      "Iteration 37, loss = 0.24431565\n",
      "Iteration 38, loss = 0.22708086\n",
      "Iteration 39, loss = 0.23584196\n",
      "Iteration 40, loss = 0.21625923\n",
      "Iteration 41, loss = 0.21381372\n",
      "Iteration 42, loss = 0.21136686\n",
      "Iteration 43, loss = 0.19689490\n",
      "Iteration 44, loss = 0.19119048\n",
      "Iteration 45, loss = 0.19186053\n",
      "Iteration 46, loss = 0.17709307\n",
      "Iteration 47, loss = 0.16812542\n",
      "Iteration 48, loss = 0.16524910\n",
      "Iteration 49, loss = 0.15767199\n",
      "Iteration 50, loss = 0.15357032\n",
      "Iteration 51, loss = 0.14864145\n",
      "Iteration 52, loss = 0.14644393\n",
      "Iteration 53, loss = 0.13785153\n",
      "Iteration 54, loss = 0.13444721\n",
      "Iteration 55, loss = 0.12530942\n",
      "Iteration 56, loss = 0.11997381\n",
      "Iteration 57, loss = 0.12136606\n",
      "Iteration 58, loss = 0.11561579\n",
      "Iteration 59, loss = 0.10697492\n",
      "Iteration 60, loss = 0.10503499\n",
      "Iteration 61, loss = 0.10152288\n",
      "Iteration 62, loss = 0.09769839\n",
      "Iteration 63, loss = 0.09828212\n",
      "Iteration 64, loss = 0.09888661\n",
      "Iteration 65, loss = 0.09176076\n",
      "Iteration 66, loss = 0.08164824\n",
      "Iteration 67, loss = 0.07935672\n",
      "Iteration 68, loss = 0.07521696\n",
      "Iteration 69, loss = 0.07701886\n",
      "Iteration 70, loss = 0.07199388\n",
      "Iteration 71, loss = 0.06812946\n",
      "Iteration 72, loss = 0.06703527\n",
      "Iteration 73, loss = 0.06471146\n",
      "Iteration 74, loss = 0.06031759\n",
      "Iteration 75, loss = 0.05646507\n",
      "Iteration 76, loss = 0.05456096\n",
      "Iteration 77, loss = 0.05121551\n",
      "Iteration 78, loss = 0.05087975\n",
      "Iteration 79, loss = 0.04976382\n",
      "Iteration 80, loss = 0.04690075\n",
      "Iteration 81, loss = 0.04399429\n",
      "Iteration 82, loss = 0.04265961\n",
      "Iteration 83, loss = 0.04108610\n",
      "Iteration 84, loss = 0.04106895\n",
      "Iteration 85, loss = 0.04115684\n",
      "Iteration 86, loss = 0.05133659\n",
      "Iteration 87, loss = 0.03914296\n",
      "Iteration 88, loss = 0.03465624\n",
      "Iteration 89, loss = 0.03558442\n",
      "Iteration 90, loss = 0.03539690\n",
      "Iteration 91, loss = 0.03126374\n",
      "Iteration 92, loss = 0.02897681\n",
      "Iteration 93, loss = 0.03089852\n",
      "Iteration 94, loss = 0.02681341\n",
      "Iteration 95, loss = 0.02620438\n",
      "Iteration 96, loss = 0.02529482\n",
      "Iteration 97, loss = 0.02705940\n",
      "Iteration 98, loss = 0.02423485\n",
      "Iteration 99, loss = 0.02358587\n",
      "Iteration 100, loss = 0.02245042\n",
      "Iteration 101, loss = 0.02699276\n",
      "Iteration 102, loss = 0.02142251\n",
      "Iteration 103, loss = 0.02105588\n",
      "Iteration 104, loss = 0.01989192\n",
      "Iteration 105, loss = 0.02024869\n",
      "Iteration 106, loss = 0.01923796\n",
      "Iteration 107, loss = 0.01954854\n",
      "Iteration 108, loss = 0.01804802\n",
      "Iteration 109, loss = 0.02520664\n",
      "Iteration 110, loss = 0.01677628\n",
      "Iteration 111, loss = 0.01749326\n",
      "Iteration 112, loss = 0.01530018\n",
      "Iteration 113, loss = 0.01466481\n",
      "Iteration 114, loss = 0.01479847\n",
      "Iteration 115, loss = 0.01383428\n",
      "Iteration 116, loss = 0.01323743\n",
      "Iteration 117, loss = 0.01302343\n",
      "Iteration 118, loss = 0.01241588\n",
      "Iteration 119, loss = 0.01302883\n",
      "Iteration 120, loss = 0.01193251\n",
      "Iteration 121, loss = 0.01136536\n",
      "Iteration 122, loss = 0.01111269\n",
      "Iteration 123, loss = 0.01066045\n",
      "Iteration 124, loss = 0.01053891\n",
      "Iteration 125, loss = 0.01033866\n",
      "Iteration 126, loss = 0.01029753\n",
      "Iteration 127, loss = 0.01128306\n",
      "Iteration 128, loss = 0.00972826\n",
      "Iteration 129, loss = 0.00936813\n",
      "Iteration 130, loss = 0.00933613\n",
      "Iteration 131, loss = 0.00874667\n",
      "Iteration 132, loss = 0.00883238\n",
      "Iteration 133, loss = 0.01010878\n",
      "Iteration 134, loss = 0.00827455\n",
      "Iteration 135, loss = 0.00843469\n",
      "Iteration 136, loss = 0.00757622\n",
      "Iteration 137, loss = 0.00735562\n",
      "Iteration 138, loss = 0.00808998\n",
      "Iteration 139, loss = 0.00735731\n",
      "Iteration 140, loss = 0.00723019\n",
      "Iteration 141, loss = 0.00685159\n",
      "Iteration 142, loss = 0.00671221\n",
      "Iteration 143, loss = 0.00670923\n",
      "Iteration 144, loss = 0.00634785\n",
      "Iteration 145, loss = 0.00641759\n",
      "Iteration 146, loss = 0.00604210\n",
      "Iteration 147, loss = 0.00605173\n",
      "Iteration 148, loss = 0.00578548\n",
      "Iteration 149, loss = 0.00557244\n",
      "Iteration 150, loss = 0.00564171\n",
      "Iteration 151, loss = 0.00567803\n",
      "Iteration 152, loss = 0.00552740\n",
      "Iteration 153, loss = 0.00515914\n",
      "Iteration 154, loss = 0.00508678\n",
      "Iteration 155, loss = 0.00500747\n",
      "Iteration 156, loss = 0.00485782\n",
      "Iteration 157, loss = 0.00549778\n",
      "Iteration 158, loss = 0.00475474\n",
      "Iteration 159, loss = 0.00451335\n",
      "Iteration 160, loss = 0.00441907\n",
      "Iteration 161, loss = 0.00529615\n",
      "Iteration 162, loss = 0.00448649\n",
      "Iteration 163, loss = 0.00557051\n",
      "Iteration 164, loss = 0.00431709\n",
      "Iteration 165, loss = 0.00396672\n",
      "Iteration 166, loss = 0.00399644\n",
      "Iteration 167, loss = 0.00375408\n",
      "Iteration 168, loss = 0.00372319\n",
      "Iteration 169, loss = 0.00375968\n",
      "Iteration 170, loss = 0.00368992\n",
      "Iteration 171, loss = 0.00386455\n",
      "Iteration 172, loss = 0.00344052\n",
      "Iteration 173, loss = 0.00346141\n",
      "Iteration 174, loss = 0.00341530\n",
      "Iteration 175, loss = 0.00327473\n",
      "Iteration 176, loss = 0.00329496\n",
      "Iteration 177, loss = 0.00329913\n",
      "Iteration 178, loss = 0.00315709\n",
      "Iteration 179, loss = 0.00310886\n",
      "Iteration 180, loss = 0.00305121\n",
      "Iteration 181, loss = 0.00293220\n",
      "Iteration 182, loss = 0.00290407\n",
      "Iteration 183, loss = 0.00286720\n",
      "Iteration 184, loss = 0.00332199\n",
      "Iteration 185, loss = 0.00568449\n",
      "Iteration 186, loss = 0.00368306\n",
      "Iteration 187, loss = 0.00283020\n",
      "Iteration 188, loss = 0.00273883\n",
      "Iteration 189, loss = 0.00267456\n",
      "Iteration 190, loss = 0.00252544\n",
      "Iteration 191, loss = 0.00251224\n",
      "Iteration 192, loss = 0.00251515\n",
      "Iteration 193, loss = 0.00249238\n",
      "Iteration 194, loss = 0.00251805\n",
      "Iteration 195, loss = 0.00239886\n",
      "Iteration 196, loss = 0.00234531\n",
      "Iteration 197, loss = 0.00236153\n",
      "Iteration 198, loss = 0.00241640\n",
      "Iteration 199, loss = 0.00235697\n",
      "Iteration 200, loss = 0.00224530\n",
      "Iteration 201, loss = 0.00219901\n",
      "Iteration 202, loss = 0.00223169\n",
      "Iteration 203, loss = 0.00219338\n",
      "Iteration 204, loss = 0.00213977\n",
      "Iteration 205, loss = 0.00214235\n",
      "Iteration 206, loss = 0.00210522\n",
      "Iteration 207, loss = 0.00208216\n",
      "Iteration 208, loss = 0.00212306\n",
      "Iteration 209, loss = 0.00204957\n",
      "Iteration 210, loss = 0.00208039\n",
      "Iteration 211, loss = 0.00202813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55596588\n",
      "Iteration 2, loss = 0.50119539\n",
      "Iteration 3, loss = 0.46771600\n",
      "Iteration 4, loss = 0.45705333\n",
      "Iteration 5, loss = 0.44663050\n",
      "Iteration 6, loss = 0.43632468\n",
      "Iteration 7, loss = 0.42777683\n",
      "Iteration 8, loss = 0.42403551\n",
      "Iteration 9, loss = 0.41277768\n",
      "Iteration 10, loss = 0.40511429\n",
      "Iteration 11, loss = 0.39519815\n",
      "Iteration 12, loss = 0.40546397\n",
      "Iteration 13, loss = 0.38590677\n",
      "Iteration 14, loss = 0.36689569\n",
      "Iteration 15, loss = 0.35632920\n",
      "Iteration 16, loss = 0.36603392\n",
      "Iteration 17, loss = 0.34126072\n",
      "Iteration 18, loss = 0.33767789\n",
      "Iteration 19, loss = 0.33256258\n",
      "Iteration 20, loss = 0.32448062\n",
      "Iteration 21, loss = 0.30772814\n",
      "Iteration 22, loss = 0.30511654\n",
      "Iteration 23, loss = 0.30043383\n",
      "Iteration 24, loss = 0.28625395\n",
      "Iteration 25, loss = 0.27765101\n",
      "Iteration 26, loss = 0.26712972\n",
      "Iteration 27, loss = 0.25831338\n",
      "Iteration 28, loss = 0.25066056\n",
      "Iteration 29, loss = 0.24092200\n",
      "Iteration 30, loss = 0.23363043\n",
      "Iteration 31, loss = 0.22393209\n",
      "Iteration 32, loss = 0.21466676\n",
      "Iteration 33, loss = 0.20977943\n",
      "Iteration 34, loss = 0.20148690\n",
      "Iteration 35, loss = 0.19204892\n",
      "Iteration 36, loss = 0.19827262\n",
      "Iteration 37, loss = 0.18542573\n",
      "Iteration 38, loss = 0.17118661\n",
      "Iteration 39, loss = 0.18797157\n",
      "Iteration 40, loss = 0.16961534\n",
      "Iteration 41, loss = 0.15580021\n",
      "Iteration 42, loss = 0.15773949\n",
      "Iteration 43, loss = 0.13782637\n",
      "Iteration 44, loss = 0.13429635\n",
      "Iteration 45, loss = 0.14442457\n",
      "Iteration 46, loss = 0.12610146\n",
      "Iteration 47, loss = 0.11746139\n",
      "Iteration 48, loss = 0.12512303\n",
      "Iteration 49, loss = 0.10903022\n",
      "Iteration 50, loss = 0.10639828\n",
      "Iteration 51, loss = 0.10035161\n",
      "Iteration 52, loss = 0.10212539\n",
      "Iteration 53, loss = 0.09295297\n",
      "Iteration 54, loss = 0.08725801\n",
      "Iteration 55, loss = 0.08252992\n",
      "Iteration 56, loss = 0.07947753\n",
      "Iteration 57, loss = 0.08515099\n",
      "Iteration 58, loss = 0.08035254\n",
      "Iteration 59, loss = 0.07268022\n",
      "Iteration 60, loss = 0.06720932\n",
      "Iteration 61, loss = 0.06986966\n",
      "Iteration 62, loss = 0.06163849\n",
      "Iteration 63, loss = 0.06408394\n",
      "Iteration 64, loss = 0.06344431\n",
      "Iteration 65, loss = 0.06347420\n",
      "Iteration 66, loss = 0.05264657\n",
      "Iteration 67, loss = 0.05087973\n",
      "Iteration 68, loss = 0.04849217\n",
      "Iteration 69, loss = 0.05022134\n",
      "Iteration 70, loss = 0.04489325\n",
      "Iteration 71, loss = 0.04375199\n",
      "Iteration 72, loss = 0.04397758\n",
      "Iteration 73, loss = 0.04605797\n",
      "Iteration 74, loss = 0.03931843\n",
      "Iteration 75, loss = 0.03636566\n",
      "Iteration 76, loss = 0.03561579\n",
      "Iteration 77, loss = 0.03320271\n",
      "Iteration 78, loss = 0.03298180\n",
      "Iteration 79, loss = 0.03281500\n",
      "Iteration 80, loss = 0.03054210\n",
      "Iteration 81, loss = 0.02796879\n",
      "Iteration 82, loss = 0.02817832\n",
      "Iteration 83, loss = 0.02651616\n",
      "Iteration 84, loss = 0.02660320\n",
      "Iteration 85, loss = 0.02726288\n",
      "Iteration 86, loss = 0.03174653\n",
      "Iteration 87, loss = 0.02545262\n",
      "Iteration 88, loss = 0.02406632\n",
      "Iteration 89, loss = 0.02377993\n",
      "Iteration 90, loss = 0.02119730\n",
      "Iteration 91, loss = 0.01900759\n",
      "Iteration 92, loss = 0.01827516\n",
      "Iteration 93, loss = 0.02129650\n",
      "Iteration 94, loss = 0.01703401\n",
      "Iteration 95, loss = 0.01686032\n",
      "Iteration 96, loss = 0.01650609\n",
      "Iteration 97, loss = 0.01653023\n",
      "Iteration 98, loss = 0.01611421\n",
      "Iteration 99, loss = 0.01508073\n",
      "Iteration 100, loss = 0.01468976\n",
      "Iteration 101, loss = 0.01458011\n",
      "Iteration 102, loss = 0.01393931\n",
      "Iteration 103, loss = 0.01289772\n",
      "Iteration 104, loss = 0.01326622\n",
      "Iteration 105, loss = 0.01397856\n",
      "Iteration 106, loss = 0.01276422\n",
      "Iteration 107, loss = 0.01401759\n",
      "Iteration 108, loss = 0.01218194\n",
      "Iteration 109, loss = 0.01943116\n",
      "Iteration 110, loss = 0.01116174\n",
      "Iteration 111, loss = 0.01132405\n",
      "Iteration 112, loss = 0.01038816\n",
      "Iteration 113, loss = 0.00951287\n",
      "Iteration 114, loss = 0.00983978\n",
      "Iteration 115, loss = 0.00909989\n",
      "Iteration 116, loss = 0.00837774\n",
      "Iteration 117, loss = 0.00818200\n",
      "Iteration 118, loss = 0.00786634\n",
      "Iteration 119, loss = 0.00901698\n",
      "Iteration 120, loss = 0.00776433\n",
      "Iteration 121, loss = 0.00732293\n",
      "Iteration 122, loss = 0.00713675\n",
      "Iteration 123, loss = 0.00696134\n",
      "Iteration 124, loss = 0.00691827\n",
      "Iteration 125, loss = 0.00661526\n",
      "Iteration 126, loss = 0.00677074\n",
      "Iteration 127, loss = 0.00668794\n",
      "Iteration 128, loss = 0.00634740\n",
      "Iteration 129, loss = 0.00601337\n",
      "Iteration 130, loss = 0.00584829\n",
      "Iteration 131, loss = 0.00566131\n",
      "Iteration 132, loss = 0.00558401\n",
      "Iteration 133, loss = 0.00616364\n",
      "Iteration 134, loss = 0.00524983\n",
      "Iteration 135, loss = 0.00560208\n",
      "Iteration 136, loss = 0.00491564\n",
      "Iteration 137, loss = 0.00492325\n",
      "Iteration 138, loss = 0.00537655\n",
      "Iteration 139, loss = 0.00465743\n",
      "Iteration 140, loss = 0.00469229\n",
      "Iteration 141, loss = 0.00449236\n",
      "Iteration 142, loss = 0.00428741\n",
      "Iteration 143, loss = 0.00439799\n",
      "Iteration 144, loss = 0.00409723\n",
      "Iteration 145, loss = 0.00426862\n",
      "Iteration 146, loss = 0.00391823\n",
      "Iteration 147, loss = 0.00396560\n",
      "Iteration 148, loss = 0.00385258\n",
      "Iteration 149, loss = 0.00365021\n",
      "Iteration 150, loss = 0.00376070\n",
      "Iteration 151, loss = 0.00378143\n",
      "Iteration 152, loss = 0.00360787\n",
      "Iteration 153, loss = 0.00336027\n",
      "Iteration 154, loss = 0.00338085\n",
      "Iteration 155, loss = 0.00320908\n",
      "Iteration 156, loss = 0.00316347\n",
      "Iteration 157, loss = 0.00344279\n",
      "Iteration 158, loss = 0.00311368\n",
      "Iteration 159, loss = 0.00296066\n",
      "Iteration 160, loss = 0.00285988\n",
      "Iteration 161, loss = 0.00640769\n",
      "Iteration 162, loss = 0.00775033\n",
      "Iteration 163, loss = 0.00847779\n",
      "Iteration 164, loss = 0.00494274\n",
      "Iteration 165, loss = 0.00348014\n",
      "Iteration 166, loss = 0.00278601\n",
      "Iteration 167, loss = 0.00258861\n",
      "Iteration 168, loss = 0.00249421\n",
      "Iteration 169, loss = 0.00240400\n",
      "Iteration 170, loss = 0.00237178\n",
      "Iteration 171, loss = 0.00233637\n",
      "Iteration 172, loss = 0.00223178\n",
      "Iteration 173, loss = 0.00221666\n",
      "Iteration 174, loss = 0.00219949\n",
      "Iteration 175, loss = 0.00217027\n",
      "Iteration 176, loss = 0.00212110\n",
      "Iteration 177, loss = 0.00220104\n",
      "Iteration 178, loss = 0.00218348\n",
      "Iteration 179, loss = 0.00203298\n",
      "Iteration 180, loss = 0.00201553\n",
      "Iteration 181, loss = 0.00198925\n",
      "Iteration 182, loss = 0.00191561\n",
      "Iteration 183, loss = 0.00187754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54889651\n",
      "Iteration 2, loss = 0.49592962\n",
      "Iteration 3, loss = 0.46430811\n",
      "Iteration 4, loss = 0.45612074\n",
      "Iteration 5, loss = 0.44745027\n",
      "Iteration 6, loss = 0.44140146\n",
      "Iteration 7, loss = 0.43606779\n",
      "Iteration 8, loss = 0.43727121\n",
      "Iteration 9, loss = 0.42638640\n",
      "Iteration 10, loss = 0.41813743\n",
      "Iteration 11, loss = 0.41275960\n",
      "Iteration 12, loss = 0.42399008\n",
      "Iteration 13, loss = 0.40843715\n",
      "Iteration 14, loss = 0.39448606\n",
      "Iteration 15, loss = 0.38541211\n",
      "Iteration 16, loss = 0.39261268\n",
      "Iteration 17, loss = 0.37206496\n",
      "Iteration 18, loss = 0.37080559\n",
      "Iteration 19, loss = 0.36944026\n",
      "Iteration 20, loss = 0.36714564\n",
      "Iteration 21, loss = 0.35032382\n",
      "Iteration 22, loss = 0.34612738\n",
      "Iteration 23, loss = 0.34074898\n",
      "Iteration 24, loss = 0.32979803\n",
      "Iteration 25, loss = 0.32177153\n",
      "Iteration 26, loss = 0.31415686\n",
      "Iteration 27, loss = 0.31262414\n",
      "Iteration 28, loss = 0.30170886\n",
      "Iteration 29, loss = 0.29318817\n",
      "Iteration 30, loss = 0.28791546\n",
      "Iteration 31, loss = 0.27954747\n",
      "Iteration 32, loss = 0.27039333\n",
      "Iteration 33, loss = 0.26445189\n",
      "Iteration 34, loss = 0.25896342\n",
      "Iteration 35, loss = 0.25053567\n",
      "Iteration 36, loss = 0.25697780\n",
      "Iteration 37, loss = 0.25537192\n",
      "Iteration 38, loss = 0.23044543\n",
      "Iteration 39, loss = 0.24336453\n",
      "Iteration 40, loss = 0.22364522\n",
      "Iteration 41, loss = 0.21549104\n",
      "Iteration 42, loss = 0.21433600\n",
      "Iteration 43, loss = 0.19804676\n",
      "Iteration 44, loss = 0.19509788\n",
      "Iteration 45, loss = 0.19865291\n",
      "Iteration 46, loss = 0.18210048\n",
      "Iteration 47, loss = 0.17353200\n",
      "Iteration 48, loss = 0.18687411\n",
      "Iteration 49, loss = 0.16448732\n",
      "Iteration 50, loss = 0.16268248\n",
      "Iteration 51, loss = 0.15661648\n",
      "Iteration 52, loss = 0.15814046\n",
      "Iteration 53, loss = 0.14360059\n",
      "Iteration 54, loss = 0.13876919\n",
      "Iteration 55, loss = 0.13425812\n",
      "Iteration 56, loss = 0.13334015\n",
      "Iteration 57, loss = 0.13382096\n",
      "Iteration 58, loss = 0.12867638\n",
      "Iteration 59, loss = 0.11831655\n",
      "Iteration 60, loss = 0.11468088\n",
      "Iteration 61, loss = 0.11372337\n",
      "Iteration 62, loss = 0.10795945\n",
      "Iteration 63, loss = 0.10751505\n",
      "Iteration 64, loss = 0.10860786\n",
      "Iteration 65, loss = 0.10452857\n",
      "Iteration 66, loss = 0.09354813\n",
      "Iteration 67, loss = 0.08930574\n",
      "Iteration 68, loss = 0.08692849\n",
      "Iteration 69, loss = 0.08612369\n",
      "Iteration 70, loss = 0.08156447\n",
      "Iteration 71, loss = 0.07634400\n",
      "Iteration 72, loss = 0.08121926\n",
      "Iteration 73, loss = 0.07629186\n",
      "Iteration 74, loss = 0.07338519\n",
      "Iteration 75, loss = 0.06723177\n",
      "Iteration 76, loss = 0.06753813\n",
      "Iteration 77, loss = 0.06100016\n",
      "Iteration 78, loss = 0.06011976\n",
      "Iteration 79, loss = 0.06315285\n",
      "Iteration 80, loss = 0.06089176\n",
      "Iteration 81, loss = 0.05328476\n",
      "Iteration 82, loss = 0.05252387\n",
      "Iteration 83, loss = 0.05063736\n",
      "Iteration 84, loss = 0.05300073\n",
      "Iteration 85, loss = 0.05351008\n",
      "Iteration 86, loss = 0.05561349\n",
      "Iteration 87, loss = 0.05020432\n",
      "Iteration 88, loss = 0.04278860\n",
      "Iteration 89, loss = 0.04384630\n",
      "Iteration 90, loss = 0.04176759\n",
      "Iteration 91, loss = 0.03933808\n",
      "Iteration 92, loss = 0.03661084\n",
      "Iteration 93, loss = 0.03780168\n",
      "Iteration 94, loss = 0.03362359\n",
      "Iteration 95, loss = 0.03365751\n",
      "Iteration 96, loss = 0.03160854\n",
      "Iteration 97, loss = 0.03165986\n",
      "Iteration 98, loss = 0.03065584\n",
      "Iteration 99, loss = 0.03014037\n",
      "Iteration 100, loss = 0.02902201\n",
      "Iteration 101, loss = 0.03077025\n",
      "Iteration 102, loss = 0.02736087\n",
      "Iteration 103, loss = 0.02691416\n",
      "Iteration 104, loss = 0.02690444\n",
      "Iteration 105, loss = 0.02680650\n",
      "Iteration 106, loss = 0.02339770\n",
      "Iteration 107, loss = 0.02568688\n",
      "Iteration 108, loss = 0.02279052\n",
      "Iteration 109, loss = 0.03017097\n",
      "Iteration 110, loss = 0.02265223\n",
      "Iteration 111, loss = 0.02196583\n",
      "Iteration 112, loss = 0.01917960\n",
      "Iteration 113, loss = 0.01947745\n",
      "Iteration 114, loss = 0.02058898\n",
      "Iteration 115, loss = 0.01752264\n",
      "Iteration 116, loss = 0.01725431\n",
      "Iteration 117, loss = 0.01685092\n",
      "Iteration 118, loss = 0.01650661\n",
      "Iteration 119, loss = 0.01755146\n",
      "Iteration 120, loss = 0.01551484\n",
      "Iteration 121, loss = 0.01515473\n",
      "Iteration 122, loss = 0.01528268\n",
      "Iteration 123, loss = 0.01807152\n",
      "Iteration 124, loss = 0.01402776\n",
      "Iteration 125, loss = 0.01352478\n",
      "Iteration 126, loss = 0.01295566\n",
      "Iteration 127, loss = 0.01373742\n",
      "Iteration 128, loss = 0.01354837\n",
      "Iteration 129, loss = 0.01292797\n",
      "Iteration 130, loss = 0.01290715\n",
      "Iteration 131, loss = 0.01152959\n",
      "Iteration 132, loss = 0.01536082\n",
      "Iteration 133, loss = 0.01222320\n",
      "Iteration 134, loss = 0.01084303\n",
      "Iteration 135, loss = 0.01169163\n",
      "Iteration 136, loss = 0.01002256\n",
      "Iteration 137, loss = 0.00984385\n",
      "Iteration 138, loss = 0.01118954\n",
      "Iteration 139, loss = 0.00964522\n",
      "Iteration 140, loss = 0.01314127\n",
      "Iteration 141, loss = 0.00988442\n",
      "Iteration 142, loss = 0.00828751\n",
      "Iteration 143, loss = 0.00886224\n",
      "Iteration 144, loss = 0.00801417\n",
      "Iteration 145, loss = 0.00846182\n",
      "Iteration 146, loss = 0.00782475\n",
      "Iteration 147, loss = 0.00731282\n",
      "Iteration 148, loss = 0.00782227\n",
      "Iteration 149, loss = 0.00707107\n",
      "Iteration 150, loss = 0.00709910\n",
      "Iteration 151, loss = 0.00693245\n",
      "Iteration 152, loss = 0.00680533\n",
      "Iteration 153, loss = 0.00637781\n",
      "Iteration 154, loss = 0.00683896\n",
      "Iteration 155, loss = 0.00637286\n",
      "Iteration 156, loss = 0.00628522\n",
      "Iteration 157, loss = 0.00889177\n",
      "Iteration 158, loss = 0.00608457\n",
      "Iteration 159, loss = 0.00605617\n",
      "Iteration 160, loss = 0.00556971\n",
      "Iteration 161, loss = 0.00677761\n",
      "Iteration 162, loss = 0.00556690\n",
      "Iteration 163, loss = 0.00642344\n",
      "Iteration 164, loss = 0.00532336\n",
      "Iteration 165, loss = 0.00479043\n",
      "Iteration 166, loss = 0.00479347\n",
      "Iteration 167, loss = 0.00473698\n",
      "Iteration 168, loss = 0.00454294\n",
      "Iteration 169, loss = 0.00441181\n",
      "Iteration 170, loss = 0.00439377\n",
      "Iteration 171, loss = 0.00490808\n",
      "Iteration 172, loss = 0.00411737\n",
      "Iteration 173, loss = 0.00398427\n",
      "Iteration 174, loss = 0.00401938\n",
      "Iteration 175, loss = 0.00415233\n",
      "Iteration 176, loss = 0.00421129\n",
      "Iteration 177, loss = 0.00415022\n",
      "Iteration 178, loss = 0.00396484\n",
      "Iteration 179, loss = 0.00367472\n",
      "Iteration 180, loss = 0.00348635\n",
      "Iteration 181, loss = 0.00349160\n",
      "Iteration 182, loss = 0.00341067\n",
      "Iteration 183, loss = 0.00327764\n",
      "Iteration 184, loss = 0.00401956\n",
      "Iteration 185, loss = 0.01014223\n",
      "Iteration 186, loss = 0.00770297\n",
      "Iteration 187, loss = 0.00422258\n",
      "Iteration 188, loss = 0.00345367\n",
      "Iteration 189, loss = 0.00297325\n",
      "Iteration 190, loss = 0.00287514\n",
      "Iteration 191, loss = 0.00279656\n",
      "Iteration 192, loss = 0.00282007\n",
      "Iteration 193, loss = 0.00268980\n",
      "Iteration 194, loss = 0.00270991\n",
      "Iteration 195, loss = 0.00262941\n",
      "Iteration 196, loss = 0.00263000\n",
      "Iteration 197, loss = 0.00265394\n",
      "Iteration 198, loss = 0.00255190\n",
      "Iteration 199, loss = 0.00268850\n",
      "Iteration 200, loss = 0.00253751\n",
      "Iteration 201, loss = 0.00238929\n",
      "Iteration 202, loss = 0.00234282\n",
      "Iteration 203, loss = 0.00239615\n",
      "Iteration 204, loss = 0.00230237\n",
      "Iteration 205, loss = 0.00227286\n",
      "Iteration 206, loss = 0.00223427\n",
      "Iteration 207, loss = 0.00229998\n",
      "Iteration 208, loss = 0.00225556\n",
      "Iteration 209, loss = 0.00216431\n",
      "Iteration 210, loss = 0.00222315\n",
      "Iteration 211, loss = 0.00212134\n",
      "Iteration 212, loss = 0.00215099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53560859\n",
      "Iteration 2, loss = 0.48766831\n",
      "Iteration 3, loss = 0.45657294\n",
      "Iteration 4, loss = 0.44798038\n",
      "Iteration 5, loss = 0.43820189\n",
      "Iteration 6, loss = 0.43321181\n",
      "Iteration 7, loss = 0.42467559\n",
      "Iteration 8, loss = 0.42695734\n",
      "Iteration 9, loss = 0.41271757\n",
      "Iteration 10, loss = 0.40472781\n",
      "Iteration 11, loss = 0.40115800\n",
      "Iteration 12, loss = 0.41205783\n",
      "Iteration 13, loss = 0.39325911\n",
      "Iteration 14, loss = 0.38251081\n",
      "Iteration 15, loss = 0.37274211\n",
      "Iteration 16, loss = 0.38127586\n",
      "Iteration 17, loss = 0.36062458\n",
      "Iteration 18, loss = 0.35862992\n",
      "Iteration 19, loss = 0.35652138\n",
      "Iteration 20, loss = 0.35292951\n",
      "Iteration 21, loss = 0.34112271\n",
      "Iteration 22, loss = 0.33486432\n",
      "Iteration 23, loss = 0.33085605\n",
      "Iteration 24, loss = 0.32295464\n",
      "Iteration 25, loss = 0.31495604\n",
      "Iteration 26, loss = 0.30614037\n",
      "Iteration 27, loss = 0.30049600\n",
      "Iteration 28, loss = 0.29474301\n",
      "Iteration 29, loss = 0.28605557\n",
      "Iteration 30, loss = 0.28437854\n",
      "Iteration 31, loss = 0.27481523\n",
      "Iteration 32, loss = 0.26604017\n",
      "Iteration 33, loss = 0.26217607\n",
      "Iteration 34, loss = 0.25661767\n",
      "Iteration 35, loss = 0.24894843\n",
      "Iteration 36, loss = 0.24704761\n",
      "Iteration 37, loss = 0.24355381\n",
      "Iteration 38, loss = 0.22993250\n",
      "Iteration 39, loss = 0.24248519\n",
      "Iteration 40, loss = 0.22384269\n",
      "Iteration 41, loss = 0.22086353\n",
      "Iteration 42, loss = 0.21533490\n",
      "Iteration 43, loss = 0.20501035\n",
      "Iteration 44, loss = 0.19945990\n",
      "Iteration 45, loss = 0.20119898\n",
      "Iteration 46, loss = 0.18825270\n",
      "Iteration 47, loss = 0.17847234\n",
      "Iteration 48, loss = 0.18247591\n",
      "Iteration 49, loss = 0.17350114\n",
      "Iteration 50, loss = 0.16907845\n",
      "Iteration 51, loss = 0.16759457\n",
      "Iteration 52, loss = 0.16993499\n",
      "Iteration 53, loss = 0.15333558\n",
      "Iteration 54, loss = 0.14982896\n",
      "Iteration 55, loss = 0.14451419\n",
      "Iteration 56, loss = 0.14270939\n",
      "Iteration 57, loss = 0.14087431\n",
      "Iteration 58, loss = 0.14942271\n",
      "Iteration 59, loss = 0.13020305\n",
      "Iteration 60, loss = 0.12719922\n",
      "Iteration 61, loss = 0.12391679\n",
      "Iteration 62, loss = 0.11536162\n",
      "Iteration 63, loss = 0.11645581\n",
      "Iteration 64, loss = 0.12139446\n",
      "Iteration 65, loss = 0.11377224\n",
      "Iteration 66, loss = 0.10560257\n",
      "Iteration 67, loss = 0.10349235\n",
      "Iteration 68, loss = 0.09805103\n",
      "Iteration 69, loss = 0.09792864\n",
      "Iteration 70, loss = 0.09165578\n",
      "Iteration 71, loss = 0.08888423\n",
      "Iteration 72, loss = 0.09183918\n",
      "Iteration 73, loss = 0.08837007\n",
      "Iteration 74, loss = 0.08547006\n",
      "Iteration 75, loss = 0.08222726\n",
      "Iteration 76, loss = 0.07857271\n",
      "Iteration 77, loss = 0.07454371\n",
      "Iteration 78, loss = 0.07278191\n",
      "Iteration 79, loss = 0.07198711\n",
      "Iteration 80, loss = 0.07140571\n",
      "Iteration 81, loss = 0.06695281\n",
      "Iteration 82, loss = 0.06562147\n",
      "Iteration 83, loss = 0.06447174\n",
      "Iteration 84, loss = 0.06632492\n",
      "Iteration 85, loss = 0.06592279\n",
      "Iteration 86, loss = 0.06987101\n",
      "Iteration 87, loss = 0.06024152\n",
      "Iteration 88, loss = 0.05953686\n",
      "Iteration 89, loss = 0.05506837\n",
      "Iteration 90, loss = 0.05314188\n",
      "Iteration 91, loss = 0.04859175\n",
      "Iteration 92, loss = 0.04837975\n",
      "Iteration 93, loss = 0.04859012\n",
      "Iteration 94, loss = 0.04409200\n",
      "Iteration 95, loss = 0.04725646\n",
      "Iteration 96, loss = 0.04230932\n",
      "Iteration 97, loss = 0.04208413\n",
      "Iteration 98, loss = 0.04009607\n",
      "Iteration 99, loss = 0.04277153\n",
      "Iteration 100, loss = 0.03878996\n",
      "Iteration 101, loss = 0.04028459\n",
      "Iteration 102, loss = 0.03877756\n",
      "Iteration 103, loss = 0.03432523\n",
      "Iteration 104, loss = 0.03460769\n",
      "Iteration 105, loss = 0.03501548\n",
      "Iteration 106, loss = 0.03195919\n",
      "Iteration 107, loss = 0.03388641\n",
      "Iteration 108, loss = 0.03069114\n",
      "Iteration 109, loss = 0.03620337\n",
      "Iteration 110, loss = 0.02989720\n",
      "Iteration 111, loss = 0.02907539\n",
      "Iteration 112, loss = 0.02610378\n",
      "Iteration 113, loss = 0.02657489\n",
      "Iteration 114, loss = 0.02630383\n",
      "Iteration 115, loss = 0.02397345\n",
      "Iteration 116, loss = 0.02424798\n",
      "Iteration 117, loss = 0.02373247\n",
      "Iteration 118, loss = 0.02300303\n",
      "Iteration 119, loss = 0.02206089\n",
      "Iteration 120, loss = 0.02105086\n",
      "Iteration 121, loss = 0.02168049\n",
      "Iteration 122, loss = 0.02105019\n",
      "Iteration 123, loss = 0.02148122\n",
      "Iteration 124, loss = 0.01994841\n",
      "Iteration 125, loss = 0.01909477\n",
      "Iteration 126, loss = 0.01898951\n",
      "Iteration 127, loss = 0.02147193\n",
      "Iteration 128, loss = 0.01990969\n",
      "Iteration 129, loss = 0.01835996\n",
      "Iteration 130, loss = 0.01955275\n",
      "Iteration 131, loss = 0.01653330\n",
      "Iteration 132, loss = 0.02054461\n",
      "Iteration 133, loss = 0.02812187\n",
      "Iteration 134, loss = 0.01696010\n",
      "Iteration 135, loss = 0.01736128\n",
      "Iteration 136, loss = 0.01426034\n",
      "Iteration 137, loss = 0.01383775\n",
      "Iteration 138, loss = 0.01652905\n",
      "Iteration 139, loss = 0.01394838\n",
      "Iteration 140, loss = 0.01344256\n",
      "Iteration 141, loss = 0.01291032\n",
      "Iteration 142, loss = 0.01293547\n",
      "Iteration 143, loss = 0.01272292\n",
      "Iteration 144, loss = 0.01176695\n",
      "Iteration 145, loss = 0.01267990\n",
      "Iteration 146, loss = 0.01107501\n",
      "Iteration 147, loss = 0.01050369\n",
      "Iteration 148, loss = 0.01027280\n",
      "Iteration 149, loss = 0.01028605\n",
      "Iteration 150, loss = 0.01022145\n",
      "Iteration 151, loss = 0.00994559\n",
      "Iteration 152, loss = 0.00988905\n",
      "Iteration 153, loss = 0.00940030\n",
      "Iteration 154, loss = 0.01009774\n",
      "Iteration 155, loss = 0.00918954\n",
      "Iteration 156, loss = 0.00890031\n",
      "Iteration 157, loss = 0.01130768\n",
      "Iteration 158, loss = 0.00844405\n",
      "Iteration 159, loss = 0.00811671\n",
      "Iteration 160, loss = 0.00780778\n",
      "Iteration 161, loss = 0.01176359\n",
      "Iteration 162, loss = 0.00880939\n",
      "Iteration 163, loss = 0.00935889\n",
      "Iteration 164, loss = 0.00768861\n",
      "Iteration 165, loss = 0.00695236\n",
      "Iteration 166, loss = 0.00697703\n",
      "Iteration 167, loss = 0.00778211\n",
      "Iteration 168, loss = 0.00668291\n",
      "Iteration 169, loss = 0.00647187\n",
      "Iteration 170, loss = 0.00684402\n",
      "Iteration 171, loss = 0.00757394\n",
      "Iteration 172, loss = 0.00630169\n",
      "Iteration 173, loss = 0.00617714\n",
      "Iteration 174, loss = 0.00597115\n",
      "Iteration 175, loss = 0.00593668\n",
      "Iteration 176, loss = 0.00594873\n",
      "Iteration 177, loss = 0.00593533\n",
      "Iteration 178, loss = 0.00580253\n",
      "Iteration 179, loss = 0.00577629\n",
      "Iteration 180, loss = 0.00548668\n",
      "Iteration 181, loss = 0.00515448\n",
      "Iteration 182, loss = 0.00495058\n",
      "Iteration 183, loss = 0.00474536\n",
      "Iteration 184, loss = 0.00624998\n",
      "Iteration 185, loss = 0.03133096\n",
      "Iteration 186, loss = 0.00885928\n",
      "Iteration 187, loss = 0.00558585\n",
      "Iteration 188, loss = 0.00469951\n",
      "Iteration 189, loss = 0.00452874\n",
      "Iteration 190, loss = 0.00427200\n",
      "Iteration 191, loss = 0.00408761\n",
      "Iteration 192, loss = 0.00404688\n",
      "Iteration 193, loss = 0.00403445\n",
      "Iteration 194, loss = 0.00416311\n",
      "Iteration 195, loss = 0.00403596\n",
      "Iteration 196, loss = 0.00381124\n",
      "Iteration 197, loss = 0.00375225\n",
      "Iteration 198, loss = 0.00390402\n",
      "Iteration 199, loss = 0.00434328\n",
      "Iteration 200, loss = 0.00397836\n",
      "Iteration 201, loss = 0.00364326\n",
      "Iteration 202, loss = 0.00338127\n",
      "Iteration 203, loss = 0.00336303\n",
      "Iteration 204, loss = 0.00330357\n",
      "Iteration 205, loss = 0.00324154\n",
      "Iteration 206, loss = 0.00320975\n",
      "Iteration 207, loss = 0.00327283\n",
      "Iteration 208, loss = 0.00322277\n",
      "Iteration 209, loss = 0.00309884\n",
      "Iteration 210, loss = 0.00305029\n",
      "Iteration 211, loss = 0.00294053\n",
      "Iteration 212, loss = 0.00313320\n",
      "Iteration 213, loss = 0.00310383\n",
      "Iteration 214, loss = 0.00299905\n",
      "Iteration 215, loss = 0.00291256\n",
      "Iteration 216, loss = 0.00274616\n",
      "Iteration 217, loss = 0.00276676\n",
      "Iteration 218, loss = 0.00270121\n",
      "Iteration 219, loss = 0.00279623\n",
      "Iteration 220, loss = 0.00290984\n",
      "Iteration 221, loss = 0.00291382\n",
      "Iteration 222, loss = 0.00281062\n",
      "Iteration 223, loss = 0.00276007\n",
      "Iteration 224, loss = 0.00262741\n",
      "Iteration 225, loss = 0.00258216\n",
      "Iteration 226, loss = 0.00255468\n",
      "Iteration 227, loss = 0.00269764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53734068\n",
      "Iteration 2, loss = 0.48429221\n",
      "Iteration 3, loss = 0.45540213\n",
      "Iteration 4, loss = 0.44761407\n",
      "Iteration 5, loss = 0.43801388\n",
      "Iteration 6, loss = 0.43719675\n",
      "Iteration 7, loss = 0.42370582\n",
      "Iteration 8, loss = 0.42586511\n",
      "Iteration 9, loss = 0.41198218\n",
      "Iteration 10, loss = 0.40337799\n",
      "Iteration 11, loss = 0.39907298\n",
      "Iteration 12, loss = 0.40771304\n",
      "Iteration 13, loss = 0.39279559\n",
      "Iteration 14, loss = 0.38292114\n",
      "Iteration 15, loss = 0.37067699\n",
      "Iteration 16, loss = 0.37696933\n",
      "Iteration 17, loss = 0.35801696\n",
      "Iteration 18, loss = 0.35643189\n",
      "Iteration 19, loss = 0.35394511\n",
      "Iteration 20, loss = 0.34923215\n",
      "Iteration 21, loss = 0.33858650\n",
      "Iteration 22, loss = 0.33342324\n",
      "Iteration 23, loss = 0.33092254\n",
      "Iteration 24, loss = 0.32114720\n",
      "Iteration 25, loss = 0.31332191\n",
      "Iteration 26, loss = 0.30525131\n",
      "Iteration 27, loss = 0.29929766\n",
      "Iteration 28, loss = 0.29250212\n",
      "Iteration 29, loss = 0.28723124\n",
      "Iteration 30, loss = 0.28394007\n",
      "Iteration 31, loss = 0.27807120\n",
      "Iteration 32, loss = 0.26688822\n",
      "Iteration 33, loss = 0.26196783\n",
      "Iteration 34, loss = 0.25911435\n",
      "Iteration 35, loss = 0.25130401\n",
      "Iteration 36, loss = 0.25503999\n",
      "Iteration 37, loss = 0.24944796\n",
      "Iteration 38, loss = 0.23334276\n",
      "Iteration 39, loss = 0.24326030\n",
      "Iteration 40, loss = 0.23520945\n",
      "Iteration 41, loss = 0.22252757\n",
      "Iteration 42, loss = 0.21866434\n",
      "Iteration 43, loss = 0.20900961\n",
      "Iteration 44, loss = 0.20529335\n",
      "Iteration 45, loss = 0.20410074\n",
      "Iteration 46, loss = 0.19838456\n",
      "Iteration 47, loss = 0.18516668\n",
      "Iteration 48, loss = 0.19035026\n",
      "Iteration 49, loss = 0.18277766\n",
      "Iteration 50, loss = 0.18198570\n",
      "Iteration 51, loss = 0.17552723\n",
      "Iteration 52, loss = 0.17300183\n",
      "Iteration 53, loss = 0.16520320\n",
      "Iteration 54, loss = 0.16100452\n",
      "Iteration 55, loss = 0.15475421\n",
      "Iteration 56, loss = 0.15370124\n",
      "Iteration 57, loss = 0.15327568\n",
      "Iteration 58, loss = 0.15533364\n",
      "Iteration 59, loss = 0.14556486\n",
      "Iteration 60, loss = 0.13734698\n",
      "Iteration 61, loss = 0.13353210\n",
      "Iteration 62, loss = 0.12511966\n",
      "Iteration 63, loss = 0.13204543\n",
      "Iteration 64, loss = 0.13524309\n",
      "Iteration 65, loss = 0.12896364\n",
      "Iteration 66, loss = 0.11766052\n",
      "Iteration 67, loss = 0.11937398\n",
      "Iteration 68, loss = 0.11289491\n",
      "Iteration 69, loss = 0.10577205\n",
      "Iteration 70, loss = 0.10316910\n",
      "Iteration 71, loss = 0.09881884\n",
      "Iteration 72, loss = 0.10302730\n",
      "Iteration 73, loss = 0.10056703\n",
      "Iteration 74, loss = 0.09776605\n",
      "Iteration 75, loss = 0.09212378\n",
      "Iteration 76, loss = 0.09066448\n",
      "Iteration 77, loss = 0.08602143\n",
      "Iteration 78, loss = 0.08573678\n",
      "Iteration 79, loss = 0.08406893\n",
      "Iteration 80, loss = 0.08338843\n",
      "Iteration 81, loss = 0.07643741\n",
      "Iteration 82, loss = 0.07681468\n",
      "Iteration 83, loss = 0.07525249\n",
      "Iteration 84, loss = 0.07597752\n",
      "Iteration 85, loss = 0.08177911\n",
      "Iteration 86, loss = 0.08398523\n",
      "Iteration 87, loss = 0.07227730\n",
      "Iteration 88, loss = 0.06918532\n",
      "Iteration 89, loss = 0.06426238\n",
      "Iteration 90, loss = 0.06119284\n",
      "Iteration 91, loss = 0.06016615\n",
      "Iteration 92, loss = 0.05652855\n",
      "Iteration 93, loss = 0.06195075\n",
      "Iteration 94, loss = 0.05201383\n",
      "Iteration 95, loss = 0.05380083\n",
      "Iteration 96, loss = 0.05252859\n",
      "Iteration 97, loss = 0.05010473\n",
      "Iteration 98, loss = 0.04822850\n",
      "Iteration 99, loss = 0.04999874\n",
      "Iteration 100, loss = 0.04648881\n",
      "Iteration 101, loss = 0.04768126\n",
      "Iteration 102, loss = 0.04750422\n",
      "Iteration 103, loss = 0.04226407\n",
      "Iteration 104, loss = 0.04380262\n",
      "Iteration 105, loss = 0.04382099\n",
      "Iteration 106, loss = 0.04116776\n",
      "Iteration 107, loss = 0.05053293\n",
      "Iteration 108, loss = 0.04059135\n",
      "Iteration 109, loss = 0.04909909\n",
      "Iteration 110, loss = 0.03926094\n",
      "Iteration 111, loss = 0.03685419\n",
      "Iteration 112, loss = 0.03271906\n",
      "Iteration 113, loss = 0.03304292\n",
      "Iteration 114, loss = 0.03304082\n",
      "Iteration 115, loss = 0.03069190\n",
      "Iteration 116, loss = 0.03108158\n",
      "Iteration 117, loss = 0.03010228\n",
      "Iteration 118, loss = 0.02868556\n",
      "Iteration 119, loss = 0.02836764\n",
      "Iteration 120, loss = 0.02673253\n",
      "Iteration 121, loss = 0.02615536\n",
      "Iteration 122, loss = 0.02590472\n",
      "Iteration 123, loss = 0.03119684\n",
      "Iteration 124, loss = 0.02556370\n",
      "Iteration 125, loss = 0.02418210\n",
      "Iteration 126, loss = 0.02317617\n",
      "Iteration 127, loss = 0.02538288\n",
      "Iteration 128, loss = 0.02399542\n",
      "Iteration 129, loss = 0.02323443\n",
      "Iteration 130, loss = 0.02843964\n",
      "Iteration 131, loss = 0.02137012\n",
      "Iteration 132, loss = 0.02289870\n",
      "Iteration 133, loss = 0.04393661\n",
      "Iteration 134, loss = 0.02470345\n",
      "Iteration 135, loss = 0.02214828\n",
      "Iteration 136, loss = 0.01936076\n",
      "Iteration 137, loss = 0.01938795\n",
      "Iteration 138, loss = 0.01858076\n",
      "Iteration 139, loss = 0.01687503\n",
      "Iteration 140, loss = 0.01881942\n",
      "Iteration 141, loss = 0.01702889\n",
      "Iteration 142, loss = 0.01691594\n",
      "Iteration 143, loss = 0.01592292\n",
      "Iteration 144, loss = 0.01530481\n",
      "Iteration 145, loss = 0.01772985\n",
      "Iteration 146, loss = 0.01452280\n",
      "Iteration 147, loss = 0.01410796\n",
      "Iteration 148, loss = 0.01338159\n",
      "Iteration 149, loss = 0.01413263\n",
      "Iteration 150, loss = 0.01359898\n",
      "Iteration 151, loss = 0.01498355\n",
      "Iteration 152, loss = 0.01295237\n",
      "Iteration 153, loss = 0.01195593\n",
      "Iteration 154, loss = 0.01276495\n",
      "Iteration 155, loss = 0.01229498\n",
      "Iteration 156, loss = 0.01185802\n",
      "Iteration 157, loss = 0.01212883\n",
      "Iteration 158, loss = 0.01104465\n",
      "Iteration 159, loss = 0.01061770\n",
      "Iteration 160, loss = 0.01025800\n",
      "Iteration 161, loss = 0.01148407\n",
      "Iteration 162, loss = 0.01084374\n",
      "Iteration 163, loss = 0.01071983\n",
      "Iteration 164, loss = 0.01057165\n",
      "Iteration 165, loss = 0.00963209\n",
      "Iteration 166, loss = 0.00942603\n",
      "Iteration 167, loss = 0.01206830\n",
      "Iteration 168, loss = 0.00879323\n",
      "Iteration 169, loss = 0.00845561\n",
      "Iteration 170, loss = 0.00894480\n",
      "Iteration 171, loss = 0.01003029\n",
      "Iteration 172, loss = 0.00812005\n",
      "Iteration 173, loss = 0.00803469\n",
      "Iteration 174, loss = 0.00785045\n",
      "Iteration 175, loss = 0.00752843\n",
      "Iteration 176, loss = 0.00774243\n",
      "Iteration 177, loss = 0.00801344\n",
      "Iteration 178, loss = 0.00882211\n",
      "Iteration 179, loss = 0.00863352\n",
      "Iteration 180, loss = 0.00899873\n",
      "Iteration 181, loss = 0.00752789\n",
      "Iteration 182, loss = 0.00721505\n",
      "Iteration 183, loss = 0.00700924\n",
      "Iteration 184, loss = 0.00786187\n",
      "Iteration 185, loss = 0.03889303\n",
      "Iteration 186, loss = 0.01483434\n",
      "Iteration 187, loss = 0.00868186\n",
      "Iteration 188, loss = 0.00640814\n",
      "Iteration 189, loss = 0.00590976\n",
      "Iteration 190, loss = 0.00573984\n",
      "Iteration 191, loss = 0.00558449\n",
      "Iteration 192, loss = 0.00531154\n",
      "Iteration 193, loss = 0.00512876\n",
      "Iteration 194, loss = 0.00520323\n",
      "Iteration 195, loss = 0.00527255\n",
      "Iteration 196, loss = 0.00506034\n",
      "Iteration 197, loss = 0.00485000\n",
      "Iteration 198, loss = 0.00558460\n",
      "Iteration 199, loss = 0.00507316\n",
      "Iteration 200, loss = 0.00509036\n",
      "Iteration 201, loss = 0.00470892\n",
      "Iteration 202, loss = 0.00424224\n",
      "Iteration 203, loss = 0.00418288\n",
      "Iteration 204, loss = 0.00421453\n",
      "Iteration 205, loss = 0.00413297\n",
      "Iteration 206, loss = 0.00393477\n",
      "Iteration 207, loss = 0.00409116\n",
      "Iteration 208, loss = 0.00400879\n",
      "Iteration 209, loss = 0.00397110\n",
      "Iteration 210, loss = 0.00377170\n",
      "Iteration 211, loss = 0.00362017\n",
      "Iteration 212, loss = 0.00390487\n",
      "Iteration 213, loss = 0.00384827\n",
      "Iteration 214, loss = 0.00379055\n",
      "Iteration 215, loss = 0.00356072\n",
      "Iteration 216, loss = 0.00340325\n",
      "Iteration 217, loss = 0.00349361\n",
      "Iteration 218, loss = 0.00344146\n",
      "Iteration 219, loss = 0.00331797\n",
      "Iteration 220, loss = 0.00351966\n",
      "Iteration 221, loss = 0.00324742\n",
      "Iteration 222, loss = 0.00331686\n",
      "Iteration 223, loss = 0.00333069\n",
      "Iteration 224, loss = 0.00313035\n",
      "Iteration 225, loss = 0.00312391\n",
      "Iteration 226, loss = 0.00312163\n",
      "Iteration 227, loss = 0.00323199\n",
      "Iteration 228, loss = 0.00310415\n",
      "Iteration 229, loss = 0.02335310\n",
      "Iteration 230, loss = 0.07725103\n",
      "Iteration 231, loss = 0.01971376\n",
      "Iteration 232, loss = 0.00613556\n",
      "Iteration 233, loss = 0.00376517\n",
      "Iteration 234, loss = 0.00337537\n",
      "Iteration 235, loss = 0.00332786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54162330\n",
      "Iteration 2, loss = 0.47805794\n",
      "Iteration 3, loss = 0.45251375\n",
      "Iteration 4, loss = 0.44489622\n",
      "Iteration 5, loss = 0.43336546\n",
      "Iteration 6, loss = 0.43571350\n",
      "Iteration 7, loss = 0.42028885\n",
      "Iteration 8, loss = 0.42286980\n",
      "Iteration 9, loss = 0.41017219\n",
      "Iteration 10, loss = 0.39862534\n",
      "Iteration 11, loss = 0.39390577\n",
      "Iteration 12, loss = 0.40097071\n",
      "Iteration 13, loss = 0.38957995\n",
      "Iteration 14, loss = 0.37748043\n",
      "Iteration 15, loss = 0.36801312\n",
      "Iteration 16, loss = 0.37724427\n",
      "Iteration 17, loss = 0.35441485\n",
      "Iteration 18, loss = 0.35345632\n",
      "Iteration 19, loss = 0.35132269\n",
      "Iteration 20, loss = 0.34544901\n",
      "Iteration 21, loss = 0.33660311\n",
      "Iteration 22, loss = 0.33270460\n",
      "Iteration 23, loss = 0.33041096\n",
      "Iteration 24, loss = 0.31760913\n",
      "Iteration 25, loss = 0.31199408\n",
      "Iteration 26, loss = 0.30555192\n",
      "Iteration 27, loss = 0.29889970\n",
      "Iteration 28, loss = 0.29285740\n",
      "Iteration 29, loss = 0.28772705\n",
      "Iteration 30, loss = 0.28369253\n",
      "Iteration 31, loss = 0.27866974\n",
      "Iteration 32, loss = 0.26939173\n",
      "Iteration 33, loss = 0.26451356\n",
      "Iteration 34, loss = 0.26282301\n",
      "Iteration 35, loss = 0.25708119\n",
      "Iteration 36, loss = 0.25891230\n",
      "Iteration 37, loss = 0.25297939\n",
      "Iteration 38, loss = 0.23753907\n",
      "Iteration 39, loss = 0.24709886\n",
      "Iteration 40, loss = 0.23772179\n",
      "Iteration 41, loss = 0.23044696\n",
      "Iteration 42, loss = 0.22935511\n",
      "Iteration 43, loss = 0.21560055\n",
      "Iteration 44, loss = 0.21229748\n",
      "Iteration 45, loss = 0.21005882\n",
      "Iteration 46, loss = 0.20362851\n",
      "Iteration 47, loss = 0.19289883\n",
      "Iteration 48, loss = 0.19940952\n",
      "Iteration 49, loss = 0.19236027\n",
      "Iteration 50, loss = 0.18644006\n",
      "Iteration 51, loss = 0.18953050\n",
      "Iteration 52, loss = 0.18216675\n",
      "Iteration 53, loss = 0.17042507\n",
      "Iteration 54, loss = 0.16714553\n",
      "Iteration 55, loss = 0.15951808\n",
      "Iteration 56, loss = 0.15936672\n",
      "Iteration 57, loss = 0.15803381\n",
      "Iteration 58, loss = 0.15782253\n",
      "Iteration 59, loss = 0.15230618\n",
      "Iteration 60, loss = 0.14595596\n",
      "Iteration 61, loss = 0.13935087\n",
      "Iteration 62, loss = 0.13249231\n",
      "Iteration 63, loss = 0.14301973\n",
      "Iteration 64, loss = 0.13433490\n",
      "Iteration 65, loss = 0.13173739\n",
      "Iteration 66, loss = 0.12640473\n",
      "Iteration 67, loss = 0.12153046\n",
      "Iteration 68, loss = 0.11838140\n",
      "Iteration 69, loss = 0.11887553\n",
      "Iteration 70, loss = 0.11183611\n",
      "Iteration 71, loss = 0.10510255\n",
      "Iteration 72, loss = 0.11285036\n",
      "Iteration 73, loss = 0.10700441\n",
      "Iteration 74, loss = 0.10551774\n",
      "Iteration 75, loss = 0.10273854\n",
      "Iteration 76, loss = 0.09788973\n",
      "Iteration 77, loss = 0.09118549\n",
      "Iteration 78, loss = 0.09393688\n",
      "Iteration 79, loss = 0.09579889\n",
      "Iteration 80, loss = 0.09567926\n",
      "Iteration 81, loss = 0.08388711\n",
      "Iteration 82, loss = 0.08201026\n",
      "Iteration 83, loss = 0.07810237\n",
      "Iteration 84, loss = 0.08686627\n",
      "Iteration 85, loss = 0.08283323\n",
      "Iteration 86, loss = 0.08624448\n",
      "Iteration 87, loss = 0.07988091\n",
      "Iteration 88, loss = 0.07728326\n",
      "Iteration 89, loss = 0.07479975\n",
      "Iteration 90, loss = 0.06763984\n",
      "Iteration 91, loss = 0.06433605\n",
      "Iteration 92, loss = 0.06164604\n",
      "Iteration 93, loss = 0.08099640\n",
      "Iteration 94, loss = 0.05858473\n",
      "Iteration 95, loss = 0.05842843\n",
      "Iteration 96, loss = 0.05639979\n",
      "Iteration 97, loss = 0.05502524\n",
      "Iteration 98, loss = 0.05343853\n",
      "Iteration 99, loss = 0.05410823\n",
      "Iteration 100, loss = 0.05237096\n",
      "Iteration 101, loss = 0.05342897\n",
      "Iteration 102, loss = 0.05444510\n",
      "Iteration 103, loss = 0.04690195\n",
      "Iteration 104, loss = 0.04603235\n",
      "Iteration 105, loss = 0.04736457\n",
      "Iteration 106, loss = 0.04572447\n",
      "Iteration 107, loss = 0.05018660\n",
      "Iteration 108, loss = 0.04387044\n",
      "Iteration 109, loss = 0.05576938\n",
      "Iteration 110, loss = 0.04177670\n",
      "Iteration 111, loss = 0.04054948\n",
      "Iteration 112, loss = 0.03605938\n",
      "Iteration 113, loss = 0.03715454\n",
      "Iteration 114, loss = 0.03910840\n",
      "Iteration 115, loss = 0.03531093\n",
      "Iteration 116, loss = 0.03883948\n",
      "Iteration 117, loss = 0.03336768\n",
      "Iteration 118, loss = 0.03252499\n",
      "Iteration 119, loss = 0.03119690\n",
      "Iteration 120, loss = 0.03142782\n",
      "Iteration 121, loss = 0.02979537\n",
      "Iteration 122, loss = 0.02939923\n",
      "Iteration 123, loss = 0.03809394\n",
      "Iteration 124, loss = 0.02808347\n",
      "Iteration 125, loss = 0.02769730\n",
      "Iteration 126, loss = 0.02704226\n",
      "Iteration 127, loss = 0.02958320\n",
      "Iteration 128, loss = 0.02661553\n",
      "Iteration 129, loss = 0.02655030\n",
      "Iteration 130, loss = 0.02770747\n",
      "Iteration 131, loss = 0.02457554\n",
      "Iteration 132, loss = 0.02636014\n",
      "Iteration 133, loss = 0.05029698\n",
      "Iteration 134, loss = 0.02602199\n",
      "Iteration 135, loss = 0.02843450\n",
      "Iteration 136, loss = 0.02203828\n",
      "Iteration 137, loss = 0.02152082\n",
      "Iteration 138, loss = 0.02070797\n",
      "Iteration 139, loss = 0.01914384\n",
      "Iteration 140, loss = 0.02104232\n",
      "Iteration 141, loss = 0.01866553\n",
      "Iteration 142, loss = 0.01887652\n",
      "Iteration 143, loss = 0.01845065\n",
      "Iteration 144, loss = 0.01756738\n",
      "Iteration 145, loss = 0.01892255\n",
      "Iteration 146, loss = 0.01667175\n",
      "Iteration 147, loss = 0.01596066\n",
      "Iteration 148, loss = 0.01611376\n",
      "Iteration 149, loss = 0.01600457\n",
      "Iteration 150, loss = 0.01632640\n",
      "Iteration 151, loss = 0.01508791\n",
      "Iteration 152, loss = 0.01471850\n",
      "Iteration 153, loss = 0.01394160\n",
      "Iteration 154, loss = 0.01465774\n",
      "Iteration 155, loss = 0.01378614\n",
      "Iteration 156, loss = 0.01305256\n",
      "Iteration 157, loss = 0.01396491\n",
      "Iteration 158, loss = 0.01332489\n",
      "Iteration 159, loss = 0.01380943\n",
      "Iteration 160, loss = 0.01292817\n",
      "Iteration 161, loss = 0.01410984\n",
      "Iteration 162, loss = 0.01231618\n",
      "Iteration 163, loss = 0.01145254\n",
      "Iteration 164, loss = 0.01403055\n",
      "Iteration 165, loss = 0.01133857\n",
      "Iteration 166, loss = 0.01122200\n",
      "Iteration 167, loss = 0.01306212\n",
      "Iteration 168, loss = 0.01110178\n",
      "Iteration 169, loss = 0.01040204\n",
      "Iteration 170, loss = 0.01032066\n",
      "Iteration 171, loss = 0.01138208\n",
      "Iteration 172, loss = 0.00916279\n",
      "Iteration 173, loss = 0.00943531\n",
      "Iteration 174, loss = 0.00940448\n",
      "Iteration 175, loss = 0.00936480\n",
      "Iteration 176, loss = 0.00827763\n",
      "Iteration 177, loss = 0.00904385\n",
      "Iteration 178, loss = 0.00854431\n",
      "Iteration 179, loss = 0.01022887\n",
      "Iteration 180, loss = 0.00970994\n",
      "Iteration 181, loss = 0.00786876\n",
      "Iteration 182, loss = 0.00780108\n",
      "Iteration 183, loss = 0.00868396\n",
      "Iteration 184, loss = 0.00795340\n",
      "Iteration 185, loss = 0.03455591\n",
      "Iteration 186, loss = 0.01047066\n",
      "Iteration 187, loss = 0.01008247\n",
      "Iteration 188, loss = 0.00723220\n",
      "Iteration 189, loss = 0.00655507\n",
      "Iteration 190, loss = 0.00644088\n",
      "Iteration 191, loss = 0.00647415\n",
      "Iteration 192, loss = 0.00594030\n",
      "Iteration 193, loss = 0.00603545\n",
      "Iteration 194, loss = 0.00620896\n",
      "Iteration 195, loss = 0.00673600\n",
      "Iteration 196, loss = 0.00882567\n",
      "Iteration 197, loss = 0.00671187\n",
      "Iteration 198, loss = 0.00681625\n",
      "Iteration 199, loss = 0.00572349\n",
      "Iteration 200, loss = 0.00611404\n",
      "Iteration 201, loss = 0.00540174\n",
      "Iteration 202, loss = 0.00522263\n",
      "Iteration 203, loss = 0.00511157\n",
      "Iteration 204, loss = 0.00536899\n",
      "Iteration 205, loss = 0.00518471\n",
      "Iteration 206, loss = 0.00505408\n",
      "Iteration 207, loss = 0.00713601\n",
      "Iteration 208, loss = 0.00658420\n",
      "Iteration 209, loss = 0.00462302\n",
      "Iteration 210, loss = 0.00418036\n",
      "Iteration 211, loss = 0.00410384\n",
      "Iteration 212, loss = 0.00430937\n",
      "Iteration 213, loss = 0.00457719\n",
      "Iteration 214, loss = 0.00479542\n",
      "Iteration 215, loss = 0.00397156\n",
      "Iteration 216, loss = 0.00381550\n",
      "Iteration 217, loss = 0.00372124\n",
      "Iteration 218, loss = 0.00367276\n",
      "Iteration 219, loss = 0.00376739\n",
      "Iteration 220, loss = 0.00378760\n",
      "Iteration 221, loss = 0.00502929\n",
      "Iteration 222, loss = 0.00493857\n",
      "Iteration 223, loss = 0.01036969\n",
      "Iteration 224, loss = 0.01168912\n",
      "Iteration 225, loss = 0.00926959\n",
      "Iteration 226, loss = 0.01312849\n",
      "Iteration 227, loss = 0.02526509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53892623\n",
      "Iteration 2, loss = 0.47576193\n",
      "Iteration 3, loss = 0.45181872\n",
      "Iteration 4, loss = 0.44479786\n",
      "Iteration 5, loss = 0.43381861\n",
      "Iteration 6, loss = 0.43290119\n",
      "Iteration 7, loss = 0.42180195\n",
      "Iteration 8, loss = 0.42452998\n",
      "Iteration 9, loss = 0.41091084\n",
      "Iteration 10, loss = 0.40064270\n",
      "Iteration 11, loss = 0.39586060\n",
      "Iteration 12, loss = 0.40346771\n",
      "Iteration 13, loss = 0.39156033\n",
      "Iteration 14, loss = 0.37912347\n",
      "Iteration 15, loss = 0.37015483\n",
      "Iteration 16, loss = 0.37803406\n",
      "Iteration 17, loss = 0.35746226\n",
      "Iteration 18, loss = 0.35740549\n",
      "Iteration 19, loss = 0.35326988\n",
      "Iteration 20, loss = 0.34449227\n",
      "Iteration 21, loss = 0.33799291\n",
      "Iteration 22, loss = 0.33794265\n",
      "Iteration 23, loss = 0.33297213\n",
      "Iteration 24, loss = 0.32107007\n",
      "Iteration 25, loss = 0.31638359\n",
      "Iteration 26, loss = 0.30883828\n",
      "Iteration 27, loss = 0.30253938\n",
      "Iteration 28, loss = 0.29527792\n",
      "Iteration 29, loss = 0.29077600\n",
      "Iteration 30, loss = 0.28725136\n",
      "Iteration 31, loss = 0.28126888\n",
      "Iteration 32, loss = 0.27135662\n",
      "Iteration 33, loss = 0.26776482\n",
      "Iteration 34, loss = 0.26506874\n",
      "Iteration 35, loss = 0.25706827\n",
      "Iteration 36, loss = 0.25756815\n",
      "Iteration 37, loss = 0.25751247\n",
      "Iteration 38, loss = 0.24217518\n",
      "Iteration 39, loss = 0.25023379\n",
      "Iteration 40, loss = 0.23857903\n",
      "Iteration 41, loss = 0.23433858\n",
      "Iteration 42, loss = 0.23694939\n",
      "Iteration 43, loss = 0.22051837\n",
      "Iteration 44, loss = 0.21371255\n",
      "Iteration 45, loss = 0.21587116\n",
      "Iteration 46, loss = 0.20633773\n",
      "Iteration 47, loss = 0.19432309\n",
      "Iteration 48, loss = 0.19842711\n",
      "Iteration 49, loss = 0.19164074\n",
      "Iteration 50, loss = 0.18740898\n",
      "Iteration 51, loss = 0.18763065\n",
      "Iteration 52, loss = 0.18653051\n",
      "Iteration 53, loss = 0.17139746\n",
      "Iteration 54, loss = 0.16764349\n",
      "Iteration 55, loss = 0.16274849\n",
      "Iteration 56, loss = 0.16066254\n",
      "Iteration 57, loss = 0.15862065\n",
      "Iteration 58, loss = 0.15395270\n",
      "Iteration 59, loss = 0.14936588\n",
      "Iteration 60, loss = 0.14557807\n",
      "Iteration 61, loss = 0.14150628\n",
      "Iteration 62, loss = 0.13493617\n",
      "Iteration 63, loss = 0.13808757\n",
      "Iteration 64, loss = 0.14054031\n",
      "Iteration 65, loss = 0.12899642\n",
      "Iteration 66, loss = 0.12530336\n",
      "Iteration 67, loss = 0.11987498\n",
      "Iteration 68, loss = 0.11691585\n",
      "Iteration 69, loss = 0.11509221\n",
      "Iteration 70, loss = 0.11173907\n",
      "Iteration 71, loss = 0.10464553\n",
      "Iteration 72, loss = 0.11366481\n",
      "Iteration 73, loss = 0.10656290\n",
      "Iteration 74, loss = 0.10651123\n",
      "Iteration 75, loss = 0.09970900\n",
      "Iteration 76, loss = 0.10460119\n",
      "Iteration 77, loss = 0.09117814\n",
      "Iteration 78, loss = 0.09338745\n",
      "Iteration 79, loss = 0.09054073\n",
      "Iteration 80, loss = 0.09287505\n",
      "Iteration 81, loss = 0.08333032\n",
      "Iteration 82, loss = 0.08136379\n",
      "Iteration 83, loss = 0.07816374\n",
      "Iteration 84, loss = 0.07949503\n",
      "Iteration 85, loss = 0.08142620\n",
      "Iteration 86, loss = 0.08335972\n",
      "Iteration 87, loss = 0.07694453\n",
      "Iteration 88, loss = 0.07464391\n",
      "Iteration 89, loss = 0.07479973\n",
      "Iteration 90, loss = 0.06670374\n",
      "Iteration 91, loss = 0.06166184\n",
      "Iteration 92, loss = 0.06046161\n",
      "Iteration 93, loss = 0.07613044\n",
      "Iteration 94, loss = 0.06149659\n",
      "Iteration 95, loss = 0.05839206\n",
      "Iteration 96, loss = 0.05727764\n",
      "Iteration 97, loss = 0.05342030\n",
      "Iteration 98, loss = 0.05123814\n",
      "Iteration 99, loss = 0.05247221\n",
      "Iteration 100, loss = 0.05202996\n",
      "Iteration 101, loss = 0.05181396\n",
      "Iteration 102, loss = 0.05763982\n",
      "Iteration 103, loss = 0.04789759\n",
      "Iteration 104, loss = 0.04498612\n",
      "Iteration 105, loss = 0.04735874\n",
      "Iteration 106, loss = 0.04554534\n",
      "Iteration 107, loss = 0.04884968\n",
      "Iteration 108, loss = 0.04212602\n",
      "Iteration 109, loss = 0.04843411\n",
      "Iteration 110, loss = 0.03991264\n",
      "Iteration 111, loss = 0.03811196\n",
      "Iteration 112, loss = 0.03644699\n",
      "Iteration 113, loss = 0.03594486\n",
      "Iteration 114, loss = 0.03580574\n",
      "Iteration 115, loss = 0.03403822\n",
      "Iteration 116, loss = 0.03750402\n",
      "Iteration 117, loss = 0.03459805\n",
      "Iteration 118, loss = 0.03156669\n",
      "Iteration 119, loss = 0.03186823\n",
      "Iteration 120, loss = 0.02991239\n",
      "Iteration 121, loss = 0.02927345\n",
      "Iteration 122, loss = 0.02826657\n",
      "Iteration 123, loss = 0.03231144\n",
      "Iteration 124, loss = 0.02767294\n",
      "Iteration 125, loss = 0.02791753\n",
      "Iteration 126, loss = 0.02827186\n",
      "Iteration 127, loss = 0.02708557\n",
      "Iteration 128, loss = 0.02581752\n",
      "Iteration 129, loss = 0.02584465\n",
      "Iteration 130, loss = 0.02723576\n",
      "Iteration 131, loss = 0.02447439\n",
      "Iteration 132, loss = 0.02420294\n",
      "Iteration 133, loss = 0.03815728\n",
      "Iteration 134, loss = 0.02518779\n",
      "Iteration 135, loss = 0.02814718\n",
      "Iteration 136, loss = 0.02090956\n",
      "Iteration 137, loss = 0.02045506\n",
      "Iteration 138, loss = 0.01984304\n",
      "Iteration 139, loss = 0.01823584\n",
      "Iteration 140, loss = 0.02026217\n",
      "Iteration 141, loss = 0.01832254\n",
      "Iteration 142, loss = 0.01747066\n",
      "Iteration 143, loss = 0.01778162\n",
      "Iteration 144, loss = 0.01712790\n",
      "Iteration 145, loss = 0.01852470\n",
      "Iteration 146, loss = 0.01639461\n",
      "Iteration 147, loss = 0.01744154\n",
      "Iteration 148, loss = 0.01740253\n",
      "Iteration 149, loss = 0.01586264\n",
      "Iteration 150, loss = 0.01564658\n",
      "Iteration 151, loss = 0.01489525\n",
      "Iteration 152, loss = 0.01403034\n",
      "Iteration 153, loss = 0.01320072\n",
      "Iteration 154, loss = 0.01384153\n",
      "Iteration 155, loss = 0.01360620\n",
      "Iteration 156, loss = 0.01304801\n",
      "Iteration 157, loss = 0.01304521\n",
      "Iteration 158, loss = 0.01266549\n",
      "Iteration 159, loss = 0.01362403\n",
      "Iteration 160, loss = 0.01219355\n",
      "Iteration 161, loss = 0.02044339\n",
      "Iteration 162, loss = 0.01336032\n",
      "Iteration 163, loss = 0.01324190\n",
      "Iteration 164, loss = 0.01423108\n",
      "Iteration 165, loss = 0.01122700\n",
      "Iteration 166, loss = 0.01083379\n",
      "Iteration 167, loss = 0.01139618\n",
      "Iteration 168, loss = 0.00969258\n",
      "Iteration 169, loss = 0.01011713\n",
      "Iteration 170, loss = 0.01016613\n",
      "Iteration 171, loss = 0.01039078\n",
      "Iteration 172, loss = 0.00866630\n",
      "Iteration 173, loss = 0.00874760\n",
      "Iteration 174, loss = 0.00834792\n",
      "Iteration 175, loss = 0.00870337\n",
      "Iteration 176, loss = 0.00822341\n",
      "Iteration 177, loss = 0.00889544\n",
      "Iteration 178, loss = 0.00971443\n",
      "Iteration 179, loss = 0.01111698\n",
      "Iteration 180, loss = 0.00903823\n",
      "Iteration 181, loss = 0.00845158\n",
      "Iteration 182, loss = 0.00775498\n",
      "Iteration 183, loss = 0.00817881\n",
      "Iteration 184, loss = 0.00898366\n",
      "Iteration 185, loss = 0.02106459\n",
      "Iteration 186, loss = 0.01107019\n",
      "Iteration 187, loss = 0.00792161\n",
      "Iteration 188, loss = 0.00640578\n",
      "Iteration 189, loss = 0.00626782\n",
      "Iteration 190, loss = 0.00606415\n",
      "Iteration 191, loss = 0.00618171\n",
      "Iteration 192, loss = 0.00592142\n",
      "Iteration 193, loss = 0.00578011\n",
      "Iteration 194, loss = 0.00584317\n",
      "Iteration 195, loss = 0.00562948\n",
      "Iteration 196, loss = 0.00562773\n",
      "Iteration 197, loss = 0.00561838\n",
      "Iteration 198, loss = 0.00620028\n",
      "Iteration 199, loss = 0.00516333\n",
      "Iteration 200, loss = 0.00552978\n",
      "Iteration 201, loss = 0.00495733\n",
      "Iteration 202, loss = 0.00469970\n",
      "Iteration 203, loss = 0.00481191\n",
      "Iteration 204, loss = 0.00466242\n",
      "Iteration 205, loss = 0.00458437\n",
      "Iteration 206, loss = 0.00435942\n",
      "Iteration 207, loss = 0.00470492\n",
      "Iteration 208, loss = 0.00533376\n",
      "Iteration 209, loss = 0.00416309\n",
      "Iteration 210, loss = 0.00415989\n",
      "Iteration 211, loss = 0.00403900\n",
      "Iteration 212, loss = 0.00429277\n",
      "Iteration 213, loss = 0.00442655\n",
      "Iteration 214, loss = 0.00477401\n",
      "Iteration 215, loss = 0.00393865\n",
      "Iteration 216, loss = 0.00418130\n",
      "Iteration 217, loss = 0.00419105\n",
      "Iteration 218, loss = 0.00525502\n",
      "Iteration 219, loss = 0.02002591\n",
      "Iteration 220, loss = 0.09914364\n",
      "Iteration 221, loss = 0.03873710\n",
      "Iteration 222, loss = 0.00899871\n",
      "Iteration 223, loss = 0.00523729\n",
      "Iteration 224, loss = 0.00464342\n",
      "Iteration 225, loss = 0.00398327\n",
      "Iteration 226, loss = 0.00381247\n",
      "Iteration 227, loss = 0.00363987\n",
      "Iteration 228, loss = 0.00357969\n",
      "Iteration 229, loss = 0.00371239\n",
      "Iteration 230, loss = 0.00341542\n",
      "Iteration 231, loss = 0.00355651\n",
      "Iteration 232, loss = 0.00343216\n",
      "Iteration 233, loss = 0.00334963\n",
      "Iteration 234, loss = 0.00321536\n",
      "Iteration 235, loss = 0.00329026\n",
      "Iteration 236, loss = 0.00315927\n",
      "Iteration 237, loss = 0.00310754\n",
      "Iteration 238, loss = 0.00306111\n",
      "Iteration 239, loss = 0.00300976\n",
      "Iteration 240, loss = 0.00303998\n",
      "Iteration 241, loss = 0.00294312\n",
      "Iteration 242, loss = 0.00298385\n",
      "Iteration 243, loss = 0.00287445\n",
      "Iteration 244, loss = 0.00283833\n",
      "Iteration 245, loss = 0.00287196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54057278\n",
      "Iteration 2, loss = 0.47389046\n",
      "Iteration 3, loss = 0.44870873\n",
      "Iteration 4, loss = 0.44026370\n",
      "Iteration 5, loss = 0.42819721\n",
      "Iteration 6, loss = 0.42471305\n",
      "Iteration 7, loss = 0.41430390\n",
      "Iteration 8, loss = 0.41681442\n",
      "Iteration 9, loss = 0.40145153\n",
      "Iteration 10, loss = 0.38905151\n",
      "Iteration 11, loss = 0.38530669\n",
      "Iteration 12, loss = 0.38683429\n",
      "Iteration 13, loss = 0.37821018\n",
      "Iteration 14, loss = 0.36393741\n",
      "Iteration 15, loss = 0.35646079\n",
      "Iteration 16, loss = 0.36360590\n",
      "Iteration 17, loss = 0.34142182\n",
      "Iteration 18, loss = 0.33813566\n",
      "Iteration 19, loss = 0.33606393\n",
      "Iteration 20, loss = 0.32336469\n",
      "Iteration 21, loss = 0.31818244\n",
      "Iteration 22, loss = 0.32138017\n",
      "Iteration 23, loss = 0.31133553\n",
      "Iteration 24, loss = 0.29811024\n",
      "Iteration 25, loss = 0.29294163\n",
      "Iteration 26, loss = 0.28626810\n",
      "Iteration 27, loss = 0.27969579\n",
      "Iteration 28, loss = 0.27164398\n",
      "Iteration 29, loss = 0.26723499\n",
      "Iteration 30, loss = 0.25815796\n",
      "Iteration 31, loss = 0.25538027\n",
      "Iteration 32, loss = 0.24486475\n",
      "Iteration 33, loss = 0.24108873\n",
      "Iteration 34, loss = 0.24353674\n",
      "Iteration 35, loss = 0.23111340\n",
      "Iteration 36, loss = 0.22651444\n",
      "Iteration 37, loss = 0.22512585\n",
      "Iteration 38, loss = 0.21238978\n",
      "Iteration 39, loss = 0.22438397\n",
      "Iteration 40, loss = 0.20473952\n",
      "Iteration 41, loss = 0.20735580\n",
      "Iteration 42, loss = 0.20390611\n",
      "Iteration 43, loss = 0.18809904\n",
      "Iteration 44, loss = 0.18638939\n",
      "Iteration 45, loss = 0.18492985\n",
      "Iteration 46, loss = 0.17797995\n",
      "Iteration 47, loss = 0.16673372\n",
      "Iteration 48, loss = 0.16609021\n",
      "Iteration 49, loss = 0.16251511\n",
      "Iteration 50, loss = 0.16356287\n",
      "Iteration 51, loss = 0.16474762\n",
      "Iteration 52, loss = 0.16693952\n",
      "Iteration 53, loss = 0.14274164\n",
      "Iteration 54, loss = 0.13839875\n",
      "Iteration 55, loss = 0.13244270\n",
      "Iteration 56, loss = 0.13405991\n",
      "Iteration 57, loss = 0.13396989\n",
      "Iteration 58, loss = 0.12784313\n",
      "Iteration 59, loss = 0.12254478\n",
      "Iteration 60, loss = 0.11850408\n",
      "Iteration 61, loss = 0.11448941\n",
      "Iteration 62, loss = 0.10924597\n",
      "Iteration 63, loss = 0.11612967\n",
      "Iteration 64, loss = 0.11530257\n",
      "Iteration 65, loss = 0.10716720\n",
      "Iteration 66, loss = 0.10334570\n",
      "Iteration 67, loss = 0.09808003\n",
      "Iteration 68, loss = 0.09753746\n",
      "Iteration 69, loss = 0.09173094\n",
      "Iteration 70, loss = 0.08752448\n",
      "Iteration 71, loss = 0.08447432\n",
      "Iteration 72, loss = 0.08448851\n",
      "Iteration 73, loss = 0.08424035\n",
      "Iteration 74, loss = 0.08359283\n",
      "Iteration 75, loss = 0.08341764\n",
      "Iteration 76, loss = 0.07618480\n",
      "Iteration 77, loss = 0.07074890\n",
      "Iteration 78, loss = 0.07300766\n",
      "Iteration 79, loss = 0.07445840\n",
      "Iteration 80, loss = 0.07510263\n",
      "Iteration 81, loss = 0.06583868\n",
      "Iteration 82, loss = 0.06408481\n",
      "Iteration 83, loss = 0.06055861\n",
      "Iteration 84, loss = 0.06267106\n",
      "Iteration 85, loss = 0.06592232\n",
      "Iteration 86, loss = 0.08485846\n",
      "Iteration 87, loss = 0.05980992\n",
      "Iteration 88, loss = 0.05574470\n",
      "Iteration 89, loss = 0.05779777\n",
      "Iteration 90, loss = 0.05029062\n",
      "Iteration 91, loss = 0.04645199\n",
      "Iteration 92, loss = 0.04636901\n",
      "Iteration 93, loss = 0.06473809\n",
      "Iteration 94, loss = 0.04610211\n",
      "Iteration 95, loss = 0.04417089\n",
      "Iteration 96, loss = 0.04264957\n",
      "Iteration 97, loss = 0.04215474\n",
      "Iteration 98, loss = 0.03936707\n",
      "Iteration 99, loss = 0.03974893\n",
      "Iteration 100, loss = 0.03984566\n",
      "Iteration 101, loss = 0.03935128\n",
      "Iteration 102, loss = 0.03897345\n",
      "Iteration 103, loss = 0.03537634\n",
      "Iteration 104, loss = 0.03375434\n",
      "Iteration 105, loss = 0.03423275\n",
      "Iteration 106, loss = 0.03286859\n",
      "Iteration 107, loss = 0.04239526\n",
      "Iteration 108, loss = 0.03437566\n",
      "Iteration 109, loss = 0.05277719\n",
      "Iteration 110, loss = 0.03071531\n",
      "Iteration 111, loss = 0.02983421\n",
      "Iteration 112, loss = 0.02713167\n",
      "Iteration 113, loss = 0.02845180\n",
      "Iteration 114, loss = 0.02763795\n",
      "Iteration 115, loss = 0.02591477\n",
      "Iteration 116, loss = 0.03266489\n",
      "Iteration 117, loss = 0.02571395\n",
      "Iteration 118, loss = 0.02322465\n",
      "Iteration 119, loss = 0.02350013\n",
      "Iteration 120, loss = 0.02331134\n",
      "Iteration 121, loss = 0.02371283\n",
      "Iteration 122, loss = 0.02262253\n",
      "Iteration 123, loss = 0.02877597\n",
      "Iteration 124, loss = 0.02047210\n",
      "Iteration 125, loss = 0.01984292\n",
      "Iteration 126, loss = 0.02006164\n",
      "Iteration 127, loss = 0.02234489\n",
      "Iteration 128, loss = 0.02797148\n",
      "Iteration 129, loss = 0.02225435\n",
      "Iteration 130, loss = 0.02739633\n",
      "Iteration 131, loss = 0.01847233\n",
      "Iteration 132, loss = 0.02067300\n",
      "Iteration 133, loss = 0.03117321\n",
      "Iteration 134, loss = 0.01740946\n",
      "Iteration 135, loss = 0.01824502\n",
      "Iteration 136, loss = 0.01464304\n",
      "Iteration 137, loss = 0.01544742\n",
      "Iteration 138, loss = 0.01644995\n",
      "Iteration 139, loss = 0.01329525\n",
      "Iteration 140, loss = 0.01461812\n",
      "Iteration 141, loss = 0.01365215\n",
      "Iteration 142, loss = 0.01289169\n",
      "Iteration 143, loss = 0.01361529\n",
      "Iteration 144, loss = 0.01264412\n",
      "Iteration 145, loss = 0.01308228\n",
      "Iteration 146, loss = 0.01178304\n",
      "Iteration 147, loss = 0.01177024\n",
      "Iteration 148, loss = 0.01292242\n",
      "Iteration 149, loss = 0.01119261\n",
      "Iteration 150, loss = 0.01300303\n",
      "Iteration 151, loss = 0.01211339\n",
      "Iteration 152, loss = 0.01075000\n",
      "Iteration 153, loss = 0.00985587\n",
      "Iteration 154, loss = 0.01231885\n",
      "Iteration 155, loss = 0.01027927\n",
      "Iteration 156, loss = 0.01099557\n",
      "Iteration 157, loss = 0.01049771\n",
      "Iteration 158, loss = 0.00965590\n",
      "Iteration 159, loss = 0.01019337\n",
      "Iteration 160, loss = 0.00940263\n",
      "Iteration 161, loss = 0.01093173\n",
      "Iteration 162, loss = 0.00984899\n",
      "Iteration 163, loss = 0.00852588\n",
      "Iteration 164, loss = 0.00988123\n",
      "Iteration 165, loss = 0.00779471\n",
      "Iteration 166, loss = 0.00795408\n",
      "Iteration 167, loss = 0.01103220\n",
      "Iteration 168, loss = 0.00853438\n",
      "Iteration 169, loss = 0.00843556\n",
      "Iteration 170, loss = 0.00814417\n",
      "Iteration 171, loss = 0.00955136\n",
      "Iteration 172, loss = 0.00693522\n",
      "Iteration 173, loss = 0.00656391\n",
      "Iteration 174, loss = 0.00728418\n",
      "Iteration 175, loss = 0.00757688\n",
      "Iteration 176, loss = 0.00708122\n",
      "Iteration 177, loss = 0.00847726\n",
      "Iteration 178, loss = 0.00667927\n",
      "Iteration 179, loss = 0.00770905\n",
      "Iteration 180, loss = 0.00571908\n",
      "Iteration 181, loss = 0.00536856\n",
      "Iteration 182, loss = 0.00738259\n",
      "Iteration 183, loss = 0.00707610\n",
      "Iteration 184, loss = 0.00823752\n",
      "Iteration 185, loss = 0.01407548\n",
      "Iteration 186, loss = 0.00634858\n",
      "Iteration 187, loss = 0.00534066\n",
      "Iteration 188, loss = 0.00501874\n",
      "Iteration 189, loss = 0.00478772\n",
      "Iteration 190, loss = 0.00617521\n",
      "Iteration 191, loss = 0.00745354\n",
      "Iteration 192, loss = 0.00555672\n",
      "Iteration 193, loss = 0.00505275\n",
      "Iteration 194, loss = 0.01014012\n",
      "Iteration 195, loss = 0.00733020\n",
      "Iteration 196, loss = 0.00452418\n",
      "Iteration 197, loss = 0.00524917\n",
      "Iteration 198, loss = 0.00686366\n",
      "Iteration 199, loss = 0.00424587\n",
      "Iteration 200, loss = 0.00465025\n",
      "Iteration 201, loss = 0.00374244\n",
      "Iteration 202, loss = 0.00338312\n",
      "Iteration 203, loss = 0.00348072\n",
      "Iteration 204, loss = 0.00334241\n",
      "Iteration 205, loss = 0.00338805\n",
      "Iteration 206, loss = 0.00342842\n",
      "Iteration 207, loss = 0.00328028\n",
      "Iteration 208, loss = 0.00328992\n",
      "Iteration 209, loss = 0.00315102\n",
      "Iteration 210, loss = 0.00300291\n",
      "Iteration 211, loss = 0.00293942\n",
      "Iteration 212, loss = 0.00317908\n",
      "Iteration 213, loss = 0.00338060\n",
      "Iteration 214, loss = 0.00506489\n",
      "Iteration 215, loss = 0.00516527\n",
      "Iteration 216, loss = 0.01060773\n",
      "Iteration 217, loss = 0.02440887\n",
      "Iteration 218, loss = 0.03586795\n",
      "Iteration 219, loss = 0.01872685\n",
      "Iteration 220, loss = 0.00891102\n",
      "Iteration 221, loss = 0.00519068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53851553\n",
      "Iteration 2, loss = 0.47218502\n",
      "Iteration 3, loss = 0.44728878\n",
      "Iteration 4, loss = 0.43773355\n",
      "Iteration 5, loss = 0.42556753\n",
      "Iteration 6, loss = 0.41992261\n",
      "Iteration 7, loss = 0.40849034\n",
      "Iteration 8, loss = 0.41233843\n",
      "Iteration 9, loss = 0.39748184\n",
      "Iteration 10, loss = 0.38295612\n",
      "Iteration 11, loss = 0.37886715\n",
      "Iteration 12, loss = 0.37894898\n",
      "Iteration 13, loss = 0.37099973\n",
      "Iteration 14, loss = 0.35543344\n",
      "Iteration 15, loss = 0.34602953\n",
      "Iteration 16, loss = 0.35265332\n",
      "Iteration 17, loss = 0.33150880\n",
      "Iteration 18, loss = 0.32716997\n",
      "Iteration 19, loss = 0.32551857\n",
      "Iteration 20, loss = 0.31238719\n",
      "Iteration 21, loss = 0.30802223\n",
      "Iteration 22, loss = 0.31294414\n",
      "Iteration 23, loss = 0.29860368\n",
      "Iteration 24, loss = 0.28628721\n",
      "Iteration 25, loss = 0.28370827\n",
      "Iteration 26, loss = 0.27334658\n",
      "Iteration 27, loss = 0.26828557\n",
      "Iteration 28, loss = 0.25818363\n",
      "Iteration 29, loss = 0.25890066\n",
      "Iteration 30, loss = 0.24970913\n",
      "Iteration 31, loss = 0.24395947\n",
      "Iteration 32, loss = 0.23334930\n",
      "Iteration 33, loss = 0.22999457\n",
      "Iteration 34, loss = 0.23441118\n",
      "Iteration 35, loss = 0.22133296\n",
      "Iteration 36, loss = 0.21451984\n",
      "Iteration 37, loss = 0.21551770\n",
      "Iteration 38, loss = 0.20243741\n",
      "Iteration 39, loss = 0.21275313\n",
      "Iteration 40, loss = 0.19883748\n",
      "Iteration 41, loss = 0.19648152\n",
      "Iteration 42, loss = 0.19353134\n",
      "Iteration 43, loss = 0.17908098\n",
      "Iteration 44, loss = 0.17905188\n",
      "Iteration 45, loss = 0.17292241\n",
      "Iteration 46, loss = 0.16579909\n",
      "Iteration 47, loss = 0.15951751\n",
      "Iteration 48, loss = 0.16049761\n",
      "Iteration 49, loss = 0.15394889\n",
      "Iteration 50, loss = 0.15265157\n",
      "Iteration 51, loss = 0.15207709\n",
      "Iteration 52, loss = 0.14558504\n",
      "Iteration 53, loss = 0.13897122\n",
      "Iteration 54, loss = 0.12991078\n",
      "Iteration 55, loss = 0.12520102\n",
      "Iteration 56, loss = 0.12521350\n",
      "Iteration 57, loss = 0.12128132\n",
      "Iteration 58, loss = 0.12265433\n",
      "Iteration 59, loss = 0.11704166\n",
      "Iteration 60, loss = 0.11215969\n",
      "Iteration 61, loss = 0.10807378\n",
      "Iteration 62, loss = 0.10021898\n",
      "Iteration 63, loss = 0.10639399\n",
      "Iteration 64, loss = 0.10918757\n",
      "Iteration 65, loss = 0.09813529\n",
      "Iteration 66, loss = 0.09815461\n",
      "Iteration 67, loss = 0.09172541\n",
      "Iteration 68, loss = 0.08741686\n",
      "Iteration 69, loss = 0.08426681\n",
      "Iteration 70, loss = 0.08080403\n",
      "Iteration 71, loss = 0.07707586\n",
      "Iteration 72, loss = 0.07718948\n",
      "Iteration 73, loss = 0.07853924\n",
      "Iteration 74, loss = 0.07899437\n",
      "Iteration 75, loss = 0.07387236\n",
      "Iteration 76, loss = 0.06908427\n",
      "Iteration 77, loss = 0.06426575\n",
      "Iteration 78, loss = 0.06958939\n",
      "Iteration 79, loss = 0.06783933\n",
      "Iteration 80, loss = 0.07138953\n",
      "Iteration 81, loss = 0.06223134\n",
      "Iteration 82, loss = 0.05900119\n",
      "Iteration 83, loss = 0.05479979\n",
      "Iteration 84, loss = 0.05819657\n",
      "Iteration 85, loss = 0.06402372\n",
      "Iteration 86, loss = 0.06439524\n",
      "Iteration 87, loss = 0.05677286\n",
      "Iteration 88, loss = 0.06031550\n",
      "Iteration 89, loss = 0.04948489\n",
      "Iteration 90, loss = 0.04490443\n",
      "Iteration 91, loss = 0.04248366\n",
      "Iteration 92, loss = 0.04265474\n",
      "Iteration 93, loss = 0.06050982\n",
      "Iteration 94, loss = 0.04150656\n",
      "Iteration 95, loss = 0.04022654\n",
      "Iteration 96, loss = 0.03793773\n",
      "Iteration 97, loss = 0.03819678\n",
      "Iteration 98, loss = 0.03591569\n",
      "Iteration 99, loss = 0.03480842\n",
      "Iteration 100, loss = 0.03550269\n",
      "Iteration 101, loss = 0.03514239\n",
      "Iteration 102, loss = 0.03759834\n",
      "Iteration 103, loss = 0.03002490\n",
      "Iteration 104, loss = 0.03140570\n",
      "Iteration 105, loss = 0.03359681\n",
      "Iteration 106, loss = 0.02985163\n",
      "Iteration 107, loss = 0.03493409\n",
      "Iteration 108, loss = 0.03008382\n",
      "Iteration 109, loss = 0.04683123\n",
      "Iteration 110, loss = 0.02990515\n",
      "Iteration 111, loss = 0.02688717\n",
      "Iteration 112, loss = 0.02433870\n",
      "Iteration 113, loss = 0.02455137\n",
      "Iteration 114, loss = 0.02429504\n",
      "Iteration 115, loss = 0.02293043\n",
      "Iteration 116, loss = 0.02491300\n",
      "Iteration 117, loss = 0.02212996\n",
      "Iteration 118, loss = 0.02090929\n",
      "Iteration 119, loss = 0.02152744\n",
      "Iteration 120, loss = 0.02038445\n",
      "Iteration 121, loss = 0.02617581\n",
      "Iteration 122, loss = 0.01980982\n",
      "Iteration 123, loss = 0.02126785\n",
      "Iteration 124, loss = 0.01852328\n",
      "Iteration 125, loss = 0.01818260\n",
      "Iteration 126, loss = 0.01786361\n",
      "Iteration 127, loss = 0.02135577\n",
      "Iteration 128, loss = 0.02786112\n",
      "Iteration 129, loss = 0.01962790\n",
      "Iteration 130, loss = 0.03000883\n",
      "Iteration 131, loss = 0.01797053\n",
      "Iteration 132, loss = 0.01616258\n",
      "Iteration 133, loss = 0.04352675\n",
      "Iteration 134, loss = 0.02013936\n",
      "Iteration 135, loss = 0.02706267\n",
      "Iteration 136, loss = 0.01467796\n",
      "Iteration 137, loss = 0.01393617\n",
      "Iteration 138, loss = 0.01374038\n",
      "Iteration 139, loss = 0.01186882\n",
      "Iteration 140, loss = 0.01342646\n",
      "Iteration 141, loss = 0.01203309\n",
      "Iteration 142, loss = 0.01304872\n",
      "Iteration 143, loss = 0.01197597\n",
      "Iteration 144, loss = 0.01175072\n",
      "Iteration 145, loss = 0.01278309\n",
      "Iteration 146, loss = 0.01049373\n",
      "Iteration 147, loss = 0.01085045\n",
      "Iteration 148, loss = 0.01114306\n",
      "Iteration 149, loss = 0.01030858\n",
      "Iteration 150, loss = 0.01116876\n",
      "Iteration 151, loss = 0.01328108\n",
      "Iteration 152, loss = 0.01014565\n",
      "Iteration 153, loss = 0.00967287\n",
      "Iteration 154, loss = 0.00938674\n",
      "Iteration 155, loss = 0.00930808\n",
      "Iteration 156, loss = 0.01009582\n",
      "Iteration 157, loss = 0.01059323\n",
      "Iteration 158, loss = 0.01043783\n",
      "Iteration 159, loss = 0.00985884\n",
      "Iteration 160, loss = 0.00841123\n",
      "Iteration 161, loss = 0.01053443\n",
      "Iteration 162, loss = 0.01108145\n",
      "Iteration 163, loss = 0.00811326\n",
      "Iteration 164, loss = 0.00798063\n",
      "Iteration 165, loss = 0.00720138\n",
      "Iteration 166, loss = 0.00722079\n",
      "Iteration 167, loss = 0.00969529\n",
      "Iteration 168, loss = 0.00741881\n",
      "Iteration 169, loss = 0.00769697\n",
      "Iteration 170, loss = 0.00763673\n",
      "Iteration 171, loss = 0.01131502\n",
      "Iteration 172, loss = 0.00879965\n",
      "Iteration 173, loss = 0.00672634\n",
      "Iteration 174, loss = 0.00678959\n",
      "Iteration 175, loss = 0.00675412\n",
      "Iteration 176, loss = 0.00681714\n",
      "Iteration 177, loss = 0.00778272\n",
      "Iteration 178, loss = 0.00692479\n",
      "Iteration 179, loss = 0.00652824\n",
      "Iteration 180, loss = 0.00548034\n",
      "Iteration 181, loss = 0.00506209\n",
      "Iteration 182, loss = 0.00580657\n",
      "Iteration 183, loss = 0.00604964\n",
      "Iteration 184, loss = 0.00775926\n",
      "Iteration 185, loss = 0.04408714\n",
      "Iteration 186, loss = 0.00919929\n",
      "Iteration 187, loss = 0.00638635\n",
      "Iteration 188, loss = 0.00556500\n",
      "Iteration 189, loss = 0.00488479\n",
      "Iteration 190, loss = 0.00443674\n",
      "Iteration 191, loss = 0.00456237\n",
      "Iteration 192, loss = 0.00420747\n",
      "Iteration 193, loss = 0.00420344\n",
      "Iteration 194, loss = 0.00468278\n",
      "Iteration 195, loss = 0.00444059\n",
      "Iteration 196, loss = 0.00414000\n",
      "Iteration 197, loss = 0.00398720\n",
      "Iteration 198, loss = 0.00564006\n",
      "Iteration 199, loss = 0.00426648\n",
      "Iteration 200, loss = 0.00401210\n",
      "Iteration 201, loss = 0.00387651\n",
      "Iteration 202, loss = 0.00352358\n",
      "Iteration 203, loss = 0.00353380\n",
      "Iteration 204, loss = 0.00344874\n",
      "Iteration 205, loss = 0.00359679\n",
      "Iteration 206, loss = 0.00337514\n",
      "Iteration 207, loss = 0.00330412\n",
      "Iteration 208, loss = 0.00362284\n",
      "Iteration 209, loss = 0.00316071\n",
      "Iteration 210, loss = 0.00315131\n",
      "Iteration 211, loss = 0.00322077\n",
      "Iteration 212, loss = 0.00338507\n",
      "Iteration 213, loss = 0.00399701\n",
      "Iteration 214, loss = 0.00408152\n",
      "Iteration 215, loss = 0.00318235\n",
      "Iteration 216, loss = 0.00560216\n",
      "Iteration 217, loss = 0.00939198\n",
      "Iteration 218, loss = 0.00636889\n",
      "Iteration 219, loss = 0.00652768\n",
      "Iteration 220, loss = 0.00646328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53458805\n",
      "Iteration 2, loss = 0.46992276\n",
      "Iteration 3, loss = 0.44217386\n",
      "Iteration 4, loss = 0.43169958\n",
      "Iteration 5, loss = 0.42055690\n",
      "Iteration 6, loss = 0.41591826\n",
      "Iteration 7, loss = 0.40224296\n",
      "Iteration 8, loss = 0.40952452\n",
      "Iteration 9, loss = 0.39161571\n",
      "Iteration 10, loss = 0.37936390\n",
      "Iteration 11, loss = 0.37238167\n",
      "Iteration 12, loss = 0.37548345\n",
      "Iteration 13, loss = 0.36683714\n",
      "Iteration 14, loss = 0.35692515\n",
      "Iteration 15, loss = 0.34318120\n",
      "Iteration 16, loss = 0.34990729\n",
      "Iteration 17, loss = 0.32988291\n",
      "Iteration 18, loss = 0.32753365\n",
      "Iteration 19, loss = 0.32353720\n",
      "Iteration 20, loss = 0.31163600\n",
      "Iteration 21, loss = 0.30565854\n",
      "Iteration 22, loss = 0.30734708\n",
      "Iteration 23, loss = 0.29730126\n",
      "Iteration 24, loss = 0.28683754\n",
      "Iteration 25, loss = 0.28178970\n",
      "Iteration 26, loss = 0.27343019\n",
      "Iteration 27, loss = 0.26775084\n",
      "Iteration 28, loss = 0.25915453\n",
      "Iteration 29, loss = 0.25884250\n",
      "Iteration 30, loss = 0.25007873\n",
      "Iteration 31, loss = 0.24202423\n",
      "Iteration 32, loss = 0.23217505\n",
      "Iteration 33, loss = 0.22677365\n",
      "Iteration 34, loss = 0.23128560\n",
      "Iteration 35, loss = 0.22115433\n",
      "Iteration 36, loss = 0.21408007\n",
      "Iteration 37, loss = 0.21750804\n",
      "Iteration 38, loss = 0.20037093\n",
      "Iteration 39, loss = 0.20964255\n",
      "Iteration 40, loss = 0.19679006\n",
      "Iteration 41, loss = 0.19253582\n",
      "Iteration 42, loss = 0.19378890\n",
      "Iteration 43, loss = 0.18023685\n",
      "Iteration 44, loss = 0.17763039\n",
      "Iteration 45, loss = 0.16962830\n",
      "Iteration 46, loss = 0.16473227\n",
      "Iteration 47, loss = 0.16070300\n",
      "Iteration 48, loss = 0.16284779\n",
      "Iteration 49, loss = 0.15288508\n",
      "Iteration 50, loss = 0.15137494\n",
      "Iteration 51, loss = 0.15016969\n",
      "Iteration 52, loss = 0.14304006\n",
      "Iteration 53, loss = 0.13378229\n",
      "Iteration 54, loss = 0.13292405\n",
      "Iteration 55, loss = 0.12433734\n",
      "Iteration 56, loss = 0.12412817\n",
      "Iteration 57, loss = 0.11873306\n",
      "Iteration 58, loss = 0.11719091\n",
      "Iteration 59, loss = 0.11359230\n",
      "Iteration 60, loss = 0.10946149\n",
      "Iteration 61, loss = 0.10681594\n",
      "Iteration 62, loss = 0.10094358\n",
      "Iteration 63, loss = 0.10361231\n",
      "Iteration 64, loss = 0.10940171\n",
      "Iteration 65, loss = 0.10194273\n",
      "Iteration 66, loss = 0.09837907\n",
      "Iteration 67, loss = 0.09006877\n",
      "Iteration 68, loss = 0.08562593\n",
      "Iteration 69, loss = 0.08215370\n",
      "Iteration 70, loss = 0.07980027\n",
      "Iteration 71, loss = 0.07506279\n",
      "Iteration 72, loss = 0.07800905\n",
      "Iteration 73, loss = 0.07961810\n",
      "Iteration 74, loss = 0.07880833\n",
      "Iteration 75, loss = 0.07319723\n",
      "Iteration 76, loss = 0.07058047\n",
      "Iteration 77, loss = 0.06392637\n",
      "Iteration 78, loss = 0.06864395\n",
      "Iteration 79, loss = 0.07066003\n",
      "Iteration 80, loss = 0.07208484\n",
      "Iteration 81, loss = 0.06710820\n",
      "Iteration 82, loss = 0.05767369\n",
      "Iteration 83, loss = 0.05430274\n",
      "Iteration 84, loss = 0.05985657\n",
      "Iteration 85, loss = 0.05775555\n",
      "Iteration 86, loss = 0.05790289\n",
      "Iteration 87, loss = 0.05157760\n",
      "Iteration 88, loss = 0.05139531\n",
      "Iteration 89, loss = 0.05232107\n",
      "Iteration 90, loss = 0.04731258\n",
      "Iteration 91, loss = 0.04253164\n",
      "Iteration 92, loss = 0.04183666\n",
      "Iteration 93, loss = 0.04904195\n",
      "Iteration 94, loss = 0.04082207\n",
      "Iteration 95, loss = 0.03894403\n",
      "Iteration 96, loss = 0.04195806\n",
      "Iteration 97, loss = 0.03840492\n",
      "Iteration 98, loss = 0.03715738\n",
      "Iteration 99, loss = 0.03512553\n",
      "Iteration 100, loss = 0.03666159\n",
      "Iteration 101, loss = 0.03617165\n",
      "Iteration 102, loss = 0.03430152\n",
      "Iteration 103, loss = 0.03007281\n",
      "Iteration 104, loss = 0.03212642\n",
      "Iteration 105, loss = 0.03461726\n",
      "Iteration 106, loss = 0.03163930\n",
      "Iteration 107, loss = 0.04185354\n",
      "Iteration 108, loss = 0.03277062\n",
      "Iteration 109, loss = 0.06431695\n",
      "Iteration 110, loss = 0.03708339\n",
      "Iteration 111, loss = 0.02816643\n",
      "Iteration 112, loss = 0.02394804\n",
      "Iteration 113, loss = 0.02626901\n",
      "Iteration 114, loss = 0.02507344\n",
      "Iteration 115, loss = 0.02266640\n",
      "Iteration 116, loss = 0.02632637\n",
      "Iteration 117, loss = 0.02226916\n",
      "Iteration 118, loss = 0.02151187\n",
      "Iteration 119, loss = 0.02253194\n",
      "Iteration 120, loss = 0.02041104\n",
      "Iteration 121, loss = 0.01961510\n",
      "Iteration 122, loss = 0.01952480\n",
      "Iteration 123, loss = 0.02001481\n",
      "Iteration 124, loss = 0.01845646\n",
      "Iteration 125, loss = 0.01819322\n",
      "Iteration 126, loss = 0.01853243\n",
      "Iteration 127, loss = 0.01966249\n",
      "Iteration 128, loss = 0.02333002\n",
      "Iteration 129, loss = 0.01973755\n",
      "Iteration 130, loss = 0.02604066\n",
      "Iteration 131, loss = 0.01692343\n",
      "Iteration 132, loss = 0.01723586\n",
      "Iteration 133, loss = 0.05910632\n",
      "Iteration 134, loss = 0.02194279\n",
      "Iteration 135, loss = 0.01859526\n",
      "Iteration 136, loss = 0.01521175\n",
      "Iteration 137, loss = 0.01396535\n",
      "Iteration 138, loss = 0.01386075\n",
      "Iteration 139, loss = 0.01211785\n",
      "Iteration 140, loss = 0.01222871\n",
      "Iteration 141, loss = 0.01211468\n",
      "Iteration 142, loss = 0.01289758\n",
      "Iteration 143, loss = 0.01218990\n",
      "Iteration 144, loss = 0.01192899\n",
      "Iteration 145, loss = 0.01253266\n",
      "Iteration 146, loss = 0.01091327\n",
      "Iteration 147, loss = 0.01111775\n",
      "Iteration 148, loss = 0.01115058\n",
      "Iteration 149, loss = 0.01089139\n",
      "Iteration 150, loss = 0.01070058\n",
      "Iteration 151, loss = 0.01172489\n",
      "Iteration 152, loss = 0.00971212\n",
      "Iteration 153, loss = 0.00967269\n",
      "Iteration 154, loss = 0.00960454\n",
      "Iteration 155, loss = 0.00980014\n",
      "Iteration 156, loss = 0.01353731\n",
      "Iteration 157, loss = 0.01105729\n",
      "Iteration 158, loss = 0.01050507\n",
      "Iteration 159, loss = 0.01073787\n",
      "Iteration 160, loss = 0.00820707\n",
      "Iteration 161, loss = 0.00996721\n",
      "Iteration 162, loss = 0.00867571\n",
      "Iteration 163, loss = 0.00844652\n",
      "Iteration 164, loss = 0.00750937\n",
      "Iteration 165, loss = 0.00703935\n",
      "Iteration 166, loss = 0.00781517\n",
      "Iteration 167, loss = 0.01120631\n",
      "Iteration 168, loss = 0.00888863\n",
      "Iteration 169, loss = 0.00798166\n",
      "Iteration 170, loss = 0.00842789\n",
      "Iteration 171, loss = 0.00823222\n",
      "Iteration 172, loss = 0.00645533\n",
      "Iteration 173, loss = 0.00700645\n",
      "Iteration 174, loss = 0.00690761\n",
      "Iteration 175, loss = 0.00705775\n",
      "Iteration 176, loss = 0.00672114\n",
      "Iteration 177, loss = 0.00709975\n",
      "Iteration 178, loss = 0.00712901\n",
      "Iteration 179, loss = 0.00799708\n",
      "Iteration 180, loss = 0.00697367\n",
      "Iteration 181, loss = 0.00584693\n",
      "Iteration 182, loss = 0.00667029\n",
      "Iteration 183, loss = 0.00802486\n",
      "Iteration 184, loss = 0.00783240\n",
      "Iteration 185, loss = 0.02567925\n",
      "Iteration 186, loss = 0.01166769\n",
      "Iteration 187, loss = 0.00756543\n",
      "Iteration 188, loss = 0.00699116\n",
      "Iteration 189, loss = 0.00618604\n",
      "Iteration 190, loss = 0.00548696\n",
      "Iteration 191, loss = 0.00483662\n",
      "Iteration 192, loss = 0.00418665\n",
      "Iteration 193, loss = 0.00457512\n",
      "Iteration 194, loss = 0.00976175\n",
      "Iteration 195, loss = 0.07365144\n",
      "Iteration 196, loss = 0.03171016\n",
      "Iteration 197, loss = 0.00959829\n",
      "Iteration 198, loss = 0.00603924\n",
      "Iteration 199, loss = 0.00458561\n",
      "Iteration 200, loss = 0.00460839\n",
      "Iteration 201, loss = 0.00392302\n",
      "Iteration 202, loss = 0.00375843\n",
      "Iteration 203, loss = 0.00362749\n",
      "Iteration 204, loss = 0.00375755\n",
      "Iteration 205, loss = 0.00359800\n",
      "Iteration 206, loss = 0.00349318\n",
      "Iteration 207, loss = 0.00343431\n",
      "Iteration 208, loss = 0.00339522\n",
      "Iteration 209, loss = 0.00334597\n",
      "Iteration 210, loss = 0.00323253\n",
      "Iteration 211, loss = 0.00316946\n",
      "Iteration 212, loss = 0.00372180\n",
      "Iteration 213, loss = 0.00374301\n",
      "Iteration 214, loss = 0.00318437\n",
      "Iteration 215, loss = 0.00310937\n",
      "Iteration 216, loss = 0.00308072\n",
      "Iteration 217, loss = 0.00311546\n",
      "Iteration 218, loss = 0.00297910\n",
      "Iteration 219, loss = 0.00295874\n",
      "Iteration 220, loss = 0.00298773\n",
      "Iteration 221, loss = 0.00285338\n",
      "Iteration 222, loss = 0.00281546\n",
      "Iteration 223, loss = 0.00283350\n",
      "Iteration 224, loss = 0.00307414\n",
      "Iteration 225, loss = 0.00277958\n",
      "Iteration 226, loss = 0.00273051\n",
      "Iteration 227, loss = 0.00277404\n",
      "Iteration 228, loss = 0.00264582\n",
      "Iteration 229, loss = 0.00339933\n",
      "Iteration 230, loss = 0.00276887\n",
      "Iteration 231, loss = 0.00267732\n",
      "Iteration 232, loss = 0.00257219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52571280\n",
      "Iteration 2, loss = 0.47156248\n",
      "Iteration 3, loss = 0.44091226\n",
      "Iteration 4, loss = 0.43062033\n",
      "Iteration 5, loss = 0.42004176\n",
      "Iteration 6, loss = 0.41308043\n",
      "Iteration 7, loss = 0.40119062\n",
      "Iteration 8, loss = 0.40942031\n",
      "Iteration 9, loss = 0.39172265\n",
      "Iteration 10, loss = 0.37982099\n",
      "Iteration 11, loss = 0.37123261\n",
      "Iteration 12, loss = 0.37327929\n",
      "Iteration 13, loss = 0.36648432\n",
      "Iteration 14, loss = 0.35761447\n",
      "Iteration 15, loss = 0.34205688\n",
      "Iteration 16, loss = 0.34912078\n",
      "Iteration 17, loss = 0.33012152\n",
      "Iteration 18, loss = 0.32640597\n",
      "Iteration 19, loss = 0.32662919\n",
      "Iteration 20, loss = 0.31270116\n",
      "Iteration 21, loss = 0.30725761\n",
      "Iteration 22, loss = 0.30655578\n",
      "Iteration 23, loss = 0.29909417\n",
      "Iteration 24, loss = 0.29160813\n",
      "Iteration 25, loss = 0.28528106\n",
      "Iteration 26, loss = 0.27622055\n",
      "Iteration 27, loss = 0.26955496\n",
      "Iteration 28, loss = 0.26354567\n",
      "Iteration 29, loss = 0.26180784\n",
      "Iteration 30, loss = 0.25519522\n",
      "Iteration 31, loss = 0.24847862\n",
      "Iteration 32, loss = 0.24006270\n",
      "Iteration 33, loss = 0.23402997\n",
      "Iteration 34, loss = 0.23632874\n",
      "Iteration 35, loss = 0.22783840\n",
      "Iteration 36, loss = 0.22057915\n",
      "Iteration 37, loss = 0.22248930\n",
      "Iteration 38, loss = 0.20929466\n",
      "Iteration 39, loss = 0.21677065\n",
      "Iteration 40, loss = 0.20715268\n",
      "Iteration 41, loss = 0.20066713\n",
      "Iteration 42, loss = 0.19452203\n",
      "Iteration 43, loss = 0.18921634\n",
      "Iteration 44, loss = 0.18539675\n",
      "Iteration 45, loss = 0.17965175\n",
      "Iteration 46, loss = 0.17496354\n",
      "Iteration 47, loss = 0.16660272\n",
      "Iteration 48, loss = 0.16450791\n",
      "Iteration 49, loss = 0.16223433\n",
      "Iteration 50, loss = 0.16327178\n",
      "Iteration 51, loss = 0.16631644\n",
      "Iteration 52, loss = 0.15554232\n",
      "Iteration 53, loss = 0.14521782\n",
      "Iteration 54, loss = 0.14371255\n",
      "Iteration 55, loss = 0.13819364\n",
      "Iteration 56, loss = 0.13638741\n",
      "Iteration 57, loss = 0.13304469\n",
      "Iteration 58, loss = 0.12581595\n",
      "Iteration 59, loss = 0.12418042\n",
      "Iteration 60, loss = 0.12161978\n",
      "Iteration 61, loss = 0.11483112\n",
      "Iteration 62, loss = 0.11176388\n",
      "Iteration 63, loss = 0.11320328\n",
      "Iteration 64, loss = 0.11622323\n",
      "Iteration 65, loss = 0.11278923\n",
      "Iteration 66, loss = 0.10479942\n",
      "Iteration 67, loss = 0.09986262\n",
      "Iteration 68, loss = 0.09623564\n",
      "Iteration 69, loss = 0.09162370\n",
      "Iteration 70, loss = 0.09139189\n",
      "Iteration 71, loss = 0.08479327\n",
      "Iteration 72, loss = 0.08779663\n",
      "Iteration 73, loss = 0.08605751\n",
      "Iteration 74, loss = 0.08722166\n",
      "Iteration 75, loss = 0.08201238\n",
      "Iteration 76, loss = 0.07730167\n",
      "Iteration 77, loss = 0.07241456\n",
      "Iteration 78, loss = 0.07603965\n",
      "Iteration 79, loss = 0.07712067\n",
      "Iteration 80, loss = 0.07379634\n",
      "Iteration 81, loss = 0.06671387\n",
      "Iteration 82, loss = 0.06461245\n",
      "Iteration 83, loss = 0.06541074\n",
      "Iteration 84, loss = 0.07033636\n",
      "Iteration 85, loss = 0.06652833\n",
      "Iteration 86, loss = 0.06346425\n",
      "Iteration 87, loss = 0.06202668\n",
      "Iteration 88, loss = 0.05749800\n",
      "Iteration 89, loss = 0.06615789\n",
      "Iteration 90, loss = 0.05435923\n",
      "Iteration 91, loss = 0.04872623\n",
      "Iteration 92, loss = 0.04888405\n",
      "Iteration 93, loss = 0.05855062\n",
      "Iteration 94, loss = 0.04811732\n",
      "Iteration 95, loss = 0.04530622\n",
      "Iteration 96, loss = 0.04838908\n",
      "Iteration 97, loss = 0.04782378\n",
      "Iteration 98, loss = 0.04338323\n",
      "Iteration 99, loss = 0.04063995\n",
      "Iteration 100, loss = 0.04183962\n",
      "Iteration 101, loss = 0.04490333\n",
      "Iteration 102, loss = 0.04194646\n",
      "Iteration 103, loss = 0.03538880\n",
      "Iteration 104, loss = 0.03501609\n",
      "Iteration 105, loss = 0.03763543\n",
      "Iteration 106, loss = 0.03654330\n",
      "Iteration 107, loss = 0.04283697\n",
      "Iteration 108, loss = 0.03534355\n",
      "Iteration 109, loss = 0.07054738\n",
      "Iteration 110, loss = 0.03482285\n",
      "Iteration 111, loss = 0.03287988\n",
      "Iteration 112, loss = 0.02860101\n",
      "Iteration 113, loss = 0.03019569\n",
      "Iteration 114, loss = 0.02871728\n",
      "Iteration 115, loss = 0.02765697\n",
      "Iteration 116, loss = 0.03236030\n",
      "Iteration 117, loss = 0.02798539\n",
      "Iteration 118, loss = 0.02546368\n",
      "Iteration 119, loss = 0.02930596\n",
      "Iteration 120, loss = 0.02496166\n",
      "Iteration 121, loss = 0.02350607\n",
      "Iteration 122, loss = 0.02200128\n",
      "Iteration 123, loss = 0.02355234\n",
      "Iteration 124, loss = 0.02306500\n",
      "Iteration 125, loss = 0.02316602\n",
      "Iteration 126, loss = 0.02120579\n",
      "Iteration 127, loss = 0.02484143\n",
      "Iteration 128, loss = 0.02567942\n",
      "Iteration 129, loss = 0.02276926\n",
      "Iteration 130, loss = 0.02055146\n",
      "Iteration 131, loss = 0.02048418\n",
      "Iteration 132, loss = 0.02032764\n",
      "Iteration 133, loss = 0.04994348\n",
      "Iteration 134, loss = 0.02241801\n",
      "Iteration 135, loss = 0.01900903\n",
      "Iteration 136, loss = 0.01655076\n",
      "Iteration 137, loss = 0.01608984\n",
      "Iteration 138, loss = 0.01603430\n",
      "Iteration 139, loss = 0.01451860\n",
      "Iteration 140, loss = 0.01467402\n",
      "Iteration 141, loss = 0.01461212\n",
      "Iteration 142, loss = 0.01411554\n",
      "Iteration 143, loss = 0.01423138\n",
      "Iteration 144, loss = 0.01336608\n",
      "Iteration 145, loss = 0.01616499\n",
      "Iteration 146, loss = 0.01352237\n",
      "Iteration 147, loss = 0.01327421\n",
      "Iteration 148, loss = 0.01223609\n",
      "Iteration 149, loss = 0.01251843\n",
      "Iteration 150, loss = 0.01270215\n",
      "Iteration 151, loss = 0.01320884\n",
      "Iteration 152, loss = 0.01226071\n",
      "Iteration 153, loss = 0.01163774\n",
      "Iteration 154, loss = 0.01083330\n",
      "Iteration 155, loss = 0.01174219\n",
      "Iteration 156, loss = 0.01274447\n",
      "Iteration 157, loss = 0.01168282\n",
      "Iteration 158, loss = 0.01065006\n",
      "Iteration 159, loss = 0.01092534\n",
      "Iteration 160, loss = 0.00959068\n",
      "Iteration 161, loss = 0.01692239\n",
      "Iteration 162, loss = 0.01111640\n",
      "Iteration 163, loss = 0.00919319\n",
      "Iteration 164, loss = 0.00960047\n",
      "Iteration 165, loss = 0.00835081\n",
      "Iteration 166, loss = 0.00878907\n",
      "Iteration 167, loss = 0.01113420\n",
      "Iteration 168, loss = 0.00905313\n",
      "Iteration 169, loss = 0.00853019\n",
      "Iteration 170, loss = 0.00814552\n",
      "Iteration 171, loss = 0.00874409\n",
      "Iteration 172, loss = 0.00814167\n",
      "Iteration 173, loss = 0.00770709\n",
      "Iteration 174, loss = 0.00790039\n",
      "Iteration 175, loss = 0.00922156\n",
      "Iteration 176, loss = 0.00797616\n",
      "Iteration 177, loss = 0.00935652\n",
      "Iteration 178, loss = 0.00786969\n",
      "Iteration 179, loss = 0.02230132\n",
      "Iteration 180, loss = 0.01227871\n",
      "Iteration 181, loss = 0.00756971\n",
      "Iteration 182, loss = 0.00652673\n",
      "Iteration 183, loss = 0.00724289\n",
      "Iteration 184, loss = 0.00735378\n",
      "Iteration 185, loss = 0.01409157\n",
      "Iteration 186, loss = 0.00977311\n",
      "Iteration 187, loss = 0.00806280\n",
      "Iteration 188, loss = 0.00588097\n",
      "Iteration 189, loss = 0.00546613\n",
      "Iteration 190, loss = 0.00550543\n",
      "Iteration 191, loss = 0.00734738\n",
      "Iteration 192, loss = 0.00515397\n",
      "Iteration 193, loss = 0.00475066\n",
      "Iteration 194, loss = 0.00554130\n",
      "Iteration 195, loss = 0.00494544\n",
      "Iteration 196, loss = 0.00462388\n",
      "Iteration 197, loss = 0.00485270\n",
      "Iteration 198, loss = 0.00612679\n",
      "Iteration 199, loss = 0.00448229\n",
      "Iteration 200, loss = 0.00524872\n",
      "Iteration 201, loss = 0.00476479\n",
      "Iteration 202, loss = 0.00406757\n",
      "Iteration 203, loss = 0.00426387\n",
      "Iteration 204, loss = 0.00406517\n",
      "Iteration 205, loss = 0.00406086\n",
      "Iteration 206, loss = 0.00408600\n",
      "Iteration 207, loss = 0.00415327\n",
      "Iteration 208, loss = 0.00379686\n",
      "Iteration 209, loss = 0.00365408\n",
      "Iteration 210, loss = 0.00377114\n",
      "Iteration 211, loss = 0.00355488\n",
      "Iteration 212, loss = 0.00402501\n",
      "Iteration 213, loss = 0.00571866\n",
      "Iteration 214, loss = 0.00722951\n",
      "Iteration 215, loss = 0.00519607\n",
      "Iteration 216, loss = 0.00515788\n",
      "Iteration 217, loss = 0.00572108\n",
      "Iteration 218, loss = 0.00471579\n",
      "Iteration 219, loss = 0.00328216\n",
      "Iteration 220, loss = 0.00339393\n",
      "Iteration 221, loss = 0.00327175\n",
      "Iteration 222, loss = 0.00309583\n",
      "Iteration 223, loss = 0.00306248\n",
      "Iteration 224, loss = 0.00327417\n",
      "Iteration 225, loss = 0.00299192\n",
      "Iteration 226, loss = 0.00284698\n",
      "Iteration 227, loss = 0.00271786\n",
      "Iteration 228, loss = 0.00262071\n",
      "Iteration 229, loss = 0.00273580\n",
      "Iteration 230, loss = 0.00257660\n",
      "Iteration 231, loss = 0.00264609\n",
      "Iteration 232, loss = 0.00287024\n",
      "Iteration 233, loss = 0.00251551\n",
      "Iteration 234, loss = 0.00258279\n",
      "Iteration 235, loss = 0.00283192\n",
      "Iteration 236, loss = 0.00260071\n",
      "Iteration 237, loss = 0.00244423\n",
      "Iteration 238, loss = 0.00243443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52839185\n",
      "Iteration 2, loss = 0.47422001\n",
      "Iteration 3, loss = 0.44279073\n",
      "Iteration 4, loss = 0.43160556\n",
      "Iteration 5, loss = 0.42007287\n",
      "Iteration 6, loss = 0.41252313\n",
      "Iteration 7, loss = 0.40004464\n",
      "Iteration 8, loss = 0.40811670\n",
      "Iteration 9, loss = 0.39055313\n",
      "Iteration 10, loss = 0.37679588\n",
      "Iteration 11, loss = 0.36828315\n",
      "Iteration 12, loss = 0.37309055\n",
      "Iteration 13, loss = 0.36010851\n",
      "Iteration 14, loss = 0.35510911\n",
      "Iteration 15, loss = 0.33850726\n",
      "Iteration 16, loss = 0.34592357\n",
      "Iteration 17, loss = 0.32528931\n",
      "Iteration 18, loss = 0.32004173\n",
      "Iteration 19, loss = 0.31865000\n",
      "Iteration 20, loss = 0.30792797\n",
      "Iteration 21, loss = 0.30068403\n",
      "Iteration 22, loss = 0.29926257\n",
      "Iteration 23, loss = 0.29394474\n",
      "Iteration 24, loss = 0.28337438\n",
      "Iteration 25, loss = 0.27822280\n",
      "Iteration 26, loss = 0.26931383\n",
      "Iteration 27, loss = 0.26277316\n",
      "Iteration 28, loss = 0.25629074\n",
      "Iteration 29, loss = 0.25324109\n",
      "Iteration 30, loss = 0.24884733\n",
      "Iteration 31, loss = 0.24154801\n",
      "Iteration 32, loss = 0.23070647\n",
      "Iteration 33, loss = 0.22607908\n",
      "Iteration 34, loss = 0.22883507\n",
      "Iteration 35, loss = 0.21827768\n",
      "Iteration 36, loss = 0.20970883\n",
      "Iteration 37, loss = 0.21567573\n",
      "Iteration 38, loss = 0.19886801\n",
      "Iteration 39, loss = 0.20538934\n",
      "Iteration 40, loss = 0.19691690\n",
      "Iteration 41, loss = 0.19092138\n",
      "Iteration 42, loss = 0.18721563\n",
      "Iteration 43, loss = 0.17643930\n",
      "Iteration 44, loss = 0.17599334\n",
      "Iteration 45, loss = 0.16934073\n",
      "Iteration 46, loss = 0.16491373\n",
      "Iteration 47, loss = 0.15563328\n",
      "Iteration 48, loss = 0.15373604\n",
      "Iteration 49, loss = 0.15376657\n",
      "Iteration 50, loss = 0.15053274\n",
      "Iteration 51, loss = 0.15518267\n",
      "Iteration 52, loss = 0.13925252\n",
      "Iteration 53, loss = 0.13156504\n",
      "Iteration 54, loss = 0.13312572\n",
      "Iteration 55, loss = 0.12540615\n",
      "Iteration 56, loss = 0.12212833\n",
      "Iteration 57, loss = 0.12494436\n",
      "Iteration 58, loss = 0.11376347\n",
      "Iteration 59, loss = 0.11527915\n",
      "Iteration 60, loss = 0.10967083\n",
      "Iteration 61, loss = 0.10340285\n",
      "Iteration 62, loss = 0.10102558\n",
      "Iteration 63, loss = 0.10105747\n",
      "Iteration 64, loss = 0.10518079\n",
      "Iteration 65, loss = 0.10729267\n",
      "Iteration 66, loss = 0.09052538\n",
      "Iteration 67, loss = 0.08929437\n",
      "Iteration 68, loss = 0.08996744\n",
      "Iteration 69, loss = 0.08179323\n",
      "Iteration 70, loss = 0.07884527\n",
      "Iteration 71, loss = 0.07497026\n",
      "Iteration 72, loss = 0.07640526\n",
      "Iteration 73, loss = 0.07552526\n",
      "Iteration 74, loss = 0.07391493\n",
      "Iteration 75, loss = 0.07486487\n",
      "Iteration 76, loss = 0.06699708\n",
      "Iteration 77, loss = 0.06361790\n",
      "Iteration 78, loss = 0.06770910\n",
      "Iteration 79, loss = 0.06695447\n",
      "Iteration 80, loss = 0.06327673\n",
      "Iteration 81, loss = 0.05691836\n",
      "Iteration 82, loss = 0.05698151\n",
      "Iteration 83, loss = 0.05507076\n",
      "Iteration 84, loss = 0.06073514\n",
      "Iteration 85, loss = 0.05943702\n",
      "Iteration 86, loss = 0.05884277\n",
      "Iteration 87, loss = 0.05475187\n",
      "Iteration 88, loss = 0.05843411\n",
      "Iteration 89, loss = 0.04947000\n",
      "Iteration 90, loss = 0.04461923\n",
      "Iteration 91, loss = 0.04230867\n",
      "Iteration 92, loss = 0.04075183\n",
      "Iteration 93, loss = 0.05036771\n",
      "Iteration 94, loss = 0.04027095\n",
      "Iteration 95, loss = 0.03780908\n",
      "Iteration 96, loss = 0.04008723\n",
      "Iteration 97, loss = 0.03808994\n",
      "Iteration 98, loss = 0.03737005\n",
      "Iteration 99, loss = 0.03405182\n",
      "Iteration 100, loss = 0.03425080\n",
      "Iteration 101, loss = 0.03480451\n",
      "Iteration 102, loss = 0.03492988\n",
      "Iteration 103, loss = 0.02876171\n",
      "Iteration 104, loss = 0.02911802\n",
      "Iteration 105, loss = 0.03222184\n",
      "Iteration 106, loss = 0.03131510\n",
      "Iteration 107, loss = 0.03663760\n",
      "Iteration 108, loss = 0.02967555\n",
      "Iteration 109, loss = 0.04674356\n",
      "Iteration 110, loss = 0.02719918\n",
      "Iteration 111, loss = 0.02604347\n",
      "Iteration 112, loss = 0.02344170\n",
      "Iteration 113, loss = 0.02452035\n",
      "Iteration 114, loss = 0.02305794\n",
      "Iteration 115, loss = 0.02348949\n",
      "Iteration 116, loss = 0.02171622\n",
      "Iteration 117, loss = 0.02271834\n",
      "Iteration 118, loss = 0.02201359\n",
      "Iteration 119, loss = 0.02330733\n",
      "Iteration 120, loss = 0.02079791\n",
      "Iteration 121, loss = 0.02175010\n",
      "Iteration 122, loss = 0.01878729\n",
      "Iteration 123, loss = 0.01870510\n",
      "Iteration 124, loss = 0.01793696\n",
      "Iteration 125, loss = 0.01800716\n",
      "Iteration 126, loss = 0.01713869\n",
      "Iteration 127, loss = 0.01858237\n",
      "Iteration 128, loss = 0.02231144\n",
      "Iteration 129, loss = 0.01922527\n",
      "Iteration 130, loss = 0.01556252\n",
      "Iteration 131, loss = 0.01797564\n",
      "Iteration 132, loss = 0.01552373\n",
      "Iteration 133, loss = 0.03035192\n",
      "Iteration 134, loss = 0.01633843\n",
      "Iteration 135, loss = 0.01545445\n",
      "Iteration 136, loss = 0.01357876\n",
      "Iteration 137, loss = 0.01332825\n",
      "Iteration 138, loss = 0.01349998\n",
      "Iteration 139, loss = 0.01180617\n",
      "Iteration 140, loss = 0.01219250\n",
      "Iteration 141, loss = 0.01207309\n",
      "Iteration 142, loss = 0.01193345\n",
      "Iteration 143, loss = 0.01166068\n",
      "Iteration 144, loss = 0.01068185\n",
      "Iteration 145, loss = 0.01331257\n",
      "Iteration 146, loss = 0.01096555\n",
      "Iteration 147, loss = 0.01086283\n",
      "Iteration 148, loss = 0.01066935\n",
      "Iteration 149, loss = 0.01013622\n",
      "Iteration 150, loss = 0.01032889\n",
      "Iteration 151, loss = 0.00975717\n",
      "Iteration 152, loss = 0.00955430\n",
      "Iteration 153, loss = 0.00944889\n",
      "Iteration 154, loss = 0.00882485\n",
      "Iteration 155, loss = 0.01079440\n",
      "Iteration 156, loss = 0.01487132\n",
      "Iteration 157, loss = 0.01490619\n",
      "Iteration 158, loss = 0.00934237\n",
      "Iteration 159, loss = 0.00799376\n",
      "Iteration 160, loss = 0.00741740\n",
      "Iteration 161, loss = 0.01916197\n",
      "Iteration 162, loss = 0.01129119\n",
      "Iteration 163, loss = 0.00823796\n",
      "Iteration 164, loss = 0.00742445\n",
      "Iteration 165, loss = 0.00700047\n",
      "Iteration 166, loss = 0.00681088\n",
      "Iteration 167, loss = 0.00712607\n",
      "Iteration 168, loss = 0.00631233\n",
      "Iteration 169, loss = 0.00644315\n",
      "Iteration 170, loss = 0.00654693\n",
      "Iteration 171, loss = 0.00731269\n",
      "Iteration 172, loss = 0.00609569\n",
      "Iteration 173, loss = 0.00585426\n",
      "Iteration 174, loss = 0.00597081\n",
      "Iteration 175, loss = 0.00656515\n",
      "Iteration 176, loss = 0.00594200\n",
      "Iteration 177, loss = 0.00630839\n",
      "Iteration 178, loss = 0.00678197\n",
      "Iteration 179, loss = 0.01196934\n",
      "Iteration 180, loss = 0.00640804\n",
      "Iteration 181, loss = 0.00532748\n",
      "Iteration 182, loss = 0.00532161\n",
      "Iteration 183, loss = 0.00567436\n",
      "Iteration 184, loss = 0.00537107\n",
      "Iteration 185, loss = 0.00916431\n",
      "Iteration 186, loss = 0.00554352\n",
      "Iteration 187, loss = 0.00524888\n",
      "Iteration 188, loss = 0.00497070\n",
      "Iteration 189, loss = 0.00429228\n",
      "Iteration 190, loss = 0.00403949\n",
      "Iteration 191, loss = 0.00422399\n",
      "Iteration 192, loss = 0.00393190\n",
      "Iteration 193, loss = 0.00384745\n",
      "Iteration 194, loss = 0.00450030\n",
      "Iteration 195, loss = 0.00409615\n",
      "Iteration 196, loss = 0.00384176\n",
      "Iteration 197, loss = 0.00377064\n",
      "Iteration 198, loss = 0.01025139\n",
      "Iteration 199, loss = 0.00777646\n",
      "Iteration 200, loss = 0.00678095\n",
      "Iteration 201, loss = 0.00545561\n",
      "Iteration 202, loss = 0.00479444\n",
      "Iteration 203, loss = 0.00565271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55150806\n",
      "Iteration 2, loss = 0.48317942\n",
      "Iteration 3, loss = 0.45575714\n",
      "Iteration 4, loss = 0.44374950\n",
      "Iteration 5, loss = 0.43405708\n",
      "Iteration 6, loss = 0.42520159\n",
      "Iteration 7, loss = 0.41737623\n",
      "Iteration 8, loss = 0.42140904\n",
      "Iteration 9, loss = 0.40600752\n",
      "Iteration 10, loss = 0.40195898\n",
      "Iteration 11, loss = 0.39316274\n",
      "Iteration 12, loss = 0.39967021\n",
      "Iteration 13, loss = 0.39239465\n",
      "Iteration 14, loss = 0.37873312\n",
      "Iteration 15, loss = 0.36998423\n",
      "Iteration 16, loss = 0.37078177\n",
      "Iteration 17, loss = 0.35951113\n",
      "Iteration 18, loss = 0.35489062\n",
      "Iteration 19, loss = 0.35361457\n",
      "Iteration 20, loss = 0.34666249\n",
      "Iteration 21, loss = 0.33950668\n",
      "Iteration 22, loss = 0.33750816\n",
      "Iteration 23, loss = 0.33254369\n",
      "Iteration 24, loss = 0.32702192\n",
      "Iteration 25, loss = 0.32148700\n",
      "Iteration 26, loss = 0.31555171\n",
      "Iteration 27, loss = 0.31008661\n",
      "Iteration 28, loss = 0.30316740\n",
      "Iteration 29, loss = 0.30121039\n",
      "Iteration 30, loss = 0.29988558\n",
      "Iteration 31, loss = 0.29302859\n",
      "Iteration 32, loss = 0.28414186\n",
      "Iteration 33, loss = 0.27860097\n",
      "Iteration 34, loss = 0.27811251\n",
      "Iteration 35, loss = 0.27234672\n",
      "Iteration 36, loss = 0.27067745\n",
      "Iteration 37, loss = 0.26907844\n",
      "Iteration 38, loss = 0.26246684\n",
      "Iteration 39, loss = 0.26371843\n",
      "Iteration 40, loss = 0.24908967\n",
      "Iteration 41, loss = 0.24694196\n",
      "Iteration 42, loss = 0.24071947\n",
      "Iteration 43, loss = 0.23759891\n",
      "Iteration 44, loss = 0.23712584\n",
      "Iteration 45, loss = 0.22968037\n",
      "Iteration 46, loss = 0.22713995\n",
      "Iteration 47, loss = 0.21827776\n",
      "Iteration 48, loss = 0.21616729\n",
      "Iteration 49, loss = 0.21361426\n",
      "Iteration 50, loss = 0.20973429\n",
      "Iteration 51, loss = 0.20802661\n",
      "Iteration 52, loss = 0.20620727\n",
      "Iteration 53, loss = 0.19860449\n",
      "Iteration 54, loss = 0.19263362\n",
      "Iteration 55, loss = 0.19250914\n",
      "Iteration 56, loss = 0.18524280\n",
      "Iteration 57, loss = 0.18617986\n",
      "Iteration 58, loss = 0.18078172\n",
      "Iteration 59, loss = 0.17891892\n",
      "Iteration 60, loss = 0.17346295\n",
      "Iteration 61, loss = 0.16931819\n",
      "Iteration 62, loss = 0.16451599\n",
      "Iteration 63, loss = 0.17359250\n",
      "Iteration 64, loss = 0.17281236\n",
      "Iteration 65, loss = 0.16050180\n",
      "Iteration 66, loss = 0.15310103\n",
      "Iteration 67, loss = 0.15225653\n",
      "Iteration 68, loss = 0.15073698\n",
      "Iteration 69, loss = 0.14905706\n",
      "Iteration 70, loss = 0.14304561\n",
      "Iteration 71, loss = 0.13874990\n",
      "Iteration 72, loss = 0.13843380\n",
      "Iteration 73, loss = 0.13337185\n",
      "Iteration 74, loss = 0.13060991\n",
      "Iteration 75, loss = 0.13295225\n",
      "Iteration 76, loss = 0.12583863\n",
      "Iteration 77, loss = 0.13144687\n",
      "Iteration 78, loss = 0.12640652\n",
      "Iteration 79, loss = 0.11985154\n",
      "Iteration 80, loss = 0.12505770\n",
      "Iteration 81, loss = 0.11550049\n",
      "Iteration 82, loss = 0.11275296\n",
      "Iteration 83, loss = 0.11071163\n",
      "Iteration 84, loss = 0.11038805\n",
      "Iteration 85, loss = 0.11296380\n",
      "Iteration 86, loss = 0.10805264\n",
      "Iteration 87, loss = 0.10890670\n",
      "Iteration 88, loss = 0.09968756\n",
      "Iteration 89, loss = 0.10277481\n",
      "Iteration 90, loss = 0.09926726\n",
      "Iteration 91, loss = 0.09200117\n",
      "Iteration 92, loss = 0.09052678\n",
      "Iteration 93, loss = 0.09389913\n",
      "Iteration 94, loss = 0.09102979\n",
      "Iteration 95, loss = 0.08662415\n",
      "Iteration 96, loss = 0.08956791\n",
      "Iteration 97, loss = 0.08546548\n",
      "Iteration 98, loss = 0.08353332\n",
      "Iteration 99, loss = 0.08035467\n",
      "Iteration 100, loss = 0.08161272\n",
      "Iteration 101, loss = 0.07750247\n",
      "Iteration 102, loss = 0.08201801\n",
      "Iteration 103, loss = 0.07284490\n",
      "Iteration 104, loss = 0.07442894\n",
      "Iteration 105, loss = 0.07675720\n",
      "Iteration 106, loss = 0.07757783\n",
      "Iteration 107, loss = 0.07331082\n",
      "Iteration 108, loss = 0.06880991\n",
      "Iteration 109, loss = 0.07454982\n",
      "Iteration 110, loss = 0.06829093\n",
      "Iteration 111, loss = 0.06494076\n",
      "Iteration 112, loss = 0.06264894\n",
      "Iteration 113, loss = 0.06613647\n",
      "Iteration 114, loss = 0.05958731\n",
      "Iteration 115, loss = 0.05905288\n",
      "Iteration 116, loss = 0.05701642\n",
      "Iteration 117, loss = 0.05538435\n",
      "Iteration 118, loss = 0.05452812\n",
      "Iteration 119, loss = 0.05494707\n",
      "Iteration 120, loss = 0.05450708\n",
      "Iteration 121, loss = 0.05092950\n",
      "Iteration 122, loss = 0.05021647\n",
      "Iteration 123, loss = 0.05435103\n",
      "Iteration 124, loss = 0.04968945\n",
      "Iteration 125, loss = 0.04773578\n",
      "Iteration 126, loss = 0.05006026\n",
      "Iteration 127, loss = 0.05717462\n",
      "Iteration 128, loss = 0.04972751\n",
      "Iteration 129, loss = 0.04751784\n",
      "Iteration 130, loss = 0.04498647\n",
      "Iteration 131, loss = 0.04428387\n",
      "Iteration 132, loss = 0.04288750\n",
      "Iteration 133, loss = 0.04514031\n",
      "Iteration 134, loss = 0.04073427\n",
      "Iteration 135, loss = 0.04640405\n",
      "Iteration 136, loss = 0.03989770\n",
      "Iteration 137, loss = 0.03848465\n",
      "Iteration 138, loss = 0.03785843\n",
      "Iteration 139, loss = 0.03549248\n",
      "Iteration 140, loss = 0.03804130\n",
      "Iteration 141, loss = 0.03483439\n",
      "Iteration 142, loss = 0.03646919\n",
      "Iteration 143, loss = 0.03816628\n",
      "Iteration 144, loss = 0.03344090\n",
      "Iteration 145, loss = 0.03373652\n",
      "Iteration 146, loss = 0.03113039\n",
      "Iteration 147, loss = 0.03252595\n",
      "Iteration 148, loss = 0.03004158\n",
      "Iteration 149, loss = 0.03146188\n",
      "Iteration 150, loss = 0.02959204\n",
      "Iteration 151, loss = 0.03198930\n",
      "Iteration 152, loss = 0.03018847\n",
      "Iteration 153, loss = 0.02836116\n",
      "Iteration 154, loss = 0.02828933\n",
      "Iteration 155, loss = 0.02835878\n",
      "Iteration 156, loss = 0.03177089\n",
      "Iteration 157, loss = 0.02649391\n",
      "Iteration 158, loss = 0.02488281\n",
      "Iteration 159, loss = 0.02427657\n",
      "Iteration 160, loss = 0.02376209\n",
      "Iteration 161, loss = 0.02611445\n",
      "Iteration 162, loss = 0.02377719\n",
      "Iteration 163, loss = 0.02255283\n",
      "Iteration 164, loss = 0.02242628\n",
      "Iteration 165, loss = 0.02125145\n",
      "Iteration 166, loss = 0.02147102\n",
      "Iteration 167, loss = 0.02745143\n",
      "Iteration 168, loss = 0.02098777\n",
      "Iteration 169, loss = 0.02054724\n",
      "Iteration 170, loss = 0.02110896\n",
      "Iteration 171, loss = 0.02411188\n",
      "Iteration 172, loss = 0.02021872\n",
      "Iteration 173, loss = 0.01904674\n",
      "Iteration 174, loss = 0.01854496\n",
      "Iteration 175, loss = 0.01859961\n",
      "Iteration 176, loss = 0.01832279\n",
      "Iteration 177, loss = 0.01993021\n",
      "Iteration 178, loss = 0.01845685\n",
      "Iteration 179, loss = 0.01837653\n",
      "Iteration 180, loss = 0.01774721\n",
      "Iteration 181, loss = 0.01583119\n",
      "Iteration 182, loss = 0.01628132\n",
      "Iteration 183, loss = 0.01560494\n",
      "Iteration 184, loss = 0.01470088\n",
      "Iteration 185, loss = 0.02054863\n",
      "Iteration 186, loss = 0.01522166\n",
      "Iteration 187, loss = 0.01501000\n",
      "Iteration 188, loss = 0.01540467\n",
      "Iteration 189, loss = 0.01497785\n",
      "Iteration 190, loss = 0.01336373\n",
      "Iteration 191, loss = 0.01410044\n",
      "Iteration 192, loss = 0.01286150\n",
      "Iteration 193, loss = 0.01289884\n",
      "Iteration 194, loss = 0.01634462\n",
      "Iteration 195, loss = 0.01545367\n",
      "Iteration 196, loss = 0.01574772\n",
      "Iteration 197, loss = 0.01488729\n",
      "Iteration 198, loss = 0.01396212\n",
      "Iteration 199, loss = 0.01192300\n",
      "Iteration 200, loss = 0.01197323\n",
      "Iteration 201, loss = 0.01261684\n",
      "Iteration 202, loss = 0.01122230\n",
      "Iteration 203, loss = 0.01197829\n",
      "Iteration 204, loss = 0.01200269\n",
      "Iteration 205, loss = 0.01115762\n",
      "Iteration 206, loss = 0.01059983\n",
      "Iteration 207, loss = 0.01085246\n",
      "Iteration 208, loss = 0.01101340\n",
      "Iteration 209, loss = 0.00964696\n",
      "Iteration 210, loss = 0.00945722\n",
      "Iteration 211, loss = 0.00935859\n",
      "Iteration 212, loss = 0.00942416\n",
      "Iteration 213, loss = 0.00961415\n",
      "Iteration 214, loss = 0.00975134\n",
      "Iteration 215, loss = 0.00931509\n",
      "Iteration 216, loss = 0.00891086\n",
      "Iteration 217, loss = 0.00862002\n",
      "Iteration 218, loss = 0.00807220\n",
      "Iteration 219, loss = 0.00830740\n",
      "Iteration 220, loss = 0.00894735\n",
      "Iteration 221, loss = 0.00884210\n",
      "Iteration 222, loss = 0.00796528\n",
      "Iteration 223, loss = 0.00771851\n",
      "Iteration 224, loss = 0.00994160\n",
      "Iteration 225, loss = 0.01075128\n",
      "Iteration 226, loss = 0.00916810\n",
      "Iteration 227, loss = 0.00812838\n",
      "Iteration 228, loss = 0.00740182\n",
      "Iteration 229, loss = 0.00686238\n",
      "Iteration 230, loss = 0.00696465\n",
      "Iteration 231, loss = 0.00717940\n",
      "Iteration 232, loss = 0.00855804\n",
      "Iteration 233, loss = 0.00691569\n",
      "Iteration 234, loss = 0.00688493\n",
      "Iteration 235, loss = 0.00721941\n",
      "Iteration 236, loss = 0.00668242\n",
      "Iteration 237, loss = 0.00623838\n",
      "Iteration 238, loss = 0.00599450\n",
      "Iteration 239, loss = 0.00584769\n",
      "Iteration 240, loss = 0.00567407\n",
      "Iteration 241, loss = 0.00571868\n",
      "Iteration 242, loss = 0.00558836\n",
      "Iteration 243, loss = 0.00555595\n",
      "Iteration 244, loss = 0.00546485\n",
      "Iteration 245, loss = 0.00704059\n",
      "Iteration 246, loss = 0.00966914\n",
      "Iteration 247, loss = 0.00974209\n",
      "Iteration 248, loss = 0.00921695\n",
      "Iteration 249, loss = 0.00740783\n",
      "Iteration 250, loss = 0.00639020\n",
      "Iteration 251, loss = 0.00541805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlps, accuracies = run_experiment_across_layers(mlp_probe_experiment, train_detect_neg_consistency_ful, train_labels_det_neg_consistency, test_detect_neg_consistency_ful, test_labels_det_neg_consistency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = generate_classification_report_all_layers(test_detect_neg_consistency_ful, test_labels_det_neg_consistency, mlps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       954\n",
      "           1       0.73      0.71      0.72       721\n",
      "\n",
      "    accuracy                           0.76      1675\n",
      "   macro avg       0.76      0.75      0.76      1675\n",
      "weighted avg       0.76      0.76      0.76      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80       954\n",
      "           1       0.75      0.68      0.71       721\n",
      "\n",
      "    accuracy                           0.77      1675\n",
      "   macro avg       0.76      0.76      0.76      1675\n",
      "weighted avg       0.76      0.77      0.76      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       954\n",
      "           1       0.73      0.72      0.72       721\n",
      "\n",
      "    accuracy                           0.76      1675\n",
      "   macro avg       0.76      0.76      0.76      1675\n",
      "weighted avg       0.76      0.76      0.76      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[0:3]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       954\n",
      "           1       0.77      0.76      0.76       721\n",
      "\n",
      "    accuracy                           0.80      1675\n",
      "   macro avg       0.79      0.79      0.79      1675\n",
      "weighted avg       0.80      0.80      0.80      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       954\n",
      "           1       0.74      0.76      0.75       721\n",
      "\n",
      "    accuracy                           0.78      1675\n",
      "   macro avg       0.78      0.78      0.78      1675\n",
      "weighted avg       0.78      0.78      0.78      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       954\n",
      "           1       0.76      0.74      0.75       721\n",
      "\n",
      "    accuracy                           0.79      1675\n",
      "   macro avg       0.78      0.78      0.78      1675\n",
      "weighted avg       0.79      0.79      0.79      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[6:9]:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83       954\n",
      "           1       0.77      0.76      0.77       721\n",
      "\n",
      "    accuracy                           0.80      1675\n",
      "   macro avg       0.80      0.80      0.80      1675\n",
      "weighted avg       0.80      0.80      0.80      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83       954\n",
      "           1       0.78      0.73      0.75       721\n",
      "\n",
      "    accuracy                           0.80      1675\n",
      "   macro avg       0.79      0.79      0.79      1675\n",
      "weighted avg       0.80      0.80      0.80      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.82       954\n",
      "           1       0.77      0.73      0.75       721\n",
      "\n",
      "    accuracy                           0.79      1675\n",
      "   macro avg       0.79      0.79      0.79      1675\n",
      "weighted avg       0.79      0.79      0.79      1675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       954\n",
      "           1       0.78      0.77      0.78       721\n",
      "\n",
      "    accuracy                           0.81      1675\n",
      "   macro avg       0.81      0.80      0.80      1675\n",
      "weighted avg       0.81      0.81      0.81      1675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for report in reports[9:]:\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_research_env)",
   "language": "python",
   "name": "new_research_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
