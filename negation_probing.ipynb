{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaWrapper(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Wrapper on roberta that gives mean-pooled representations of each layer in a list\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.model_obj = RobertaModel.from_pretrained(\n",
    "            \"roberta-base\").eval()\n",
    "        self.model_obj.eval()\n",
    "        self.tokenizer_obj = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        self.config_obj = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    def forward(self, input_text):\n",
    "\n",
    "        encoder_ret = self.tokenizer_obj(\n",
    "            input_text, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        encoder_text_ids = encoder_ret.input_ids.to(self.device)\n",
    "        attention_mask = encoder_ret.attention_mask.to(self.device) # 1 for not pad\n",
    "\n",
    "        ic(encoder_text_ids.device)\n",
    "        encoder_states = self.model_obj(\n",
    "            encoder_text_ids, output_hidden_states=True, attention_mask=attention_mask)\n",
    "\n",
    "        ic(self.model_obj.device)\n",
    "        hs_tuple = encoder_states[\"hidden_states\"]\n",
    "\n",
    "        mean_pooled_all_layers = []\n",
    "\n",
    "        for layer, hs in enumerate(hs_tuple):\n",
    "            ic(hs_tuple[layer].size())\n",
    "            # hs = hs_tuple[layer] # (batch_size x sequence_length x dimension)\n",
    "            hs_masked = hs * attention_mask[:, :, None] # ideally zeros out the pad associated representations\n",
    "            ic(hs_masked.size())\n",
    "            seq_lengths = attention_mask.sum(dim=1) # each line here represents sequence length\n",
    "\n",
    "            hs_masked_sum = hs_masked.sum(dim=1)\n",
    "            hs_avg = hs_masked_sum / seq_lengths[:, None]\n",
    "            mean_pooled_all_layers.append(hs_avg)\n",
    "\n",
    "        return mean_pooled_all_layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset data_xor/default to /home/mrcreator/research/main_thread/fresh_repo/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': '70230897f97c0425f1c23dd623529f9f6e057014c5f1753a92cf17966f2c89f0', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_xor/default/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset data_xor downloaded and prepared to /home/mrcreator/research/main_thread/fresh_repo/{'cache_dir': None, 'config_name': None, 'data_dir': None, 'data_files': None, 'hash': '70230897f97c0425f1c23dd623529f9f6e057014c5f1753a92cf17966f2c89f0', 'features': None, 'use_auth_token': None, 'base_path': 'data_scripts', 'add_sep': False}/data_xor/default/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 203.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id_': tensor([1, 2, 3]), 'content': ['Library of Alexandria is located in Alexandria. Library of Alexandria is located in Alexandria.', 'Library of Alexandria is located in Alexandria. Library of Alexandria is not located in Alexandria.', 'Library of Alexandria is not located in Alexandria. Library of Alexandria is located in Alexandria.'], 'label': tensor([0, 1, 1])}\n",
      "13\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model_wrapped = RobertaWrapper()\n",
    "test_dataset_xor = datasets.load_dataset(\"data_scripts/data_xor.py\", add_sep=False)[\"train\"]\n",
    "test_dataloader = DataLoader(test_dataset_xor, batch_size=3)\n",
    "\n",
    "print(next(iter(test_dataloader)))\n",
    "output = model_wrapped(next(iter(test_dataloader))[\"content\"])\n",
    "print(len(output))\n",
    "print(output[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states_many_examples(model, data, n=100, layer=-1):\n",
    "    \"\"\"\n",
    "    Takes a bunch of sequences and runs them through RoBERTa to generate the mean-pooled hidden states.\n",
    "\n",
    "    This is unbatched and kept inefficient for simplicity\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_hidden_states, all_labels = [], []\n",
    "    # all_hidden_states: will have elements for each RoBERTa layer, each element represents the mean-pooled representations for the whole data at that layer\n",
    "\n",
    "\n",
    "    # loop\n",
    "    for idx in tqdm(range(n)):\n",
    "\n",
    "        text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "        print(data[idx][\"content\"])\n",
    "        print(data[idx][\"label\"])\n",
    "\n",
    "        # get hidden states\n",
    "        with torch.no_grad():\n",
    "            outs = model(text)\n",
    "        # outs: [hidden states]\n",
    "\n",
    "        # initialize if empty\n",
    "        if len(all_hidden_states) == 0:\n",
    "            for i in range(len(outs)):\n",
    "                all_hidden_states.append([])\n",
    "\n",
    "\n",
    "        # collect\n",
    "        for i, hidden_state in enumerate(outs):\n",
    "            all_hidden_states[i].append(hidden_state)\n",
    "\n",
    "        all_labels.append(true_label)\n",
    "\n",
    "    ic(len(all_hidden_states))\n",
    "    ic(len(all_hidden_states[0]))\n",
    "    ic(all_hidden_states[0][0].size())\n",
    "    ic(torch.cat(all_hidden_states[0], dim=0).size())\n",
    "\n",
    "    all_hidden_states = [torch.cat(all_hidden_states[i], dim=0) for i in range(len(all_hidden_states))]\n",
    "\n",
    "\n",
    "    return all_hidden_states, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:00<00:00, 19.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library of Alexandria is located in Alexandria. Library of Alexandria is located in Alexandria.\n",
      "0\n",
      "Library of Alexandria is located in Alexandria. Library of Alexandria is not located in Alexandria.\n",
      "1\n",
      "Library of Alexandria is not located in Alexandria. Library of Alexandria is located in Alexandria.\n",
      "1\n",
      "Library of Alexandria is not located in Alexandria. Library of Alexandria is not located in Alexandria.\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 17.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([4, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ic.disable()\n",
    "outs = get_hidden_states_many_examples(model_wrapped, test_dataset_xor, n=4)\n",
    "print(len(outs[0]))\n",
    "print(outs[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_across_layers(experiment, train_input, train_labels, test_input, test_labels):\n",
    "    \"\"\"\n",
    "    Runs a probing experiment over representations from all layers of the model.\n",
    "    The whole thing works on cached embeddings\n",
    "\n",
    "    experiment: method (train: Tensor, test: Tensor, label_train: Tensor, label_test: Tensor) -> (fit_model, metrics). Each experiment will fit _some_ model on the data and return the model and the results\n",
    "    train_input: list of 13 elements, each of which is a tensor of size (num_datapoints, embedding_dim)\n",
    "    train_labels: tensor (num_datapoints, )\n",
    "    test_input: same format as train_input\n",
    "    test_labels: same format as train_labels\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_results = []\n",
    "    list_of_probing_models = []\n",
    "\n",
    "    for i in range(len(train_input)):\n",
    "        train_current_layer = train_input[i]\n",
    "        test_current_layer = test_input[i]\n",
    "\n",
    "        model, metrics = experiment(train_current_layer, test_current_layer, train_labels, test_labels)\n",
    "\n",
    "        list_of_results.append(metrics)\n",
    "        list_of_probing_models.append(model)\n",
    "\n",
    "    return list_of_probing_models, list_of_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_experiment(train_input, test_input, train_labels, test_labels, probe_model):\n",
    "    \"\"\"\n",
    "    Gets an initialized probe model and fits it on data and runs some experiments\n",
    "    expected to be curried and sent as a callback to run_experiment_across_layers\n",
    "    \"\"\"\n",
    "\n",
    "    train_input_numpy = train_input.detach().numpy()\n",
    "    test_input_numpy = test_input.detach().numpy()\n",
    "    train_labels_numpy = train_labels.detach().numpy()\n",
    "    test_labels_numpy = test_labels.detach().numpy()\n",
    "\n",
    "    model.fit(train_input_numpy, train_labels_numpy)\n",
    "\n",
    "    accuracy = model.score(test_input_numpy, test_labels_numpy)\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "def linear_probe_experiment(train_input, test_input, train_labels, test_labels):\n",
    "    # initialize linear probe and run probe experiment\n",
    "    lr = LogisticRegression(class_weight=\"balanced\", verbose=1, max_iter=1000)\n",
    "    return probe_experiment(train_input, test_input, train_labels, test_labels, lr)\n",
    "\n",
    "\n",
    "def mlp_probe_experiment(train_input, test_input, train_labels, test_labels):\n",
    "    # initialize an mlp probe and run probe experiment\n",
    "    mlp = MLPClassifier(random_state=1, max_iter=1000, verbose=True)\n",
    "    return probe_experiment(train_input, test_input, train_labels, test_labels, mlp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
